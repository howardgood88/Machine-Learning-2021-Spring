{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"「ML2021Spring - HW1.ipynb」的副本","provenance":[{"file_id":"https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb","timestamp":1615271751271}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"mz0_QVkxCrX3"},"source":["# **Homework 1: COVID-19 Cases Prediction (Regression)**"]},{"cell_type":"markdown","metadata":{"id":"ZeZnPAiwDRWG"},"source":["Author: Heng-Jui Chang\n","\n","Slides: https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.pdf  \n","Video: TBA\n","\n","Objectives:\n","* Solve a regression problem with deep neural networks (DNN).\n","* Understand basic DNN training tips.\n","* Get familiar with PyTorch.\n","\n","If any questions, please contact the TAs via TA hours, NTU COOL, or email.\n"]},{"cell_type":"markdown","metadata":{"id":"Jx3x1nDkG-Uy"},"source":["# **Download Data**\n","\n","\n","If the Google drive links are dead, you can download data from [kaggle](https://www.kaggle.com/c/ml2021spring-hw1/data), and upload data manually to the workspace."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tMj55YDKG6ch","executionInfo":{"status":"ok","timestamp":1616231178310,"user_tz":-480,"elapsed":2158,"user":{"displayName":"張君豪","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giewpu4H1RC4LywWkj1iyfrLc5i6hgwttwazWU6EQ=s64","userId":"03999016312843703738"}},"outputId":"f3b74a0e-6147-4d6a-a674-92347a2e16ae"},"source":["tr_path = 'covid.train.csv'  # path to training data\n","tt_path = 'covid.test.csv'   # path to testing data\n","\n","!gdown --id '19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF' --output covid.train.csv\n","!gdown --id '1CE240jLm2npU-tdz81-oVKEF3T2yfT1O' --output covid.test.csv"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF\n","To: /content/covid.train.csv\n","100% 2.00M/2.00M [00:00<00:00, 132MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1CE240jLm2npU-tdz81-oVKEF3T2yfT1O\n","To: /content/covid.test.csv\n","100% 651k/651k [00:00<00:00, 95.8MB/s]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wS_4-77xHk44"},"source":["# **Import Some Packages**"]},{"cell_type":"code","metadata":{"id":"k-onQd4JNA5H"},"source":["# PyTorch\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","# For data preprocess\n","import numpy as np\n","import csv\n","import os\n","\n","# For plotting\n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import figure\n","\n","myseed = 42069  # set a random seed for reproducibility\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","np.random.seed(myseed)\n","torch.manual_seed(myseed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(myseed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BtE3b6JEH7rw"},"source":["# **Some Utilities**\n","\n","You do not need to modify this part."]},{"cell_type":"code","metadata":{"id":"FWMT3uf1NGQp"},"source":["def get_device():\n","    ''' Get device (if GPU is available, use GPU) '''\n","    return 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","def plot_learning_curve(loss_record, title=''):\n","    ''' Plot learning curve of your DNN (train & dev loss) '''\n","    total_steps = len(loss_record['train'])\n","    x_1 = range(total_steps)\n","    x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]\n","    figure(figsize=(6, 4))\n","    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n","    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')\n","    plt.ylim(0.0, 5.)\n","    plt.xlabel('Training steps')\n","    plt.ylabel('MSE loss')\n","    plt.title('Learning curve of {}'.format(title))\n","    plt.legend()\n","    plt.show()\n","\n","\n","def plot_pred(dv_set, model, device, lim=35., preds=None, targets=None):\n","    ''' Plot prediction of your DNN '''\n","    if preds is None or targets is None:\n","        model.eval()\n","        preds, targets = [], []\n","        for x, y in dv_set:\n","            x, y = x.to(device), y.to(device)\n","            with torch.no_grad():\n","                pred = model(x)\n","                preds.append(pred.detach().cpu())\n","                targets.append(y.detach().cpu())\n","        preds = torch.cat(preds, dim=0).numpy()\n","        targets = torch.cat(targets, dim=0).numpy()\n","\n","    figure(figsize=(5, 5))\n","    plt.scatter(targets, preds, c='r', alpha=0.5)\n","    plt.plot([-0.2, lim], [-0.2, lim], c='b')\n","    plt.xlim(-0.2, lim)\n","    plt.ylim(-0.2, lim)\n","    plt.xlabel('ground truth value')\n","    plt.ylabel('predicted value')\n","    plt.title('Ground Truth v.s. Prediction')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"39U_XFX6KOoj"},"source":["# **Preprocess**\n","\n","We have three kinds of datasets:\n","* `train`: for training\n","* `dev`: for validation\n","* `test`: for testing (w/o target value)"]},{"cell_type":"markdown","metadata":{"id":"TQ-MdwpLL7Dt"},"source":["## **Dataset**\n","\n","The `COVID19Dataset` below does:\n","* read `.csv` files\n","* extract features\n","* split `covid.train.csv` into train/dev sets\n","* normalize features\n","\n","Finishing `TODO` below might make you pass medium baseline."]},{"cell_type":"code","metadata":{"id":"0zlpIp9ANJRU"},"source":["class COVID19Dataset(Dataset):\n","    ''' Dataset for loading and preprocessing the COVID19 dataset '''\n","    def __init__(self,\n","                 path,\n","                 mode='train',\n","                 target_only=False):\n","        self.mode = mode\n","\n","        # Read data into numpy arrays\n","        with open(path, 'r') as fp:\n","            data = list(csv.reader(fp))\n","            data = np.array(data[1:])[:, 1:].astype(float)\n","        \n","        if not target_only:\n","            feats = list(range(93))\n","        else:\n","            # TODO: Using 40 states & 2 tested_positive features (indices = 57 & 75)\n","            # day_feats = [0, 4, 10, 11, 15, 17]\n","            # feats = [i + 40 for i in day_feats] + [i + 58 for i in day_feats] + [i + 76 for i in day_feats][:-1]\n","            feats = list(range(40, 93))\n","            pass\n","\n","        if mode == 'test':\n","            # Testing data\n","            # data: 893 x 93 (40 states + day 1 (18) + day 2 (18) + day 3 (17))\n","            data = data[:, feats]\n","            self.data = torch.FloatTensor(data)\n","        else:\n","            # Training data (train/dev sets)\n","            # data: 2700 x 94 (40 states + day 1 (18) + day 2 (18) + day 3 (18))\n","            target = data[:, -1]\n","            data = data[:, feats]\n","            \n","            # Splitting training data into train & dev sets\n","            # if mode == 'train':\n","            #     indices = [i for i in range(len(data)) if i % 10 != 0]\n","            # elif mode == 'dev':\n","            #     indices = [i for i in range(len(data)) if i % 10 == 0]\n","            \n","            # Convert data into PyTorch tensors\n","            self.data = torch.FloatTensor(data)\n","            self.target = torch.FloatTensor(target)\n","\n","        # Normalize features (you may remove this part to see what will happen)\n","        # self.data[:, 40:] = \\\n","        #     (self.data[:, 40:] - self.data[:, 40:].mean(dim=0, keepdim=True)) \\\n","        #     / self.data[:, 40:].std(dim=0, keepdim=True)\n","\n","        self.dim = self.data.shape[1]\n","\n","        print('Finished reading the {} set of COVID19 Dataset ({} samples found, each dim = {})'\n","              .format(mode, len(self.data), self.dim))\n","\n","    def __getitem__(self, index):\n","        # Returns one sample at a time\n","        if self.mode in ['train', 'dev']:\n","            # For training\n","            return self.data[index], self.target[index]\n","        else:\n","            # For testing (no target)\n","            return self.data[index]\n","\n","    def __len__(self):\n","        # Returns the size of the dataset\n","        return len(self.data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AlhTlkE7MDo3"},"source":["## **DataLoader**\n","\n","A `DataLoader` loads data from a given `Dataset` into batches.\n"]},{"cell_type":"code","metadata":{"id":"hlhLk5t6MBX3"},"source":["def prep_dataloader(path, mode, batch_size, n_jobs=0, target_only=False):\n","    ''' Generates a dataset, then is put into a dataloader. '''\n","    dataset = COVID19Dataset(path, mode=mode, target_only=target_only)  # Construct dataset\n","    dataloader = DataLoader(\n","        dataset, batch_size,\n","        shuffle=(mode == 'train'), drop_last=False,\n","        num_workers=n_jobs, pin_memory=True)                            # Construct dataloader\n","    return dataloader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SGuycwR0MeQB"},"source":["# **Deep Neural Network**\n","\n","`NeuralNet` is an `nn.Module` designed for regression.\n","The DNN consists of 2 fully-connected layers with ReLU activation.\n","This module also included a function `cal_loss` for calculating loss.\n"]},{"cell_type":"code","metadata":{"id":"49-uXYovOAI0"},"source":["class NeuralNet(nn.Module):\n","    ''' A simple fully-connected deep neural network '''\n","    def __init__(self, input_dim):\n","        super(NeuralNet, self).__init__()\n","\n","        # Define your neural network here\n","        # TODO: How to modify this model to achieve better performance?\n","        self.net = nn.Sequential(\n","            nn.Linear(input_dim, 1)\n","        )\n","\n","        # Mean squared error loss\n","        self.criterion = nn.MSELoss(reduction='mean')\n","\n","    def forward(self, x):\n","        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n","        return self.net(x).squeeze(1)\n","\n","    def cal_loss(self, pred, target):\n","        ''' Calculate loss '''\n","        # TODO: you may implement L2 regularization here\n","        return self.criterion(pred, target)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DvFWVjZ5Nvga"},"source":["# **Train/Dev/Test**"]},{"cell_type":"markdown","metadata":{"id":"MAM8QecJOyqn"},"source":["## **Training**"]},{"cell_type":"code","metadata":{"id":"lOqcmYzMO7jB"},"source":["def train(tr_set, dv_set, model, config, device):\n","    ''' DNN training '''\n","\n","    n_epochs = config['n_epochs']  # Maximum number of epochs\n","\n","    # Setup optimizer\n","    optimizer = getattr(torch.optim, config['optimizer'])(\n","        model.parameters(), **config['optim_hparas'])\n","\n","    min_rmse = 1000\n","    loss_record = {'train': [], 'dev': []}      # for recording training loss\n","    early_stop_cnt = 0\n","    epoch = 0\n","    while epoch < n_epochs:\n","        model.train()                           # set model to training mode\n","        total_loss = 0\n","        for x, y in tr_set:                     # iterate through the dataloader\n","            optimizer.zero_grad()               # set gradient to zero\n","            x, y = x.to(device), y.to(device)   # move data to device (cpu/cuda)\n","            pred = model(x)                     # forward pass (compute output)\n","            rmse_loss = torch.sqrt(model.cal_loss(pred, y))  # compute loss\n","            rmse_loss.backward()                 # compute gradient (backpropagation)\n","            optimizer.step()                    # update model with optimizer\n","\n","            total_loss += rmse_loss.detach().cpu().item() * len(x)    # accumulate loss\n","            loss_record['train'].append(rmse_loss.detach().cpu().item())    # record loss\n","        total_loss = total_loss / len(dv_set.dataset)              # compute averaged loss\n","\n","        # After each epoch, test your model on the validation (development) set.\n","        dev_rmse = dev(dv_set, model, device)\n","        if dev_rmse < min_rmse:\n","            # Save model if your model improved\n","            min_rmse = dev_rmse\n","            print('Saving model (epoch = {:4d}, train loss = {:.4f}, dev loss = {:.4f})'\n","                .format(epoch + 1, total_loss, min_rmse))\n","            torch.save(model.state_dict(), config['save_path'])  # Save model to specified path\n","            early_stop_cnt = 0\n","        else:\n","            early_stop_cnt += 1\n","\n","        epoch += 1\n","        loss_record['dev'].append(dev_rmse)\n","        if early_stop_cnt > config['early_stop']:\n","            # Stop training if your model stops improving for \"config['early_stop']\" epochs.\n","            break\n","\n","    print('Finished training after {} epochs'.format(epoch))\n","    return min_rmse, loss_record"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0hSd4Bn3O2PL"},"source":["## **Validation**"]},{"cell_type":"code","metadata":{"id":"yrxrD3YsN3U2"},"source":["def dev(dv_set, model, device):\n","    model.eval()                                # set model to evalutation mode\n","    total_loss = 0\n","    for x, y in dv_set:                         # iterate through the dataloader\n","        x, y = x.to(device), y.to(device)       # move data to device (cpu/cuda)\n","        with torch.no_grad():                   # disable gradient calculation\n","            pred = model(x)                     # forward pass (compute output)\n","            rmse_loss = torch.sqrt(model.cal_loss(pred, y))  # compute loss\n","        total_loss += rmse_loss.detach().cpu().item() * len(x)  # accumulate loss\n","    total_loss = total_loss / len(dv_set.dataset)              # compute averaged loss\n","\n","    return total_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g0pdrhQAO41L"},"source":["## **Testing**"]},{"cell_type":"code","metadata":{"id":"aSBMRFlYN5tB"},"source":["def test(tt_set, model, device):\n","    model.eval()                                # set model to evalutation mode\n","    preds = []\n","    for x in tt_set:                            # iterate through the dataloader\n","        x = x.to(device)                        # move data to device (cpu/cuda)\n","        with torch.no_grad():                   # disable gradient calculation\n","            pred = model(x)                     # forward pass (compute output)\n","            preds.append(pred.detach().cpu())   # collect prediction\n","    preds = torch.cat(preds, dim=0).numpy()     # concatenate all predictions and convert to a numpy array\n","    return preds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SvckkF5dvf0j"},"source":["# **Setup Hyper-parameters**\n","\n","`config` contains hyper-parameters for training and the path to save your model."]},{"cell_type":"code","metadata":{"id":"NPXpdumwPjE7"},"source":["device = get_device()                 # get the current available device ('cpu' or 'cuda')\n","os.makedirs('models', exist_ok=True)  # The trained model will be saved to ./models/\n","target_only = True                   # TODO: Using 40 states & 2 tested_positive features\n","\n","# TODO: How to tune these hyper-parameters to improve your model's performance?\n","config = {\n","    'n_epochs': 15000,                # maximum number of epochs\n","    'batch_size': 128,               # mini-batch size for dataloader\n","    'optimizer': 'Adam',              # optimization algorithm (optimizer in torch.optim)\n","    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n","        'lr': 0.0001,                 # learning rate of Adam\n","        'weight_decay': 0.001            # L2 penalty for Adam\n","    },\n","    'early_stop': 300,               # early stopping epochs (the number epochs since your model's last improvement)\n","    'save_path': 'models/model.pth'  # your model will be saved here\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6j1eOV3TOH-j"},"source":["# **Load data and model**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eNrYBMmePLKm","executionInfo":{"status":"ok","timestamp":1616231178817,"user_tz":-480,"elapsed":2637,"user":{"displayName":"張君豪","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giewpu4H1RC4LywWkj1iyfrLc5i6hgwttwazWU6EQ=s64","userId":"03999016312843703738"}},"outputId":"703a710c-84ea-428a-9022-e9c629c3064b"},"source":["tr_set = prep_dataloader(tr_path, 'train', config['batch_size'], target_only=target_only)\n","dv_set = prep_dataloader(tr_path, 'dev', config['batch_size'], target_only=target_only)\n","tt_set = prep_dataloader(tt_path, 'test', config['batch_size'], target_only=target_only)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Finished reading the train set of COVID19 Dataset (2700 samples found, each dim = 53)\n","Finished reading the dev set of COVID19 Dataset (2700 samples found, each dim = 53)\n","Finished reading the test set of COVID19 Dataset (893 samples found, each dim = 53)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FHylSirLP9oh"},"source":["model = NeuralNet(tr_set.dataset.dim).to(device)  # Construct model and move to device"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sX2B_zgSOPTJ"},"source":["# **Start Training!**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GrEbUxazQAAZ","executionInfo":{"status":"ok","timestamp":1616231875021,"user_tz":-480,"elapsed":698830,"user":{"displayName":"張君豪","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giewpu4H1RC4LywWkj1iyfrLc5i6hgwttwazWU6EQ=s64","userId":"03999016312843703738"}},"outputId":"02c4b927-3a35-400c-d375-69d22d51c04b"},"source":["model_loss, model_loss_record = train(tr_set, dv_set, model, config, device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Saving model (epoch =    1, train loss = 30.7624, dev loss = 28.7211)\n","Saving model (epoch =    2, train loss = 27.6783, dev loss = 25.6250)\n","Saving model (epoch =    3, train loss = 24.6209, dev loss = 22.5525)\n","Saving model (epoch =    4, train loss = 21.5979, dev loss = 19.5161)\n","Saving model (epoch =    5, train loss = 18.6382, dev loss = 16.5432)\n","Saving model (epoch =    6, train loss = 15.7591, dev loss = 13.6787)\n","Saving model (epoch =    7, train loss = 13.0280, dev loss = 11.0159)\n","Saving model (epoch =    8, train loss = 10.5498, dev loss = 8.7568)\n","Saving model (epoch =    9, train loss = 8.4850, dev loss = 7.1586)\n","Saving model (epoch =   10, train loss = 7.0898, dev loss = 6.3023)\n","Saving model (epoch =   11, train loss = 6.3699, dev loss = 5.9809)\n","Saving model (epoch =   12, train loss = 6.1141, dev loss = 5.8646)\n","Saving model (epoch =   13, train loss = 6.0312, dev loss = 5.8124)\n","Saving model (epoch =   14, train loss = 5.9757, dev loss = 5.7669)\n","Saving model (epoch =   15, train loss = 5.9249, dev loss = 5.7201)\n","Saving model (epoch =   16, train loss = 5.8789, dev loss = 5.6720)\n","Saving model (epoch =   17, train loss = 5.8264, dev loss = 5.6229)\n","Saving model (epoch =   18, train loss = 5.7711, dev loss = 5.5719)\n","Saving model (epoch =   19, train loss = 5.7148, dev loss = 5.5202)\n","Saving model (epoch =   20, train loss = 5.6579, dev loss = 5.4682)\n","Saving model (epoch =   21, train loss = 5.6059, dev loss = 5.4150)\n","Saving model (epoch =   22, train loss = 5.5496, dev loss = 5.3606)\n","Saving model (epoch =   23, train loss = 5.4920, dev loss = 5.3047)\n","Saving model (epoch =   24, train loss = 5.4329, dev loss = 5.2481)\n","Saving model (epoch =   25, train loss = 5.3714, dev loss = 5.1934)\n","Saving model (epoch =   26, train loss = 5.3121, dev loss = 5.1362)\n","Saving model (epoch =   27, train loss = 5.2576, dev loss = 5.0780)\n","Saving model (epoch =   28, train loss = 5.1923, dev loss = 5.0200)\n","Saving model (epoch =   29, train loss = 5.1302, dev loss = 4.9625)\n","Saving model (epoch =   30, train loss = 5.0754, dev loss = 4.9060)\n","Saving model (epoch =   31, train loss = 5.0126, dev loss = 4.8480)\n","Saving model (epoch =   32, train loss = 4.9472, dev loss = 4.7903)\n","Saving model (epoch =   33, train loss = 4.8881, dev loss = 4.7317)\n","Saving model (epoch =   34, train loss = 4.8295, dev loss = 4.6753)\n","Saving model (epoch =   35, train loss = 4.7650, dev loss = 4.6148)\n","Saving model (epoch =   36, train loss = 4.7077, dev loss = 4.5567)\n","Saving model (epoch =   37, train loss = 4.6450, dev loss = 4.5000)\n","Saving model (epoch =   38, train loss = 4.5863, dev loss = 4.4418)\n","Saving model (epoch =   39, train loss = 4.5219, dev loss = 4.3842)\n","Saving model (epoch =   40, train loss = 4.4687, dev loss = 4.3294)\n","Saving model (epoch =   41, train loss = 4.4106, dev loss = 4.2750)\n","Saving model (epoch =   42, train loss = 4.3542, dev loss = 4.2191)\n","Saving model (epoch =   43, train loss = 4.2982, dev loss = 4.1654)\n","Saving model (epoch =   44, train loss = 4.2405, dev loss = 4.1095)\n","Saving model (epoch =   45, train loss = 4.1865, dev loss = 4.0583)\n","Saving model (epoch =   46, train loss = 4.1313, dev loss = 4.0035)\n","Saving model (epoch =   47, train loss = 4.0826, dev loss = 3.9522)\n","Saving model (epoch =   48, train loss = 4.0249, dev loss = 3.9042)\n","Saving model (epoch =   49, train loss = 3.9771, dev loss = 3.8523)\n","Saving model (epoch =   50, train loss = 3.9245, dev loss = 3.8029)\n","Saving model (epoch =   51, train loss = 3.8754, dev loss = 3.7533)\n","Saving model (epoch =   52, train loss = 3.8273, dev loss = 3.7070)\n","Saving model (epoch =   53, train loss = 3.7861, dev loss = 3.6605)\n","Saving model (epoch =   54, train loss = 3.7400, dev loss = 3.6172)\n","Saving model (epoch =   55, train loss = 3.6947, dev loss = 3.5744)\n","Saving model (epoch =   56, train loss = 3.6573, dev loss = 3.5325)\n","Saving model (epoch =   57, train loss = 3.6185, dev loss = 3.4905)\n","Saving model (epoch =   58, train loss = 3.5786, dev loss = 3.4526)\n","Saving model (epoch =   59, train loss = 3.5380, dev loss = 3.4134)\n","Saving model (epoch =   60, train loss = 3.5027, dev loss = 3.3755)\n","Saving model (epoch =   61, train loss = 3.4650, dev loss = 3.3384)\n","Saving model (epoch =   62, train loss = 3.4354, dev loss = 3.3032)\n","Saving model (epoch =   63, train loss = 3.3955, dev loss = 3.2698)\n","Saving model (epoch =   64, train loss = 3.3685, dev loss = 3.2362)\n","Saving model (epoch =   65, train loss = 3.3368, dev loss = 3.2055)\n","Saving model (epoch =   66, train loss = 3.3124, dev loss = 3.1758)\n","Saving model (epoch =   67, train loss = 3.2883, dev loss = 3.1462)\n","Saving model (epoch =   68, train loss = 3.2600, dev loss = 3.1188)\n","Saving model (epoch =   69, train loss = 3.2360, dev loss = 3.0930)\n","Saving model (epoch =   70, train loss = 3.2118, dev loss = 3.0668)\n","Saving model (epoch =   71, train loss = 3.1865, dev loss = 3.0425)\n","Saving model (epoch =   72, train loss = 3.1679, dev loss = 3.0187)\n","Saving model (epoch =   73, train loss = 3.1478, dev loss = 2.9967)\n","Saving model (epoch =   74, train loss = 3.1244, dev loss = 2.9748)\n","Saving model (epoch =   75, train loss = 3.1056, dev loss = 2.9540)\n","Saving model (epoch =   76, train loss = 3.0885, dev loss = 2.9354)\n","Saving model (epoch =   77, train loss = 3.0706, dev loss = 2.9163)\n","Saving model (epoch =   78, train loss = 3.0494, dev loss = 2.8983)\n","Saving model (epoch =   79, train loss = 3.0387, dev loss = 2.8804)\n","Saving model (epoch =   80, train loss = 3.0237, dev loss = 2.8635)\n","Saving model (epoch =   81, train loss = 3.0049, dev loss = 2.8475)\n","Saving model (epoch =   82, train loss = 2.9990, dev loss = 2.8317)\n","Saving model (epoch =   83, train loss = 2.9757, dev loss = 2.8179)\n","Saving model (epoch =   84, train loss = 2.9631, dev loss = 2.8024)\n","Saving model (epoch =   85, train loss = 2.9543, dev loss = 2.7893)\n","Saving model (epoch =   86, train loss = 2.9410, dev loss = 2.7764)\n","Saving model (epoch =   87, train loss = 2.9275, dev loss = 2.7634)\n","Saving model (epoch =   88, train loss = 2.9066, dev loss = 2.7513)\n","Saving model (epoch =   89, train loss = 2.9022, dev loss = 2.7398)\n","Saving model (epoch =   90, train loss = 2.8953, dev loss = 2.7284)\n","Saving model (epoch =   91, train loss = 2.8803, dev loss = 2.7177)\n","Saving model (epoch =   92, train loss = 2.8682, dev loss = 2.7069)\n","Saving model (epoch =   93, train loss = 2.8598, dev loss = 2.6976)\n","Saving model (epoch =   94, train loss = 2.8494, dev loss = 2.6862)\n","Saving model (epoch =   95, train loss = 2.8407, dev loss = 2.6768)\n","Saving model (epoch =   96, train loss = 2.8304, dev loss = 2.6688)\n","Saving model (epoch =   97, train loss = 2.8186, dev loss = 2.6583)\n","Saving model (epoch =   98, train loss = 2.8139, dev loss = 2.6510)\n","Saving model (epoch =   99, train loss = 2.8021, dev loss = 2.6405)\n","Saving model (epoch =  100, train loss = 2.7967, dev loss = 2.6356)\n","Saving model (epoch =  101, train loss = 2.7858, dev loss = 2.6239)\n","Saving model (epoch =  102, train loss = 2.7789, dev loss = 2.6171)\n","Saving model (epoch =  103, train loss = 2.7745, dev loss = 2.6058)\n","Saving model (epoch =  104, train loss = 2.7562, dev loss = 2.5998)\n","Saving model (epoch =  105, train loss = 2.7479, dev loss = 2.5897)\n","Saving model (epoch =  106, train loss = 2.7419, dev loss = 2.5820)\n","Saving model (epoch =  107, train loss = 2.7340, dev loss = 2.5744)\n","Saving model (epoch =  108, train loss = 2.7264, dev loss = 2.5666)\n","Saving model (epoch =  109, train loss = 2.7091, dev loss = 2.5607)\n","Saving model (epoch =  110, train loss = 2.7113, dev loss = 2.5505)\n","Saving model (epoch =  112, train loss = 2.6931, dev loss = 2.5350)\n","Saving model (epoch =  113, train loss = 2.6840, dev loss = 2.5276)\n","Saving model (epoch =  114, train loss = 2.6771, dev loss = 2.5213)\n","Saving model (epoch =  115, train loss = 2.6645, dev loss = 2.5135)\n","Saving model (epoch =  116, train loss = 2.6593, dev loss = 2.5049)\n","Saving model (epoch =  117, train loss = 2.6499, dev loss = 2.4974)\n","Saving model (epoch =  118, train loss = 2.6383, dev loss = 2.4891)\n","Saving model (epoch =  119, train loss = 2.6279, dev loss = 2.4831)\n","Saving model (epoch =  120, train loss = 2.6208, dev loss = 2.4741)\n","Saving model (epoch =  121, train loss = 2.6150, dev loss = 2.4665)\n","Saving model (epoch =  122, train loss = 2.6101, dev loss = 2.4596)\n","Saving model (epoch =  123, train loss = 2.5998, dev loss = 2.4534)\n","Saving model (epoch =  124, train loss = 2.5909, dev loss = 2.4441)\n","Saving model (epoch =  125, train loss = 2.5789, dev loss = 2.4366)\n","Saving model (epoch =  126, train loss = 2.5772, dev loss = 2.4300)\n","Saving model (epoch =  127, train loss = 2.5796, dev loss = 2.4294)\n","Saving model (epoch =  128, train loss = 2.5535, dev loss = 2.4151)\n","Saving model (epoch =  129, train loss = 2.5480, dev loss = 2.4092)\n","Saving model (epoch =  130, train loss = 2.5396, dev loss = 2.3998)\n","Saving model (epoch =  131, train loss = 2.5334, dev loss = 2.3943)\n","Saving model (epoch =  132, train loss = 2.5282, dev loss = 2.3894)\n","Saving model (epoch =  133, train loss = 2.5145, dev loss = 2.3786)\n","Saving model (epoch =  134, train loss = 2.5093, dev loss = 2.3699)\n","Saving model (epoch =  135, train loss = 2.4987, dev loss = 2.3649)\n","Saving model (epoch =  136, train loss = 2.4893, dev loss = 2.3549)\n","Saving model (epoch =  137, train loss = 2.4834, dev loss = 2.3495)\n","Saving model (epoch =  138, train loss = 2.4762, dev loss = 2.3411)\n","Saving model (epoch =  139, train loss = 2.4720, dev loss = 2.3331)\n","Saving model (epoch =  141, train loss = 2.4554, dev loss = 2.3189)\n","Saving model (epoch =  142, train loss = 2.4451, dev loss = 2.3110)\n","Saving model (epoch =  143, train loss = 2.4355, dev loss = 2.3057)\n","Saving model (epoch =  144, train loss = 2.4269, dev loss = 2.2958)\n","Saving model (epoch =  145, train loss = 2.4194, dev loss = 2.2889)\n","Saving model (epoch =  146, train loss = 2.4088, dev loss = 2.2809)\n","Saving model (epoch =  147, train loss = 2.4021, dev loss = 2.2731)\n","Saving model (epoch =  148, train loss = 2.3971, dev loss = 2.2661)\n","Saving model (epoch =  149, train loss = 2.3830, dev loss = 2.2582)\n","Saving model (epoch =  150, train loss = 2.3735, dev loss = 2.2513)\n","Saving model (epoch =  151, train loss = 2.3726, dev loss = 2.2445)\n","Saving model (epoch =  152, train loss = 2.3610, dev loss = 2.2400)\n","Saving model (epoch =  153, train loss = 2.3571, dev loss = 2.2301)\n","Saving model (epoch =  154, train loss = 2.3485, dev loss = 2.2233)\n","Saving model (epoch =  155, train loss = 2.3345, dev loss = 2.2140)\n","Saving model (epoch =  156, train loss = 2.3274, dev loss = 2.2077)\n","Saving model (epoch =  157, train loss = 2.3187, dev loss = 2.1995)\n","Saving model (epoch =  158, train loss = 2.3126, dev loss = 2.1928)\n","Saving model (epoch =  159, train loss = 2.3007, dev loss = 2.1851)\n","Saving model (epoch =  160, train loss = 2.2971, dev loss = 2.1790)\n","Saving model (epoch =  161, train loss = 2.2863, dev loss = 2.1771)\n","Saving model (epoch =  162, train loss = 2.2827, dev loss = 2.1629)\n","Saving model (epoch =  163, train loss = 2.2739, dev loss = 2.1554)\n","Saving model (epoch =  164, train loss = 2.2633, dev loss = 2.1481)\n","Saving model (epoch =  165, train loss = 2.2526, dev loss = 2.1409)\n","Saving model (epoch =  166, train loss = 2.2470, dev loss = 2.1340)\n","Saving model (epoch =  167, train loss = 2.2386, dev loss = 2.1266)\n","Saving model (epoch =  168, train loss = 2.2348, dev loss = 2.1204)\n","Saving model (epoch =  169, train loss = 2.2248, dev loss = 2.1128)\n","Saving model (epoch =  170, train loss = 2.2131, dev loss = 2.1085)\n","Saving model (epoch =  171, train loss = 2.2061, dev loss = 2.0972)\n","Saving model (epoch =  172, train loss = 2.1991, dev loss = 2.0924)\n","Saving model (epoch =  174, train loss = 2.1825, dev loss = 2.0754)\n","Saving model (epoch =  175, train loss = 2.1700, dev loss = 2.0713)\n","Saving model (epoch =  176, train loss = 2.1666, dev loss = 2.0624)\n","Saving model (epoch =  177, train loss = 2.1629, dev loss = 2.0542)\n","Saving model (epoch =  178, train loss = 2.1528, dev loss = 2.0501)\n","Saving model (epoch =  179, train loss = 2.1418, dev loss = 2.0394)\n","Saving model (epoch =  180, train loss = 2.1267, dev loss = 2.0370)\n","Saving model (epoch =  181, train loss = 2.1293, dev loss = 2.0259)\n","Saving model (epoch =  182, train loss = 2.1298, dev loss = 2.0194)\n","Saving model (epoch =  183, train loss = 2.1128, dev loss = 2.0133)\n","Saving model (epoch =  184, train loss = 2.1015, dev loss = 2.0041)\n","Saving model (epoch =  185, train loss = 2.0934, dev loss = 1.9965)\n","Saving model (epoch =  186, train loss = 2.0868, dev loss = 1.9917)\n","Saving model (epoch =  187, train loss = 2.0801, dev loss = 1.9833)\n","Saving model (epoch =  188, train loss = 2.0703, dev loss = 1.9757)\n","Saving model (epoch =  189, train loss = 2.0658, dev loss = 1.9702)\n","Saving model (epoch =  190, train loss = 2.0521, dev loss = 1.9619)\n","Saving model (epoch =  191, train loss = 2.0516, dev loss = 1.9554)\n","Saving model (epoch =  192, train loss = 2.0465, dev loss = 1.9508)\n","Saving model (epoch =  193, train loss = 2.0309, dev loss = 1.9466)\n","Saving model (epoch =  194, train loss = 2.0243, dev loss = 1.9346)\n","Saving model (epoch =  195, train loss = 2.0153, dev loss = 1.9320)\n","Saving model (epoch =  196, train loss = 2.0088, dev loss = 1.9219)\n","Saving model (epoch =  197, train loss = 2.0032, dev loss = 1.9142)\n","Saving model (epoch =  199, train loss = 1.9940, dev loss = 1.9020)\n","Saving model (epoch =  200, train loss = 1.9816, dev loss = 1.8993)\n","Saving model (epoch =  201, train loss = 1.9759, dev loss = 1.8872)\n","Saving model (epoch =  202, train loss = 1.9676, dev loss = 1.8802)\n","Saving model (epoch =  203, train loss = 1.9567, dev loss = 1.8747)\n","Saving model (epoch =  204, train loss = 1.9566, dev loss = 1.8684)\n","Saving model (epoch =  205, train loss = 1.9502, dev loss = 1.8616)\n","Saving model (epoch =  206, train loss = 1.9408, dev loss = 1.8540)\n","Saving model (epoch =  207, train loss = 1.9318, dev loss = 1.8491)\n","Saving model (epoch =  208, train loss = 1.9215, dev loss = 1.8407)\n","Saving model (epoch =  209, train loss = 1.9152, dev loss = 1.8368)\n","Saving model (epoch =  210, train loss = 1.9074, dev loss = 1.8295)\n","Saving model (epoch =  211, train loss = 1.9023, dev loss = 1.8209)\n","Saving model (epoch =  212, train loss = 1.8926, dev loss = 1.8139)\n","Saving model (epoch =  213, train loss = 1.8865, dev loss = 1.8093)\n","Saving model (epoch =  214, train loss = 1.8829, dev loss = 1.8037)\n","Saving model (epoch =  215, train loss = 1.8738, dev loss = 1.7945)\n","Saving model (epoch =  216, train loss = 1.8635, dev loss = 1.7902)\n","Saving model (epoch =  217, train loss = 1.8601, dev loss = 1.7848)\n","Saving model (epoch =  218, train loss = 1.8507, dev loss = 1.7751)\n","Saving model (epoch =  219, train loss = 1.8462, dev loss = 1.7697)\n","Saving model (epoch =  220, train loss = 1.8385, dev loss = 1.7629)\n","Saving model (epoch =  221, train loss = 1.8285, dev loss = 1.7566)\n","Saving model (epoch =  222, train loss = 1.8255, dev loss = 1.7511)\n","Saving model (epoch =  224, train loss = 1.8155, dev loss = 1.7385)\n","Saving model (epoch =  225, train loss = 1.8039, dev loss = 1.7327)\n","Saving model (epoch =  226, train loss = 1.7969, dev loss = 1.7255)\n","Saving model (epoch =  227, train loss = 1.7913, dev loss = 1.7211)\n","Saving model (epoch =  228, train loss = 1.7852, dev loss = 1.7138)\n","Saving model (epoch =  229, train loss = 1.7746, dev loss = 1.7078)\n","Saving model (epoch =  230, train loss = 1.7750, dev loss = 1.7035)\n","Saving model (epoch =  231, train loss = 1.7629, dev loss = 1.6953)\n","Saving model (epoch =  232, train loss = 1.7661, dev loss = 1.6898)\n","Saving model (epoch =  233, train loss = 1.7584, dev loss = 1.6867)\n","Saving model (epoch =  234, train loss = 1.7461, dev loss = 1.6780)\n","Saving model (epoch =  236, train loss = 1.7402, dev loss = 1.6662)\n","Saving model (epoch =  237, train loss = 1.7268, dev loss = 1.6602)\n","Saving model (epoch =  239, train loss = 1.7244, dev loss = 1.6492)\n","Saving model (epoch =  240, train loss = 1.7085, dev loss = 1.6434)\n","Saving model (epoch =  241, train loss = 1.7034, dev loss = 1.6381)\n","Saving model (epoch =  242, train loss = 1.6976, dev loss = 1.6326)\n","Saving model (epoch =  243, train loss = 1.6931, dev loss = 1.6280)\n","Saving model (epoch =  245, train loss = 1.6800, dev loss = 1.6180)\n","Saving model (epoch =  246, train loss = 1.6724, dev loss = 1.6147)\n","Saving model (epoch =  247, train loss = 1.6671, dev loss = 1.6076)\n","Saving model (epoch =  248, train loss = 1.6599, dev loss = 1.6027)\n","Saving model (epoch =  249, train loss = 1.6570, dev loss = 1.5947)\n","Saving model (epoch =  250, train loss = 1.6504, dev loss = 1.5892)\n","Saving model (epoch =  251, train loss = 1.6483, dev loss = 1.5840)\n","Saving model (epoch =  252, train loss = 1.6401, dev loss = 1.5808)\n","Saving model (epoch =  253, train loss = 1.6352, dev loss = 1.5798)\n","Saving model (epoch =  254, train loss = 1.6241, dev loss = 1.5688)\n","Saving model (epoch =  255, train loss = 1.6191, dev loss = 1.5652)\n","Saving model (epoch =  256, train loss = 1.6152, dev loss = 1.5606)\n","Saving model (epoch =  257, train loss = 1.6179, dev loss = 1.5563)\n","Saving model (epoch =  258, train loss = 1.6085, dev loss = 1.5487)\n","Saving model (epoch =  259, train loss = 1.5998, dev loss = 1.5461)\n","Saving model (epoch =  260, train loss = 1.5966, dev loss = 1.5390)\n","Saving model (epoch =  261, train loss = 1.5890, dev loss = 1.5377)\n","Saving model (epoch =  262, train loss = 1.5872, dev loss = 1.5350)\n","Saving model (epoch =  263, train loss = 1.5807, dev loss = 1.5332)\n","Saving model (epoch =  264, train loss = 1.5795, dev loss = 1.5260)\n","Saving model (epoch =  265, train loss = 1.5720, dev loss = 1.5153)\n","Saving model (epoch =  266, train loss = 1.5658, dev loss = 1.5104)\n","Saving model (epoch =  267, train loss = 1.5655, dev loss = 1.5069)\n","Saving model (epoch =  269, train loss = 1.5542, dev loss = 1.4973)\n","Saving model (epoch =  270, train loss = 1.5436, dev loss = 1.4923)\n","Saving model (epoch =  271, train loss = 1.5369, dev loss = 1.4919)\n","Saving model (epoch =  272, train loss = 1.5359, dev loss = 1.4859)\n","Saving model (epoch =  273, train loss = 1.5282, dev loss = 1.4795)\n","Saving model (epoch =  274, train loss = 1.5219, dev loss = 1.4745)\n","Saving model (epoch =  275, train loss = 1.5221, dev loss = 1.4699)\n","Saving model (epoch =  276, train loss = 1.5202, dev loss = 1.4658)\n","Saving model (epoch =  277, train loss = 1.5213, dev loss = 1.4634)\n","Saving model (epoch =  279, train loss = 1.5081, dev loss = 1.4529)\n","Saving model (epoch =  280, train loss = 1.4957, dev loss = 1.4515)\n","Saving model (epoch =  281, train loss = 1.4944, dev loss = 1.4443)\n","Saving model (epoch =  283, train loss = 1.4965, dev loss = 1.4388)\n","Saving model (epoch =  284, train loss = 1.4787, dev loss = 1.4323)\n","Saving model (epoch =  287, train loss = 1.4756, dev loss = 1.4213)\n","Saving model (epoch =  289, train loss = 1.4558, dev loss = 1.4143)\n","Saving model (epoch =  290, train loss = 1.4569, dev loss = 1.4091)\n","Saving model (epoch =  291, train loss = 1.4503, dev loss = 1.4080)\n","Saving model (epoch =  292, train loss = 1.4529, dev loss = 1.4049)\n","Saving model (epoch =  293, train loss = 1.4446, dev loss = 1.4009)\n","Saving model (epoch =  294, train loss = 1.4472, dev loss = 1.3945)\n","Saving model (epoch =  295, train loss = 1.4349, dev loss = 1.3913)\n","Saving model (epoch =  296, train loss = 1.4312, dev loss = 1.3890)\n","Saving model (epoch =  297, train loss = 1.4312, dev loss = 1.3838)\n","Saving model (epoch =  298, train loss = 1.4240, dev loss = 1.3800)\n","Saving model (epoch =  299, train loss = 1.4207, dev loss = 1.3770)\n","Saving model (epoch =  301, train loss = 1.4136, dev loss = 1.3698)\n","Saving model (epoch =  302, train loss = 1.4092, dev loss = 1.3664)\n","Saving model (epoch =  303, train loss = 1.4056, dev loss = 1.3635)\n","Saving model (epoch =  304, train loss = 1.4041, dev loss = 1.3604)\n","Saving model (epoch =  305, train loss = 1.4017, dev loss = 1.3600)\n","Saving model (epoch =  307, train loss = 1.4000, dev loss = 1.3598)\n","Saving model (epoch =  308, train loss = 1.3917, dev loss = 1.3471)\n","Saving model (epoch =  309, train loss = 1.3874, dev loss = 1.3438)\n","Saving model (epoch =  310, train loss = 1.3837, dev loss = 1.3405)\n","Saving model (epoch =  311, train loss = 1.3787, dev loss = 1.3376)\n","Saving model (epoch =  312, train loss = 1.3731, dev loss = 1.3355)\n","Saving model (epoch =  314, train loss = 1.3716, dev loss = 1.3282)\n","Saving model (epoch =  316, train loss = 1.3729, dev loss = 1.3266)\n","Saving model (epoch =  317, train loss = 1.3632, dev loss = 1.3194)\n","Saving model (epoch =  318, train loss = 1.3593, dev loss = 1.3176)\n","Saving model (epoch =  319, train loss = 1.3582, dev loss = 1.3162)\n","Saving model (epoch =  320, train loss = 1.3528, dev loss = 1.3133)\n","Saving model (epoch =  321, train loss = 1.3497, dev loss = 1.3118)\n","Saving model (epoch =  322, train loss = 1.3509, dev loss = 1.3054)\n","Saving model (epoch =  323, train loss = 1.3442, dev loss = 1.3025)\n","Saving model (epoch =  324, train loss = 1.3376, dev loss = 1.3019)\n","Saving model (epoch =  325, train loss = 1.3344, dev loss = 1.2978)\n","Saving model (epoch =  326, train loss = 1.3335, dev loss = 1.2959)\n","Saving model (epoch =  327, train loss = 1.3354, dev loss = 1.2921)\n","Saving model (epoch =  328, train loss = 1.3285, dev loss = 1.2905)\n","Saving model (epoch =  329, train loss = 1.3259, dev loss = 1.2868)\n","Saving model (epoch =  330, train loss = 1.3230, dev loss = 1.2843)\n","Saving model (epoch =  331, train loss = 1.3209, dev loss = 1.2820)\n","Saving model (epoch =  332, train loss = 1.3178, dev loss = 1.2790)\n","Saving model (epoch =  335, train loss = 1.3152, dev loss = 1.2720)\n","Saving model (epoch =  336, train loss = 1.3120, dev loss = 1.2703)\n","Saving model (epoch =  338, train loss = 1.3048, dev loss = 1.2650)\n","Saving model (epoch =  339, train loss = 1.3023, dev loss = 1.2641)\n","Saving model (epoch =  340, train loss = 1.3001, dev loss = 1.2605)\n","Saving model (epoch =  341, train loss = 1.2992, dev loss = 1.2604)\n","Saving model (epoch =  342, train loss = 1.2924, dev loss = 1.2561)\n","Saving model (epoch =  345, train loss = 1.2876, dev loss = 1.2496)\n","Saving model (epoch =  346, train loss = 1.2896, dev loss = 1.2476)\n","Saving model (epoch =  347, train loss = 1.2812, dev loss = 1.2470)\n","Saving model (epoch =  348, train loss = 1.2845, dev loss = 1.2437)\n","Saving model (epoch =  350, train loss = 1.2804, dev loss = 1.2402)\n","Saving model (epoch =  353, train loss = 1.2785, dev loss = 1.2367)\n","Saving model (epoch =  354, train loss = 1.2673, dev loss = 1.2320)\n","Saving model (epoch =  355, train loss = 1.2683, dev loss = 1.2308)\n","Saving model (epoch =  356, train loss = 1.2654, dev loss = 1.2283)\n","Saving model (epoch =  357, train loss = 1.2589, dev loss = 1.2281)\n","Saving model (epoch =  358, train loss = 1.2633, dev loss = 1.2246)\n","Saving model (epoch =  359, train loss = 1.2617, dev loss = 1.2228)\n","Saving model (epoch =  363, train loss = 1.2633, dev loss = 1.2162)\n","Saving model (epoch =  365, train loss = 1.2499, dev loss = 1.2130)\n","Saving model (epoch =  367, train loss = 1.2451, dev loss = 1.2100)\n","Saving model (epoch =  369, train loss = 1.2428, dev loss = 1.2085)\n","Saving model (epoch =  372, train loss = 1.2468, dev loss = 1.2035)\n","Saving model (epoch =  375, train loss = 1.2358, dev loss = 1.1985)\n","Saving model (epoch =  377, train loss = 1.2291, dev loss = 1.1981)\n","Saving model (epoch =  378, train loss = 1.2369, dev loss = 1.1979)\n","Saving model (epoch =  379, train loss = 1.2296, dev loss = 1.1929)\n","Saving model (epoch =  380, train loss = 1.2268, dev loss = 1.1915)\n","Saving model (epoch =  382, train loss = 1.2264, dev loss = 1.1898)\n","Saving model (epoch =  385, train loss = 1.2234, dev loss = 1.1886)\n","Saving model (epoch =  386, train loss = 1.2184, dev loss = 1.1839)\n","Saving model (epoch =  387, train loss = 1.2194, dev loss = 1.1828)\n","Saving model (epoch =  390, train loss = 1.2193, dev loss = 1.1793)\n","Saving model (epoch =  393, train loss = 1.2110, dev loss = 1.1761)\n","Saving model (epoch =  395, train loss = 1.2158, dev loss = 1.1750)\n","Saving model (epoch =  401, train loss = 1.2204, dev loss = 1.1749)\n","Saving model (epoch =  402, train loss = 1.2028, dev loss = 1.1720)\n","Saving model (epoch =  403, train loss = 1.2030, dev loss = 1.1660)\n","Saving model (epoch =  405, train loss = 1.1997, dev loss = 1.1642)\n","Saving model (epoch =  406, train loss = 1.1998, dev loss = 1.1632)\n","Saving model (epoch =  412, train loss = 1.1964, dev loss = 1.1583)\n","Saving model (epoch =  414, train loss = 1.1918, dev loss = 1.1568)\n","Saving model (epoch =  415, train loss = 1.1929, dev loss = 1.1555)\n","Saving model (epoch =  416, train loss = 1.1912, dev loss = 1.1549)\n","Saving model (epoch =  418, train loss = 1.1869, dev loss = 1.1538)\n","Saving model (epoch =  419, train loss = 1.1868, dev loss = 1.1526)\n","Saving model (epoch =  421, train loss = 1.1858, dev loss = 1.1512)\n","Saving model (epoch =  423, train loss = 1.1853, dev loss = 1.1496)\n","Saving model (epoch =  424, train loss = 1.1877, dev loss = 1.1489)\n","Saving model (epoch =  425, train loss = 1.1854, dev loss = 1.1484)\n","Saving model (epoch =  428, train loss = 1.1855, dev loss = 1.1478)\n","Saving model (epoch =  429, train loss = 1.1877, dev loss = 1.1458)\n","Saving model (epoch =  430, train loss = 1.1811, dev loss = 1.1451)\n","Saving model (epoch =  431, train loss = 1.1817, dev loss = 1.1446)\n","Saving model (epoch =  433, train loss = 1.1839, dev loss = 1.1431)\n","Saving model (epoch =  435, train loss = 1.1834, dev loss = 1.1421)\n","Saving model (epoch =  437, train loss = 1.1773, dev loss = 1.1410)\n","Saving model (epoch =  438, train loss = 1.1767, dev loss = 1.1401)\n","Saving model (epoch =  440, train loss = 1.1777, dev loss = 1.1392)\n","Saving model (epoch =  442, train loss = 1.1771, dev loss = 1.1381)\n","Saving model (epoch =  444, train loss = 1.1766, dev loss = 1.1368)\n","Saving model (epoch =  445, train loss = 1.1698, dev loss = 1.1365)\n","Saving model (epoch =  446, train loss = 1.1734, dev loss = 1.1356)\n","Saving model (epoch =  448, train loss = 1.1710, dev loss = 1.1354)\n","Saving model (epoch =  449, train loss = 1.1684, dev loss = 1.1345)\n","Saving model (epoch =  450, train loss = 1.1714, dev loss = 1.1337)\n","Saving model (epoch =  453, train loss = 1.1711, dev loss = 1.1336)\n","Saving model (epoch =  455, train loss = 1.1719, dev loss = 1.1313)\n","Saving model (epoch =  457, train loss = 1.1691, dev loss = 1.1301)\n","Saving model (epoch =  459, train loss = 1.1637, dev loss = 1.1292)\n","Saving model (epoch =  460, train loss = 1.1663, dev loss = 1.1291)\n","Saving model (epoch =  461, train loss = 1.1659, dev loss = 1.1289)\n","Saving model (epoch =  463, train loss = 1.1688, dev loss = 1.1275)\n","Saving model (epoch =  464, train loss = 1.1623, dev loss = 1.1271)\n","Saving model (epoch =  466, train loss = 1.1675, dev loss = 1.1266)\n","Saving model (epoch =  468, train loss = 1.1627, dev loss = 1.1259)\n","Saving model (epoch =  469, train loss = 1.1643, dev loss = 1.1258)\n","Saving model (epoch =  471, train loss = 1.1599, dev loss = 1.1242)\n","Saving model (epoch =  473, train loss = 1.1617, dev loss = 1.1241)\n","Saving model (epoch =  474, train loss = 1.1598, dev loss = 1.1229)\n","Saving model (epoch =  476, train loss = 1.1550, dev loss = 1.1227)\n","Saving model (epoch =  479, train loss = 1.1609, dev loss = 1.1211)\n","Saving model (epoch =  480, train loss = 1.1598, dev loss = 1.1209)\n","Saving model (epoch =  482, train loss = 1.1573, dev loss = 1.1204)\n","Saving model (epoch =  486, train loss = 1.1630, dev loss = 1.1194)\n","Saving model (epoch =  488, train loss = 1.1537, dev loss = 1.1185)\n","Saving model (epoch =  490, train loss = 1.1510, dev loss = 1.1173)\n","Saving model (epoch =  491, train loss = 1.1532, dev loss = 1.1173)\n","Saving model (epoch =  493, train loss = 1.1518, dev loss = 1.1162)\n","Saving model (epoch =  500, train loss = 1.1498, dev loss = 1.1149)\n","Saving model (epoch =  501, train loss = 1.1535, dev loss = 1.1137)\n","Saving model (epoch =  504, train loss = 1.1513, dev loss = 1.1129)\n","Saving model (epoch =  511, train loss = 1.1469, dev loss = 1.1117)\n","Saving model (epoch =  518, train loss = 1.1433, dev loss = 1.1088)\n","Saving model (epoch =  519, train loss = 1.1447, dev loss = 1.1084)\n","Saving model (epoch =  525, train loss = 1.1440, dev loss = 1.1079)\n","Saving model (epoch =  528, train loss = 1.1432, dev loss = 1.1060)\n","Saving model (epoch =  532, train loss = 1.1393, dev loss = 1.1058)\n","Saving model (epoch =  538, train loss = 1.1449, dev loss = 1.1053)\n","Saving model (epoch =  539, train loss = 1.1372, dev loss = 1.1033)\n","Saving model (epoch =  540, train loss = 1.1436, dev loss = 1.1032)\n","Saving model (epoch =  541, train loss = 1.1397, dev loss = 1.1029)\n","Saving model (epoch =  545, train loss = 1.1395, dev loss = 1.1025)\n","Saving model (epoch =  546, train loss = 1.1366, dev loss = 1.1021)\n","Saving model (epoch =  547, train loss = 1.1347, dev loss = 1.1016)\n","Saving model (epoch =  551, train loss = 1.1381, dev loss = 1.1007)\n","Saving model (epoch =  559, train loss = 1.1391, dev loss = 1.0999)\n","Saving model (epoch =  560, train loss = 1.1338, dev loss = 1.0987)\n","Saving model (epoch =  561, train loss = 1.1329, dev loss = 1.0985)\n","Saving model (epoch =  568, train loss = 1.1334, dev loss = 1.0984)\n","Saving model (epoch =  569, train loss = 1.1344, dev loss = 1.0969)\n","Saving model (epoch =  571, train loss = 1.1322, dev loss = 1.0965)\n","Saving model (epoch =  576, train loss = 1.1319, dev loss = 1.0956)\n","Saving model (epoch =  577, train loss = 1.1338, dev loss = 1.0956)\n","Saving model (epoch =  581, train loss = 1.1329, dev loss = 1.0953)\n","Saving model (epoch =  585, train loss = 1.1426, dev loss = 1.0946)\n","Saving model (epoch =  586, train loss = 1.1358, dev loss = 1.0941)\n","Saving model (epoch =  589, train loss = 1.1301, dev loss = 1.0930)\n","Saving model (epoch =  590, train loss = 1.1337, dev loss = 1.0928)\n","Saving model (epoch =  593, train loss = 1.1321, dev loss = 1.0922)\n","Saving model (epoch =  598, train loss = 1.1280, dev loss = 1.0913)\n","Saving model (epoch =  604, train loss = 1.1262, dev loss = 1.0903)\n","Saving model (epoch =  605, train loss = 1.1264, dev loss = 1.0901)\n","Saving model (epoch =  609, train loss = 1.1259, dev loss = 1.0897)\n","Saving model (epoch =  612, train loss = 1.1253, dev loss = 1.0896)\n","Saving model (epoch =  616, train loss = 1.1230, dev loss = 1.0883)\n","Saving model (epoch =  618, train loss = 1.1256, dev loss = 1.0880)\n","Saving model (epoch =  623, train loss = 1.1225, dev loss = 1.0875)\n","Saving model (epoch =  627, train loss = 1.1260, dev loss = 1.0867)\n","Saving model (epoch =  630, train loss = 1.1229, dev loss = 1.0865)\n","Saving model (epoch =  632, train loss = 1.1205, dev loss = 1.0857)\n","Saving model (epoch =  633, train loss = 1.1205, dev loss = 1.0857)\n","Saving model (epoch =  637, train loss = 1.1228, dev loss = 1.0856)\n","Saving model (epoch =  640, train loss = 1.1187, dev loss = 1.0851)\n","Saving model (epoch =  644, train loss = 1.1239, dev loss = 1.0844)\n","Saving model (epoch =  648, train loss = 1.1176, dev loss = 1.0834)\n","Saving model (epoch =  654, train loss = 1.1195, dev loss = 1.0823)\n","Saving model (epoch =  659, train loss = 1.1179, dev loss = 1.0816)\n","Saving model (epoch =  662, train loss = 1.1181, dev loss = 1.0816)\n","Saving model (epoch =  664, train loss = 1.1167, dev loss = 1.0810)\n","Saving model (epoch =  666, train loss = 1.1182, dev loss = 1.0806)\n","Saving model (epoch =  673, train loss = 1.1194, dev loss = 1.0805)\n","Saving model (epoch =  674, train loss = 1.1128, dev loss = 1.0796)\n","Saving model (epoch =  675, train loss = 1.1131, dev loss = 1.0793)\n","Saving model (epoch =  676, train loss = 1.1159, dev loss = 1.0792)\n","Saving model (epoch =  679, train loss = 1.1162, dev loss = 1.0789)\n","Saving model (epoch =  684, train loss = 1.1160, dev loss = 1.0786)\n","Saving model (epoch =  686, train loss = 1.1109, dev loss = 1.0777)\n","Saving model (epoch =  687, train loss = 1.1150, dev loss = 1.0776)\n","Saving model (epoch =  688, train loss = 1.1149, dev loss = 1.0775)\n","Saving model (epoch =  694, train loss = 1.1209, dev loss = 1.0769)\n","Saving model (epoch =  695, train loss = 1.1120, dev loss = 1.0765)\n","Saving model (epoch =  696, train loss = 1.1138, dev loss = 1.0765)\n","Saving model (epoch =  699, train loss = 1.1178, dev loss = 1.0761)\n","Saving model (epoch =  700, train loss = 1.1129, dev loss = 1.0759)\n","Saving model (epoch =  702, train loss = 1.1129, dev loss = 1.0757)\n","Saving model (epoch =  703, train loss = 1.1110, dev loss = 1.0755)\n","Saving model (epoch =  704, train loss = 1.1126, dev loss = 1.0755)\n","Saving model (epoch =  707, train loss = 1.1128, dev loss = 1.0752)\n","Saving model (epoch =  710, train loss = 1.1123, dev loss = 1.0745)\n","Saving model (epoch =  714, train loss = 1.1092, dev loss = 1.0741)\n","Saving model (epoch =  716, train loss = 1.1102, dev loss = 1.0738)\n","Saving model (epoch =  720, train loss = 1.1128, dev loss = 1.0731)\n","Saving model (epoch =  729, train loss = 1.1086, dev loss = 1.0725)\n","Saving model (epoch =  730, train loss = 1.1063, dev loss = 1.0721)\n","Saving model (epoch =  734, train loss = 1.1089, dev loss = 1.0711)\n","Saving model (epoch =  741, train loss = 1.1147, dev loss = 1.0702)\n","Saving model (epoch =  750, train loss = 1.1073, dev loss = 1.0692)\n","Saving model (epoch =  755, train loss = 1.1073, dev loss = 1.0686)\n","Saving model (epoch =  758, train loss = 1.1065, dev loss = 1.0681)\n","Saving model (epoch =  760, train loss = 1.1042, dev loss = 1.0678)\n","Saving model (epoch =  762, train loss = 1.1040, dev loss = 1.0678)\n","Saving model (epoch =  764, train loss = 1.1028, dev loss = 1.0673)\n","Saving model (epoch =  769, train loss = 1.0998, dev loss = 1.0666)\n","Saving model (epoch =  774, train loss = 1.1033, dev loss = 1.0661)\n","Saving model (epoch =  775, train loss = 1.1024, dev loss = 1.0659)\n","Saving model (epoch =  778, train loss = 1.1031, dev loss = 1.0656)\n","Saving model (epoch =  779, train loss = 1.1009, dev loss = 1.0655)\n","Saving model (epoch =  780, train loss = 1.1023, dev loss = 1.0653)\n","Saving model (epoch =  782, train loss = 1.0995, dev loss = 1.0650)\n","Saving model (epoch =  784, train loss = 1.1014, dev loss = 1.0648)\n","Saving model (epoch =  785, train loss = 1.1010, dev loss = 1.0647)\n","Saving model (epoch =  789, train loss = 1.0998, dev loss = 1.0643)\n","Saving model (epoch =  793, train loss = 1.0995, dev loss = 1.0642)\n","Saving model (epoch =  794, train loss = 1.0995, dev loss = 1.0637)\n","Saving model (epoch =  797, train loss = 1.1044, dev loss = 1.0632)\n","Saving model (epoch =  799, train loss = 1.0983, dev loss = 1.0630)\n","Saving model (epoch =  803, train loss = 1.0998, dev loss = 1.0627)\n","Saving model (epoch =  804, train loss = 1.1043, dev loss = 1.0624)\n","Saving model (epoch =  808, train loss = 1.0993, dev loss = 1.0619)\n","Saving model (epoch =  814, train loss = 1.1015, dev loss = 1.0618)\n","Saving model (epoch =  815, train loss = 1.1002, dev loss = 1.0611)\n","Saving model (epoch =  818, train loss = 1.0952, dev loss = 1.0611)\n","Saving model (epoch =  824, train loss = 1.0959, dev loss = 1.0600)\n","Saving model (epoch =  827, train loss = 1.0944, dev loss = 1.0600)\n","Saving model (epoch =  829, train loss = 1.1022, dev loss = 1.0595)\n","Saving model (epoch =  834, train loss = 1.0966, dev loss = 1.0588)\n","Saving model (epoch =  835, train loss = 1.0954, dev loss = 1.0588)\n","Saving model (epoch =  839, train loss = 1.0957, dev loss = 1.0585)\n","Saving model (epoch =  843, train loss = 1.0923, dev loss = 1.0578)\n","Saving model (epoch =  851, train loss = 1.0952, dev loss = 1.0568)\n","Saving model (epoch =  854, train loss = 1.0924, dev loss = 1.0568)\n","Saving model (epoch =  856, train loss = 1.0892, dev loss = 1.0563)\n","Saving model (epoch =  862, train loss = 1.0934, dev loss = 1.0559)\n","Saving model (epoch =  867, train loss = 1.0903, dev loss = 1.0558)\n","Saving model (epoch =  869, train loss = 1.0870, dev loss = 1.0548)\n","Saving model (epoch =  871, train loss = 1.0912, dev loss = 1.0547)\n","Saving model (epoch =  876, train loss = 1.0887, dev loss = 1.0541)\n","Saving model (epoch =  880, train loss = 1.0924, dev loss = 1.0536)\n","Saving model (epoch =  884, train loss = 1.0910, dev loss = 1.0533)\n","Saving model (epoch =  885, train loss = 1.0883, dev loss = 1.0532)\n","Saving model (epoch =  886, train loss = 1.0873, dev loss = 1.0529)\n","Saving model (epoch =  892, train loss = 1.0931, dev loss = 1.0523)\n","Saving model (epoch =  895, train loss = 1.0881, dev loss = 1.0520)\n","Saving model (epoch =  903, train loss = 1.0883, dev loss = 1.0514)\n","Saving model (epoch =  905, train loss = 1.0918, dev loss = 1.0514)\n","Saving model (epoch =  906, train loss = 1.0877, dev loss = 1.0508)\n","Saving model (epoch =  908, train loss = 1.0866, dev loss = 1.0508)\n","Saving model (epoch =  910, train loss = 1.0881, dev loss = 1.0505)\n","Saving model (epoch =  911, train loss = 1.0870, dev loss = 1.0504)\n","Saving model (epoch =  917, train loss = 1.0926, dev loss = 1.0500)\n","Saving model (epoch =  925, train loss = 1.0860, dev loss = 1.0488)\n","Saving model (epoch =  933, train loss = 1.0851, dev loss = 1.0480)\n","Saving model (epoch =  936, train loss = 1.0840, dev loss = 1.0478)\n","Saving model (epoch =  938, train loss = 1.0854, dev loss = 1.0474)\n","Saving model (epoch =  943, train loss = 1.0852, dev loss = 1.0471)\n","Saving model (epoch =  946, train loss = 1.0815, dev loss = 1.0470)\n","Saving model (epoch =  952, train loss = 1.0812, dev loss = 1.0467)\n","Saving model (epoch =  953, train loss = 1.0792, dev loss = 1.0458)\n","Saving model (epoch =  954, train loss = 1.0829, dev loss = 1.0456)\n","Saving model (epoch =  955, train loss = 1.0828, dev loss = 1.0455)\n","Saving model (epoch =  957, train loss = 1.0797, dev loss = 1.0454)\n","Saving model (epoch =  960, train loss = 1.0806, dev loss = 1.0453)\n","Saving model (epoch =  961, train loss = 1.0803, dev loss = 1.0452)\n","Saving model (epoch =  964, train loss = 1.0905, dev loss = 1.0450)\n","Saving model (epoch =  966, train loss = 1.0790, dev loss = 1.0444)\n","Saving model (epoch =  969, train loss = 1.0803, dev loss = 1.0442)\n","Saving model (epoch =  972, train loss = 1.0824, dev loss = 1.0440)\n","Saving model (epoch =  973, train loss = 1.0792, dev loss = 1.0438)\n","Saving model (epoch =  978, train loss = 1.0804, dev loss = 1.0432)\n","Saving model (epoch =  982, train loss = 1.0779, dev loss = 1.0428)\n","Saving model (epoch =  986, train loss = 1.0785, dev loss = 1.0424)\n","Saving model (epoch =  987, train loss = 1.0767, dev loss = 1.0423)\n","Saving model (epoch =  990, train loss = 1.0779, dev loss = 1.0420)\n","Saving model (epoch =  996, train loss = 1.0792, dev loss = 1.0419)\n","Saving model (epoch =  998, train loss = 1.0778, dev loss = 1.0414)\n","Saving model (epoch = 1001, train loss = 1.0745, dev loss = 1.0409)\n","Saving model (epoch = 1002, train loss = 1.0758, dev loss = 1.0408)\n","Saving model (epoch = 1003, train loss = 1.0782, dev loss = 1.0407)\n","Saving model (epoch = 1009, train loss = 1.0761, dev loss = 1.0402)\n","Saving model (epoch = 1014, train loss = 1.0758, dev loss = 1.0398)\n","Saving model (epoch = 1015, train loss = 1.0766, dev loss = 1.0395)\n","Saving model (epoch = 1018, train loss = 1.0789, dev loss = 1.0392)\n","Saving model (epoch = 1023, train loss = 1.0747, dev loss = 1.0389)\n","Saving model (epoch = 1031, train loss = 1.0734, dev loss = 1.0386)\n","Saving model (epoch = 1035, train loss = 1.0698, dev loss = 1.0382)\n","Saving model (epoch = 1036, train loss = 1.0727, dev loss = 1.0377)\n","Saving model (epoch = 1037, train loss = 1.0720, dev loss = 1.0375)\n","Saving model (epoch = 1046, train loss = 1.0738, dev loss = 1.0365)\n","Saving model (epoch = 1057, train loss = 1.0688, dev loss = 1.0356)\n","Saving model (epoch = 1059, train loss = 1.0731, dev loss = 1.0353)\n","Saving model (epoch = 1063, train loss = 1.0713, dev loss = 1.0353)\n","Saving model (epoch = 1068, train loss = 1.0698, dev loss = 1.0351)\n","Saving model (epoch = 1071, train loss = 1.0721, dev loss = 1.0348)\n","Saving model (epoch = 1072, train loss = 1.0738, dev loss = 1.0346)\n","Saving model (epoch = 1073, train loss = 1.0671, dev loss = 1.0341)\n","Saving model (epoch = 1076, train loss = 1.0723, dev loss = 1.0338)\n","Saving model (epoch = 1082, train loss = 1.0677, dev loss = 1.0332)\n","Saving model (epoch = 1086, train loss = 1.0695, dev loss = 1.0329)\n","Saving model (epoch = 1092, train loss = 1.0691, dev loss = 1.0324)\n","Saving model (epoch = 1093, train loss = 1.0676, dev loss = 1.0322)\n","Saving model (epoch = 1106, train loss = 1.0680, dev loss = 1.0310)\n","Saving model (epoch = 1116, train loss = 1.0654, dev loss = 1.0301)\n","Saving model (epoch = 1119, train loss = 1.0657, dev loss = 1.0298)\n","Saving model (epoch = 1120, train loss = 1.0683, dev loss = 1.0297)\n","Saving model (epoch = 1121, train loss = 1.0647, dev loss = 1.0296)\n","Saving model (epoch = 1129, train loss = 1.0676, dev loss = 1.0290)\n","Saving model (epoch = 1136, train loss = 1.0657, dev loss = 1.0288)\n","Saving model (epoch = 1137, train loss = 1.0639, dev loss = 1.0285)\n","Saving model (epoch = 1138, train loss = 1.0631, dev loss = 1.0281)\n","Saving model (epoch = 1143, train loss = 1.0639, dev loss = 1.0280)\n","Saving model (epoch = 1146, train loss = 1.0658, dev loss = 1.0273)\n","Saving model (epoch = 1147, train loss = 1.0637, dev loss = 1.0273)\n","Saving model (epoch = 1152, train loss = 1.0605, dev loss = 1.0271)\n","Saving model (epoch = 1156, train loss = 1.0652, dev loss = 1.0271)\n","Saving model (epoch = 1158, train loss = 1.0616, dev loss = 1.0270)\n","Saving model (epoch = 1162, train loss = 1.0616, dev loss = 1.0265)\n","Saving model (epoch = 1164, train loss = 1.0622, dev loss = 1.0260)\n","Saving model (epoch = 1166, train loss = 1.0636, dev loss = 1.0256)\n","Saving model (epoch = 1170, train loss = 1.0606, dev loss = 1.0253)\n","Saving model (epoch = 1171, train loss = 1.0623, dev loss = 1.0253)\n","Saving model (epoch = 1175, train loss = 1.0597, dev loss = 1.0250)\n","Saving model (epoch = 1177, train loss = 1.0590, dev loss = 1.0249)\n","Saving model (epoch = 1180, train loss = 1.0597, dev loss = 1.0245)\n","Saving model (epoch = 1187, train loss = 1.0622, dev loss = 1.0240)\n","Saving model (epoch = 1192, train loss = 1.0627, dev loss = 1.0235)\n","Saving model (epoch = 1198, train loss = 1.0635, dev loss = 1.0233)\n","Saving model (epoch = 1200, train loss = 1.0578, dev loss = 1.0233)\n","Saving model (epoch = 1201, train loss = 1.0550, dev loss = 1.0229)\n","Saving model (epoch = 1204, train loss = 1.0596, dev loss = 1.0224)\n","Saving model (epoch = 1209, train loss = 1.0556, dev loss = 1.0220)\n","Saving model (epoch = 1211, train loss = 1.0560, dev loss = 1.0218)\n","Saving model (epoch = 1216, train loss = 1.0530, dev loss = 1.0215)\n","Saving model (epoch = 1224, train loss = 1.0629, dev loss = 1.0208)\n","Saving model (epoch = 1225, train loss = 1.0537, dev loss = 1.0207)\n","Saving model (epoch = 1231, train loss = 1.0578, dev loss = 1.0202)\n","Saving model (epoch = 1236, train loss = 1.0574, dev loss = 1.0199)\n","Saving model (epoch = 1249, train loss = 1.0560, dev loss = 1.0188)\n","Saving model (epoch = 1250, train loss = 1.0509, dev loss = 1.0186)\n","Saving model (epoch = 1253, train loss = 1.0584, dev loss = 1.0184)\n","Saving model (epoch = 1256, train loss = 1.0555, dev loss = 1.0183)\n","Saving model (epoch = 1257, train loss = 1.0560, dev loss = 1.0181)\n","Saving model (epoch = 1259, train loss = 1.0560, dev loss = 1.0180)\n","Saving model (epoch = 1265, train loss = 1.0548, dev loss = 1.0176)\n","Saving model (epoch = 1268, train loss = 1.0579, dev loss = 1.0172)\n","Saving model (epoch = 1277, train loss = 1.0521, dev loss = 1.0171)\n","Saving model (epoch = 1281, train loss = 1.0516, dev loss = 1.0162)\n","Saving model (epoch = 1288, train loss = 1.0489, dev loss = 1.0157)\n","Saving model (epoch = 1297, train loss = 1.0495, dev loss = 1.0152)\n","Saving model (epoch = 1300, train loss = 1.0518, dev loss = 1.0148)\n","Saving model (epoch = 1307, train loss = 1.0493, dev loss = 1.0142)\n","Saving model (epoch = 1309, train loss = 1.0492, dev loss = 1.0140)\n","Saving model (epoch = 1315, train loss = 1.0508, dev loss = 1.0140)\n","Saving model (epoch = 1317, train loss = 1.0516, dev loss = 1.0136)\n","Saving model (epoch = 1319, train loss = 1.0474, dev loss = 1.0133)\n","Saving model (epoch = 1324, train loss = 1.0521, dev loss = 1.0131)\n","Saving model (epoch = 1327, train loss = 1.0494, dev loss = 1.0130)\n","Saving model (epoch = 1328, train loss = 1.0476, dev loss = 1.0125)\n","Saving model (epoch = 1338, train loss = 1.0461, dev loss = 1.0118)\n","Saving model (epoch = 1341, train loss = 1.0501, dev loss = 1.0115)\n","Saving model (epoch = 1355, train loss = 1.0470, dev loss = 1.0110)\n","Saving model (epoch = 1356, train loss = 1.0456, dev loss = 1.0105)\n","Saving model (epoch = 1357, train loss = 1.0445, dev loss = 1.0104)\n","Saving model (epoch = 1364, train loss = 1.0475, dev loss = 1.0099)\n","Saving model (epoch = 1373, train loss = 1.0447, dev loss = 1.0095)\n","Saving model (epoch = 1382, train loss = 1.0497, dev loss = 1.0093)\n","Saving model (epoch = 1384, train loss = 1.0466, dev loss = 1.0087)\n","Saving model (epoch = 1388, train loss = 1.0478, dev loss = 1.0081)\n","Saving model (epoch = 1395, train loss = 1.0476, dev loss = 1.0081)\n","Saving model (epoch = 1398, train loss = 1.0446, dev loss = 1.0075)\n","Saving model (epoch = 1399, train loss = 1.0433, dev loss = 1.0074)\n","Saving model (epoch = 1406, train loss = 1.0442, dev loss = 1.0070)\n","Saving model (epoch = 1410, train loss = 1.0438, dev loss = 1.0067)\n","Saving model (epoch = 1413, train loss = 1.0442, dev loss = 1.0063)\n","Saving model (epoch = 1417, train loss = 1.0414, dev loss = 1.0063)\n","Saving model (epoch = 1418, train loss = 1.0421, dev loss = 1.0061)\n","Saving model (epoch = 1423, train loss = 1.0379, dev loss = 1.0059)\n","Saving model (epoch = 1429, train loss = 1.0400, dev loss = 1.0052)\n","Saving model (epoch = 1431, train loss = 1.0389, dev loss = 1.0051)\n","Saving model (epoch = 1437, train loss = 1.0381, dev loss = 1.0047)\n","Saving model (epoch = 1442, train loss = 1.0456, dev loss = 1.0046)\n","Saving model (epoch = 1448, train loss = 1.0386, dev loss = 1.0039)\n","Saving model (epoch = 1455, train loss = 1.0401, dev loss = 1.0035)\n","Saving model (epoch = 1458, train loss = 1.0410, dev loss = 1.0032)\n","Saving model (epoch = 1462, train loss = 1.0383, dev loss = 1.0030)\n","Saving model (epoch = 1469, train loss = 1.0371, dev loss = 1.0026)\n","Saving model (epoch = 1470, train loss = 1.0391, dev loss = 1.0024)\n","Saving model (epoch = 1471, train loss = 1.0367, dev loss = 1.0023)\n","Saving model (epoch = 1479, train loss = 1.0341, dev loss = 1.0021)\n","Saving model (epoch = 1480, train loss = 1.0340, dev loss = 1.0020)\n","Saving model (epoch = 1481, train loss = 1.0376, dev loss = 1.0017)\n","Saving model (epoch = 1483, train loss = 1.0373, dev loss = 1.0015)\n","Saving model (epoch = 1485, train loss = 1.0362, dev loss = 1.0015)\n","Saving model (epoch = 1486, train loss = 1.0382, dev loss = 1.0013)\n","Saving model (epoch = 1492, train loss = 1.0356, dev loss = 1.0012)\n","Saving model (epoch = 1493, train loss = 1.0354, dev loss = 1.0009)\n","Saving model (epoch = 1494, train loss = 1.0374, dev loss = 1.0008)\n","Saving model (epoch = 1500, train loss = 1.0358, dev loss = 1.0004)\n","Saving model (epoch = 1512, train loss = 1.0315, dev loss = 0.9997)\n","Saving model (epoch = 1515, train loss = 1.0336, dev loss = 0.9995)\n","Saving model (epoch = 1522, train loss = 1.0324, dev loss = 0.9991)\n","Saving model (epoch = 1523, train loss = 1.0340, dev loss = 0.9991)\n","Saving model (epoch = 1530, train loss = 1.0315, dev loss = 0.9985)\n","Saving model (epoch = 1531, train loss = 1.0333, dev loss = 0.9985)\n","Saving model (epoch = 1541, train loss = 1.0316, dev loss = 0.9983)\n","Saving model (epoch = 1543, train loss = 1.0338, dev loss = 0.9979)\n","Saving model (epoch = 1546, train loss = 1.0308, dev loss = 0.9979)\n","Saving model (epoch = 1551, train loss = 1.0316, dev loss = 0.9972)\n","Saving model (epoch = 1556, train loss = 1.0328, dev loss = 0.9971)\n","Saving model (epoch = 1561, train loss = 1.0316, dev loss = 0.9967)\n","Saving model (epoch = 1576, train loss = 1.0319, dev loss = 0.9957)\n","Saving model (epoch = 1577, train loss = 1.0283, dev loss = 0.9957)\n","Saving model (epoch = 1581, train loss = 1.0314, dev loss = 0.9957)\n","Saving model (epoch = 1587, train loss = 1.0293, dev loss = 0.9951)\n","Saving model (epoch = 1588, train loss = 1.0284, dev loss = 0.9950)\n","Saving model (epoch = 1592, train loss = 1.0285, dev loss = 0.9948)\n","Saving model (epoch = 1593, train loss = 1.0330, dev loss = 0.9947)\n","Saving model (epoch = 1603, train loss = 1.0306, dev loss = 0.9942)\n","Saving model (epoch = 1607, train loss = 1.0292, dev loss = 0.9939)\n","Saving model (epoch = 1609, train loss = 1.0326, dev loss = 0.9939)\n","Saving model (epoch = 1612, train loss = 1.0275, dev loss = 0.9936)\n","Saving model (epoch = 1619, train loss = 1.0282, dev loss = 0.9933)\n","Saving model (epoch = 1627, train loss = 1.0304, dev loss = 0.9927)\n","Saving model (epoch = 1628, train loss = 1.0266, dev loss = 0.9927)\n","Saving model (epoch = 1636, train loss = 1.0282, dev loss = 0.9922)\n","Saving model (epoch = 1639, train loss = 1.0253, dev loss = 0.9920)\n","Saving model (epoch = 1641, train loss = 1.0270, dev loss = 0.9919)\n","Saving model (epoch = 1642, train loss = 1.0262, dev loss = 0.9919)\n","Saving model (epoch = 1649, train loss = 1.0284, dev loss = 0.9917)\n","Saving model (epoch = 1652, train loss = 1.0330, dev loss = 0.9913)\n","Saving model (epoch = 1660, train loss = 1.0256, dev loss = 0.9913)\n","Saving model (epoch = 1662, train loss = 1.0235, dev loss = 0.9910)\n","Saving model (epoch = 1664, train loss = 1.0319, dev loss = 0.9908)\n","Saving model (epoch = 1667, train loss = 1.0299, dev loss = 0.9904)\n","Saving model (epoch = 1674, train loss = 1.0249, dev loss = 0.9901)\n","Saving model (epoch = 1683, train loss = 1.0270, dev loss = 0.9896)\n","Saving model (epoch = 1690, train loss = 1.0252, dev loss = 0.9893)\n","Saving model (epoch = 1693, train loss = 1.0253, dev loss = 0.9891)\n","Saving model (epoch = 1700, train loss = 1.0267, dev loss = 0.9887)\n","Saving model (epoch = 1706, train loss = 1.0287, dev loss = 0.9883)\n","Saving model (epoch = 1709, train loss = 1.0283, dev loss = 0.9882)\n","Saving model (epoch = 1713, train loss = 1.0220, dev loss = 0.9880)\n","Saving model (epoch = 1716, train loss = 1.0214, dev loss = 0.9880)\n","Saving model (epoch = 1719, train loss = 1.0221, dev loss = 0.9877)\n","Saving model (epoch = 1727, train loss = 1.0229, dev loss = 0.9873)\n","Saving model (epoch = 1730, train loss = 1.0197, dev loss = 0.9872)\n","Saving model (epoch = 1739, train loss = 1.0212, dev loss = 0.9866)\n","Saving model (epoch = 1741, train loss = 1.0224, dev loss = 0.9864)\n","Saving model (epoch = 1749, train loss = 1.0224, dev loss = 0.9861)\n","Saving model (epoch = 1755, train loss = 1.0202, dev loss = 0.9860)\n","Saving model (epoch = 1756, train loss = 1.0181, dev loss = 0.9856)\n","Saving model (epoch = 1758, train loss = 1.0196, dev loss = 0.9856)\n","Saving model (epoch = 1762, train loss = 1.0181, dev loss = 0.9853)\n","Saving model (epoch = 1765, train loss = 1.0211, dev loss = 0.9851)\n","Saving model (epoch = 1770, train loss = 1.0202, dev loss = 0.9849)\n","Saving model (epoch = 1778, train loss = 1.0183, dev loss = 0.9847)\n","Saving model (epoch = 1779, train loss = 1.0168, dev loss = 0.9844)\n","Saving model (epoch = 1791, train loss = 1.0211, dev loss = 0.9838)\n","Saving model (epoch = 1800, train loss = 1.0169, dev loss = 0.9835)\n","Saving model (epoch = 1803, train loss = 1.0167, dev loss = 0.9832)\n","Saving model (epoch = 1806, train loss = 1.0172, dev loss = 0.9830)\n","Saving model (epoch = 1808, train loss = 1.0177, dev loss = 0.9830)\n","Saving model (epoch = 1810, train loss = 1.0190, dev loss = 0.9829)\n","Saving model (epoch = 1814, train loss = 1.0181, dev loss = 0.9829)\n","Saving model (epoch = 1815, train loss = 1.0136, dev loss = 0.9828)\n","Saving model (epoch = 1816, train loss = 1.0169, dev loss = 0.9826)\n","Saving model (epoch = 1823, train loss = 1.0169, dev loss = 0.9822)\n","Saving model (epoch = 1825, train loss = 1.0192, dev loss = 0.9821)\n","Saving model (epoch = 1827, train loss = 1.0175, dev loss = 0.9820)\n","Saving model (epoch = 1831, train loss = 1.0189, dev loss = 0.9818)\n","Saving model (epoch = 1837, train loss = 1.0160, dev loss = 0.9815)\n","Saving model (epoch = 1843, train loss = 1.0177, dev loss = 0.9813)\n","Saving model (epoch = 1845, train loss = 1.0195, dev loss = 0.9812)\n","Saving model (epoch = 1846, train loss = 1.0140, dev loss = 0.9810)\n","Saving model (epoch = 1855, train loss = 1.0154, dev loss = 0.9807)\n","Saving model (epoch = 1858, train loss = 1.0141, dev loss = 0.9807)\n","Saving model (epoch = 1862, train loss = 1.0167, dev loss = 0.9805)\n","Saving model (epoch = 1868, train loss = 1.0204, dev loss = 0.9800)\n","Saving model (epoch = 1874, train loss = 1.0133, dev loss = 0.9799)\n","Saving model (epoch = 1876, train loss = 1.0142, dev loss = 0.9796)\n","Saving model (epoch = 1881, train loss = 1.0129, dev loss = 0.9793)\n","Saving model (epoch = 1883, train loss = 1.0124, dev loss = 0.9792)\n","Saving model (epoch = 1890, train loss = 1.0142, dev loss = 0.9788)\n","Saving model (epoch = 1892, train loss = 1.0130, dev loss = 0.9787)\n","Saving model (epoch = 1901, train loss = 1.0120, dev loss = 0.9783)\n","Saving model (epoch = 1905, train loss = 1.0126, dev loss = 0.9782)\n","Saving model (epoch = 1908, train loss = 1.0120, dev loss = 0.9780)\n","Saving model (epoch = 1909, train loss = 1.0136, dev loss = 0.9779)\n","Saving model (epoch = 1914, train loss = 1.0152, dev loss = 0.9777)\n","Saving model (epoch = 1918, train loss = 1.0113, dev loss = 0.9776)\n","Saving model (epoch = 1922, train loss = 1.0065, dev loss = 0.9775)\n","Saving model (epoch = 1926, train loss = 1.0212, dev loss = 0.9773)\n","Saving model (epoch = 1927, train loss = 1.0119, dev loss = 0.9772)\n","Saving model (epoch = 1933, train loss = 1.0129, dev loss = 0.9771)\n","Saving model (epoch = 1943, train loss = 1.0109, dev loss = 0.9769)\n","Saving model (epoch = 1945, train loss = 1.0080, dev loss = 0.9766)\n","Saving model (epoch = 1948, train loss = 1.0094, dev loss = 0.9764)\n","Saving model (epoch = 1950, train loss = 1.0116, dev loss = 0.9762)\n","Saving model (epoch = 1956, train loss = 1.0071, dev loss = 0.9759)\n","Saving model (epoch = 1962, train loss = 1.0108, dev loss = 0.9757)\n","Saving model (epoch = 1964, train loss = 1.0148, dev loss = 0.9756)\n","Saving model (epoch = 1965, train loss = 1.0153, dev loss = 0.9755)\n","Saving model (epoch = 1968, train loss = 1.0077, dev loss = 0.9754)\n","Saving model (epoch = 1973, train loss = 1.0119, dev loss = 0.9753)\n","Saving model (epoch = 1983, train loss = 1.0101, dev loss = 0.9750)\n","Saving model (epoch = 1985, train loss = 1.0089, dev loss = 0.9746)\n","Saving model (epoch = 1986, train loss = 1.0085, dev loss = 0.9746)\n","Saving model (epoch = 1988, train loss = 1.0107, dev loss = 0.9745)\n","Saving model (epoch = 1994, train loss = 1.0069, dev loss = 0.9743)\n","Saving model (epoch = 2002, train loss = 1.0091, dev loss = 0.9739)\n","Saving model (epoch = 2012, train loss = 1.0178, dev loss = 0.9737)\n","Saving model (epoch = 2015, train loss = 1.0070, dev loss = 0.9734)\n","Saving model (epoch = 2018, train loss = 1.0058, dev loss = 0.9733)\n","Saving model (epoch = 2022, train loss = 1.0066, dev loss = 0.9732)\n","Saving model (epoch = 2025, train loss = 1.0114, dev loss = 0.9730)\n","Saving model (epoch = 2027, train loss = 1.0037, dev loss = 0.9730)\n","Saving model (epoch = 2030, train loss = 1.0076, dev loss = 0.9728)\n","Saving model (epoch = 2037, train loss = 1.0072, dev loss = 0.9726)\n","Saving model (epoch = 2038, train loss = 1.0066, dev loss = 0.9725)\n","Saving model (epoch = 2039, train loss = 1.0088, dev loss = 0.9725)\n","Saving model (epoch = 2040, train loss = 1.0062, dev loss = 0.9724)\n","Saving model (epoch = 2044, train loss = 1.0029, dev loss = 0.9723)\n","Saving model (epoch = 2046, train loss = 1.0081, dev loss = 0.9722)\n","Saving model (epoch = 2054, train loss = 1.0052, dev loss = 0.9719)\n","Saving model (epoch = 2055, train loss = 1.0063, dev loss = 0.9718)\n","Saving model (epoch = 2060, train loss = 1.0059, dev loss = 0.9716)\n","Saving model (epoch = 2068, train loss = 1.0087, dev loss = 0.9713)\n","Saving model (epoch = 2072, train loss = 1.0065, dev loss = 0.9712)\n","Saving model (epoch = 2079, train loss = 1.0053, dev loss = 0.9711)\n","Saving model (epoch = 2090, train loss = 1.0040, dev loss = 0.9705)\n","Saving model (epoch = 2091, train loss = 1.0043, dev loss = 0.9704)\n","Saving model (epoch = 2094, train loss = 1.0032, dev loss = 0.9704)\n","Saving model (epoch = 2101, train loss = 1.0049, dev loss = 0.9700)\n","Saving model (epoch = 2114, train loss = 1.0026, dev loss = 0.9696)\n","Saving model (epoch = 2116, train loss = 1.0057, dev loss = 0.9696)\n","Saving model (epoch = 2118, train loss = 1.0038, dev loss = 0.9694)\n","Saving model (epoch = 2124, train loss = 1.0025, dev loss = 0.9692)\n","Saving model (epoch = 2125, train loss = 1.0011, dev loss = 0.9692)\n","Saving model (epoch = 2126, train loss = 1.0012, dev loss = 0.9691)\n","Saving model (epoch = 2131, train loss = 1.0012, dev loss = 0.9688)\n","Saving model (epoch = 2139, train loss = 1.0013, dev loss = 0.9686)\n","Saving model (epoch = 2143, train loss = 1.0028, dev loss = 0.9685)\n","Saving model (epoch = 2154, train loss = 1.0016, dev loss = 0.9679)\n","Saving model (epoch = 2160, train loss = 1.0033, dev loss = 0.9677)\n","Saving model (epoch = 2166, train loss = 1.0025, dev loss = 0.9675)\n","Saving model (epoch = 2176, train loss = 1.0039, dev loss = 0.9674)\n","Saving model (epoch = 2181, train loss = 1.0073, dev loss = 0.9669)\n","Saving model (epoch = 2188, train loss = 1.0001, dev loss = 0.9667)\n","Saving model (epoch = 2189, train loss = 0.9997, dev loss = 0.9667)\n","Saving model (epoch = 2193, train loss = 1.0001, dev loss = 0.9665)\n","Saving model (epoch = 2197, train loss = 1.0013, dev loss = 0.9664)\n","Saving model (epoch = 2203, train loss = 0.9985, dev loss = 0.9662)\n","Saving model (epoch = 2204, train loss = 0.9976, dev loss = 0.9661)\n","Saving model (epoch = 2208, train loss = 0.9994, dev loss = 0.9660)\n","Saving model (epoch = 2212, train loss = 1.0018, dev loss = 0.9658)\n","Saving model (epoch = 2213, train loss = 0.9991, dev loss = 0.9658)\n","Saving model (epoch = 2219, train loss = 1.0005, dev loss = 0.9655)\n","Saving model (epoch = 2223, train loss = 1.0033, dev loss = 0.9654)\n","Saving model (epoch = 2226, train loss = 1.0011, dev loss = 0.9653)\n","Saving model (epoch = 2228, train loss = 1.0003, dev loss = 0.9653)\n","Saving model (epoch = 2231, train loss = 0.9990, dev loss = 0.9652)\n","Saving model (epoch = 2236, train loss = 0.9977, dev loss = 0.9650)\n","Saving model (epoch = 2237, train loss = 0.9975, dev loss = 0.9649)\n","Saving model (epoch = 2245, train loss = 0.9963, dev loss = 0.9648)\n","Saving model (epoch = 2246, train loss = 0.9980, dev loss = 0.9646)\n","Saving model (epoch = 2254, train loss = 1.0054, dev loss = 0.9644)\n","Saving model (epoch = 2262, train loss = 1.0017, dev loss = 0.9641)\n","Saving model (epoch = 2267, train loss = 0.9975, dev loss = 0.9639)\n","Saving model (epoch = 2278, train loss = 0.9981, dev loss = 0.9636)\n","Saving model (epoch = 2281, train loss = 0.9987, dev loss = 0.9635)\n","Saving model (epoch = 2288, train loss = 0.9956, dev loss = 0.9634)\n","Saving model (epoch = 2289, train loss = 0.9963, dev loss = 0.9633)\n","Saving model (epoch = 2290, train loss = 0.9984, dev loss = 0.9632)\n","Saving model (epoch = 2294, train loss = 0.9961, dev loss = 0.9631)\n","Saving model (epoch = 2305, train loss = 0.9972, dev loss = 0.9628)\n","Saving model (epoch = 2309, train loss = 0.9978, dev loss = 0.9627)\n","Saving model (epoch = 2320, train loss = 0.9965, dev loss = 0.9625)\n","Saving model (epoch = 2326, train loss = 0.9956, dev loss = 0.9622)\n","Saving model (epoch = 2344, train loss = 0.9939, dev loss = 0.9617)\n","Saving model (epoch = 2354, train loss = 0.9954, dev loss = 0.9612)\n","Saving model (epoch = 2358, train loss = 0.9951, dev loss = 0.9610)\n","Saving model (epoch = 2376, train loss = 0.9940, dev loss = 0.9606)\n","Saving model (epoch = 2377, train loss = 0.9938, dev loss = 0.9605)\n","Saving model (epoch = 2378, train loss = 0.9956, dev loss = 0.9605)\n","Saving model (epoch = 2391, train loss = 0.9963, dev loss = 0.9601)\n","Saving model (epoch = 2398, train loss = 0.9971, dev loss = 0.9598)\n","Saving model (epoch = 2399, train loss = 0.9943, dev loss = 0.9598)\n","Saving model (epoch = 2408, train loss = 0.9936, dev loss = 0.9596)\n","Saving model (epoch = 2414, train loss = 1.0028, dev loss = 0.9595)\n","Saving model (epoch = 2421, train loss = 0.9940, dev loss = 0.9594)\n","Saving model (epoch = 2422, train loss = 0.9919, dev loss = 0.9592)\n","Saving model (epoch = 2441, train loss = 0.9938, dev loss = 0.9587)\n","Saving model (epoch = 2442, train loss = 0.9940, dev loss = 0.9585)\n","Saving model (epoch = 2452, train loss = 0.9925, dev loss = 0.9582)\n","Saving model (epoch = 2458, train loss = 0.9909, dev loss = 0.9581)\n","Saving model (epoch = 2461, train loss = 0.9918, dev loss = 0.9581)\n","Saving model (epoch = 2470, train loss = 0.9925, dev loss = 0.9578)\n","Saving model (epoch = 2477, train loss = 0.9915, dev loss = 0.9576)\n","Saving model (epoch = 2485, train loss = 0.9905, dev loss = 0.9573)\n","Saving model (epoch = 2489, train loss = 0.9895, dev loss = 0.9572)\n","Saving model (epoch = 2499, train loss = 0.9893, dev loss = 0.9570)\n","Saving model (epoch = 2503, train loss = 0.9893, dev loss = 0.9568)\n","Saving model (epoch = 2508, train loss = 0.9920, dev loss = 0.9567)\n","Saving model (epoch = 2515, train loss = 0.9904, dev loss = 0.9565)\n","Saving model (epoch = 2523, train loss = 0.9886, dev loss = 0.9564)\n","Saving model (epoch = 2524, train loss = 0.9897, dev loss = 0.9562)\n","Saving model (epoch = 2538, train loss = 0.9990, dev loss = 0.9559)\n","Saving model (epoch = 2539, train loss = 0.9917, dev loss = 0.9558)\n","Saving model (epoch = 2553, train loss = 0.9938, dev loss = 0.9555)\n","Saving model (epoch = 2563, train loss = 0.9872, dev loss = 0.9552)\n","Saving model (epoch = 2568, train loss = 0.9938, dev loss = 0.9550)\n","Saving model (epoch = 2569, train loss = 0.9888, dev loss = 0.9550)\n","Saving model (epoch = 2576, train loss = 0.9907, dev loss = 0.9548)\n","Saving model (epoch = 2578, train loss = 0.9853, dev loss = 0.9547)\n","Saving model (epoch = 2587, train loss = 0.9888, dev loss = 0.9547)\n","Saving model (epoch = 2590, train loss = 0.9905, dev loss = 0.9545)\n","Saving model (epoch = 2600, train loss = 0.9890, dev loss = 0.9543)\n","Saving model (epoch = 2610, train loss = 0.9880, dev loss = 0.9540)\n","Saving model (epoch = 2621, train loss = 0.9908, dev loss = 0.9538)\n","Saving model (epoch = 2626, train loss = 0.9867, dev loss = 0.9536)\n","Saving model (epoch = 2632, train loss = 0.9866, dev loss = 0.9534)\n","Saving model (epoch = 2647, train loss = 0.9894, dev loss = 0.9530)\n","Saving model (epoch = 2649, train loss = 0.9874, dev loss = 0.9529)\n","Saving model (epoch = 2651, train loss = 0.9845, dev loss = 0.9529)\n","Saving model (epoch = 2665, train loss = 0.9841, dev loss = 0.9526)\n","Saving model (epoch = 2671, train loss = 0.9870, dev loss = 0.9525)\n","Saving model (epoch = 2672, train loss = 0.9874, dev loss = 0.9524)\n","Saving model (epoch = 2677, train loss = 0.9853, dev loss = 0.9524)\n","Saving model (epoch = 2691, train loss = 0.9847, dev loss = 0.9524)\n","Saving model (epoch = 2693, train loss = 0.9857, dev loss = 0.9520)\n","Saving model (epoch = 2695, train loss = 0.9915, dev loss = 0.9519)\n","Saving model (epoch = 2697, train loss = 0.9865, dev loss = 0.9519)\n","Saving model (epoch = 2698, train loss = 0.9873, dev loss = 0.9518)\n","Saving model (epoch = 2702, train loss = 0.9856, dev loss = 0.9517)\n","Saving model (epoch = 2705, train loss = 0.9897, dev loss = 0.9516)\n","Saving model (epoch = 2713, train loss = 0.9856, dev loss = 0.9514)\n","Saving model (epoch = 2741, train loss = 0.9852, dev loss = 0.9511)\n","Saving model (epoch = 2749, train loss = 0.9862, dev loss = 0.9507)\n","Saving model (epoch = 2751, train loss = 0.9853, dev loss = 0.9506)\n","Saving model (epoch = 2756, train loss = 0.9836, dev loss = 0.9506)\n","Saving model (epoch = 2762, train loss = 0.9848, dev loss = 0.9504)\n","Saving model (epoch = 2766, train loss = 0.9837, dev loss = 0.9502)\n","Saving model (epoch = 2767, train loss = 0.9854, dev loss = 0.9502)\n","Saving model (epoch = 2775, train loss = 0.9914, dev loss = 0.9501)\n","Saving model (epoch = 2781, train loss = 0.9834, dev loss = 0.9499)\n","Saving model (epoch = 2787, train loss = 0.9840, dev loss = 0.9498)\n","Saving model (epoch = 2791, train loss = 0.9843, dev loss = 0.9497)\n","Saving model (epoch = 2808, train loss = 0.9840, dev loss = 0.9495)\n","Saving model (epoch = 2809, train loss = 0.9821, dev loss = 0.9494)\n","Saving model (epoch = 2810, train loss = 0.9814, dev loss = 0.9494)\n","Saving model (epoch = 2812, train loss = 0.9827, dev loss = 0.9493)\n","Saving model (epoch = 2821, train loss = 0.9821, dev loss = 0.9490)\n","Saving model (epoch = 2827, train loss = 0.9838, dev loss = 0.9489)\n","Saving model (epoch = 2834, train loss = 0.9828, dev loss = 0.9488)\n","Saving model (epoch = 2849, train loss = 0.9817, dev loss = 0.9486)\n","Saving model (epoch = 2850, train loss = 0.9826, dev loss = 0.9485)\n","Saving model (epoch = 2857, train loss = 0.9815, dev loss = 0.9485)\n","Saving model (epoch = 2865, train loss = 0.9872, dev loss = 0.9482)\n","Saving model (epoch = 2866, train loss = 0.9784, dev loss = 0.9481)\n","Saving model (epoch = 2872, train loss = 0.9870, dev loss = 0.9481)\n","Saving model (epoch = 2882, train loss = 0.9823, dev loss = 0.9480)\n","Saving model (epoch = 2883, train loss = 0.9823, dev loss = 0.9478)\n","Saving model (epoch = 2895, train loss = 0.9804, dev loss = 0.9475)\n","Saving model (epoch = 2904, train loss = 0.9817, dev loss = 0.9474)\n","Saving model (epoch = 2907, train loss = 0.9800, dev loss = 0.9473)\n","Saving model (epoch = 2908, train loss = 0.9797, dev loss = 0.9473)\n","Saving model (epoch = 2915, train loss = 0.9793, dev loss = 0.9472)\n","Saving model (epoch = 2917, train loss = 0.9828, dev loss = 0.9471)\n","Saving model (epoch = 2920, train loss = 0.9802, dev loss = 0.9471)\n","Saving model (epoch = 2927, train loss = 0.9809, dev loss = 0.9470)\n","Saving model (epoch = 2944, train loss = 0.9852, dev loss = 0.9466)\n","Saving model (epoch = 2955, train loss = 0.9795, dev loss = 0.9464)\n","Saving model (epoch = 2961, train loss = 0.9803, dev loss = 0.9462)\n","Saving model (epoch = 2967, train loss = 0.9783, dev loss = 0.9462)\n","Saving model (epoch = 2973, train loss = 0.9789, dev loss = 0.9460)\n","Saving model (epoch = 2982, train loss = 0.9801, dev loss = 0.9460)\n","Saving model (epoch = 2986, train loss = 0.9806, dev loss = 0.9458)\n","Saving model (epoch = 2989, train loss = 0.9783, dev loss = 0.9458)\n","Saving model (epoch = 2991, train loss = 0.9824, dev loss = 0.9457)\n","Saving model (epoch = 2994, train loss = 0.9803, dev loss = 0.9456)\n","Saving model (epoch = 2996, train loss = 0.9773, dev loss = 0.9456)\n","Saving model (epoch = 3009, train loss = 0.9796, dev loss = 0.9455)\n","Saving model (epoch = 3012, train loss = 0.9793, dev loss = 0.9454)\n","Saving model (epoch = 3024, train loss = 0.9793, dev loss = 0.9452)\n","Saving model (epoch = 3031, train loss = 0.9856, dev loss = 0.9451)\n","Saving model (epoch = 3032, train loss = 0.9805, dev loss = 0.9451)\n","Saving model (epoch = 3039, train loss = 0.9796, dev loss = 0.9450)\n","Saving model (epoch = 3044, train loss = 0.9776, dev loss = 0.9448)\n","Saving model (epoch = 3051, train loss = 0.9771, dev loss = 0.9448)\n","Saving model (epoch = 3052, train loss = 0.9776, dev loss = 0.9446)\n","Saving model (epoch = 3064, train loss = 0.9778, dev loss = 0.9445)\n","Saving model (epoch = 3071, train loss = 0.9795, dev loss = 0.9444)\n","Saving model (epoch = 3086, train loss = 0.9773, dev loss = 0.9440)\n","Saving model (epoch = 3087, train loss = 0.9783, dev loss = 0.9440)\n","Saving model (epoch = 3103, train loss = 0.9802, dev loss = 0.9439)\n","Saving model (epoch = 3113, train loss = 0.9797, dev loss = 0.9436)\n","Saving model (epoch = 3128, train loss = 0.9796, dev loss = 0.9435)\n","Saving model (epoch = 3132, train loss = 0.9756, dev loss = 0.9433)\n","Saving model (epoch = 3145, train loss = 0.9763, dev loss = 0.9432)\n","Saving model (epoch = 3151, train loss = 0.9844, dev loss = 0.9431)\n","Saving model (epoch = 3152, train loss = 0.9764, dev loss = 0.9431)\n","Saving model (epoch = 3166, train loss = 0.9771, dev loss = 0.9429)\n","Saving model (epoch = 3172, train loss = 0.9857, dev loss = 0.9426)\n","Saving model (epoch = 3177, train loss = 0.9745, dev loss = 0.9426)\n","Saving model (epoch = 3186, train loss = 0.9770, dev loss = 0.9425)\n","Saving model (epoch = 3187, train loss = 0.9756, dev loss = 0.9425)\n","Saving model (epoch = 3192, train loss = 0.9765, dev loss = 0.9424)\n","Saving model (epoch = 3193, train loss = 0.9749, dev loss = 0.9423)\n","Saving model (epoch = 3195, train loss = 0.9754, dev loss = 0.9423)\n","Saving model (epoch = 3200, train loss = 0.9778, dev loss = 0.9422)\n","Saving model (epoch = 3208, train loss = 0.9743, dev loss = 0.9421)\n","Saving model (epoch = 3210, train loss = 0.9778, dev loss = 0.9421)\n","Saving model (epoch = 3218, train loss = 0.9757, dev loss = 0.9420)\n","Saving model (epoch = 3222, train loss = 0.9784, dev loss = 0.9419)\n","Saving model (epoch = 3230, train loss = 0.9747, dev loss = 0.9419)\n","Saving model (epoch = 3233, train loss = 0.9728, dev loss = 0.9418)\n","Saving model (epoch = 3238, train loss = 0.9773, dev loss = 0.9417)\n","Saving model (epoch = 3243, train loss = 0.9743, dev loss = 0.9416)\n","Saving model (epoch = 3249, train loss = 0.9747, dev loss = 0.9415)\n","Saving model (epoch = 3259, train loss = 0.9752, dev loss = 0.9414)\n","Saving model (epoch = 3261, train loss = 0.9732, dev loss = 0.9413)\n","Saving model (epoch = 3264, train loss = 0.9759, dev loss = 0.9413)\n","Saving model (epoch = 3266, train loss = 0.9746, dev loss = 0.9413)\n","Saving model (epoch = 3267, train loss = 0.9732, dev loss = 0.9412)\n","Saving model (epoch = 3269, train loss = 0.9746, dev loss = 0.9412)\n","Saving model (epoch = 3274, train loss = 0.9743, dev loss = 0.9412)\n","Saving model (epoch = 3288, train loss = 0.9817, dev loss = 0.9410)\n","Saving model (epoch = 3302, train loss = 0.9753, dev loss = 0.9410)\n","Saving model (epoch = 3304, train loss = 0.9751, dev loss = 0.9408)\n","Saving model (epoch = 3308, train loss = 0.9746, dev loss = 0.9407)\n","Saving model (epoch = 3329, train loss = 0.9779, dev loss = 0.9404)\n","Saving model (epoch = 3342, train loss = 0.9764, dev loss = 0.9403)\n","Saving model (epoch = 3354, train loss = 0.9721, dev loss = 0.9402)\n","Saving model (epoch = 3356, train loss = 0.9742, dev loss = 0.9402)\n","Saving model (epoch = 3359, train loss = 0.9731, dev loss = 0.9400)\n","Saving model (epoch = 3360, train loss = 0.9748, dev loss = 0.9399)\n","Saving model (epoch = 3370, train loss = 0.9760, dev loss = 0.9399)\n","Saving model (epoch = 3380, train loss = 0.9735, dev loss = 0.9397)\n","Saving model (epoch = 3387, train loss = 0.9731, dev loss = 0.9396)\n","Saving model (epoch = 3398, train loss = 0.9720, dev loss = 0.9395)\n","Saving model (epoch = 3406, train loss = 0.9737, dev loss = 0.9394)\n","Saving model (epoch = 3410, train loss = 0.9703, dev loss = 0.9393)\n","Saving model (epoch = 3415, train loss = 0.9733, dev loss = 0.9393)\n","Saving model (epoch = 3416, train loss = 0.9697, dev loss = 0.9392)\n","Saving model (epoch = 3418, train loss = 0.9792, dev loss = 0.9392)\n","Saving model (epoch = 3426, train loss = 0.9725, dev loss = 0.9391)\n","Saving model (epoch = 3429, train loss = 0.9728, dev loss = 0.9391)\n","Saving model (epoch = 3442, train loss = 0.9737, dev loss = 0.9390)\n","Saving model (epoch = 3446, train loss = 0.9721, dev loss = 0.9390)\n","Saving model (epoch = 3448, train loss = 0.9714, dev loss = 0.9389)\n","Saving model (epoch = 3451, train loss = 0.9701, dev loss = 0.9388)\n","Saving model (epoch = 3452, train loss = 0.9700, dev loss = 0.9388)\n","Saving model (epoch = 3454, train loss = 0.9709, dev loss = 0.9388)\n","Saving model (epoch = 3466, train loss = 0.9732, dev loss = 0.9387)\n","Saving model (epoch = 3467, train loss = 0.9704, dev loss = 0.9386)\n","Saving model (epoch = 3487, train loss = 0.9733, dev loss = 0.9385)\n","Saving model (epoch = 3493, train loss = 0.9713, dev loss = 0.9385)\n","Saving model (epoch = 3501, train loss = 0.9720, dev loss = 0.9382)\n","Saving model (epoch = 3507, train loss = 0.9732, dev loss = 0.9381)\n","Saving model (epoch = 3513, train loss = 0.9705, dev loss = 0.9380)\n","Saving model (epoch = 3522, train loss = 0.9712, dev loss = 0.9379)\n","Saving model (epoch = 3527, train loss = 0.9697, dev loss = 0.9379)\n","Saving model (epoch = 3534, train loss = 0.9699, dev loss = 0.9379)\n","Saving model (epoch = 3539, train loss = 0.9715, dev loss = 0.9378)\n","Saving model (epoch = 3540, train loss = 0.9722, dev loss = 0.9378)\n","Saving model (epoch = 3545, train loss = 0.9705, dev loss = 0.9378)\n","Saving model (epoch = 3551, train loss = 0.9710, dev loss = 0.9378)\n","Saving model (epoch = 3554, train loss = 0.9746, dev loss = 0.9376)\n","Saving model (epoch = 3558, train loss = 0.9709, dev loss = 0.9375)\n","Saving model (epoch = 3568, train loss = 0.9736, dev loss = 0.9374)\n","Saving model (epoch = 3582, train loss = 0.9727, dev loss = 0.9374)\n","Saving model (epoch = 3585, train loss = 0.9721, dev loss = 0.9374)\n","Saving model (epoch = 3587, train loss = 0.9763, dev loss = 0.9371)\n","Saving model (epoch = 3596, train loss = 0.9691, dev loss = 0.9371)\n","Saving model (epoch = 3601, train loss = 0.9710, dev loss = 0.9369)\n","Saving model (epoch = 3627, train loss = 0.9691, dev loss = 0.9366)\n","Saving model (epoch = 3639, train loss = 0.9708, dev loss = 0.9365)\n","Saving model (epoch = 3651, train loss = 0.9720, dev loss = 0.9363)\n","Saving model (epoch = 3652, train loss = 0.9641, dev loss = 0.9363)\n","Saving model (epoch = 3662, train loss = 0.9697, dev loss = 0.9363)\n","Saving model (epoch = 3669, train loss = 0.9744, dev loss = 0.9362)\n","Saving model (epoch = 3681, train loss = 0.9678, dev loss = 0.9361)\n","Saving model (epoch = 3685, train loss = 0.9697, dev loss = 0.9360)\n","Saving model (epoch = 3687, train loss = 0.9703, dev loss = 0.9360)\n","Saving model (epoch = 3694, train loss = 0.9711, dev loss = 0.9359)\n","Saving model (epoch = 3699, train loss = 0.9662, dev loss = 0.9359)\n","Saving model (epoch = 3701, train loss = 0.9725, dev loss = 0.9358)\n","Saving model (epoch = 3720, train loss = 0.9713, dev loss = 0.9357)\n","Saving model (epoch = 3744, train loss = 0.9680, dev loss = 0.9354)\n","Saving model (epoch = 3745, train loss = 0.9691, dev loss = 0.9353)\n","Saving model (epoch = 3755, train loss = 0.9697, dev loss = 0.9353)\n","Saving model (epoch = 3774, train loss = 0.9679, dev loss = 0.9351)\n","Saving model (epoch = 3777, train loss = 0.9692, dev loss = 0.9350)\n","Saving model (epoch = 3791, train loss = 0.9659, dev loss = 0.9349)\n","Saving model (epoch = 3803, train loss = 0.9705, dev loss = 0.9348)\n","Saving model (epoch = 3811, train loss = 0.9679, dev loss = 0.9347)\n","Saving model (epoch = 3826, train loss = 0.9700, dev loss = 0.9346)\n","Saving model (epoch = 3832, train loss = 0.9656, dev loss = 0.9345)\n","Saving model (epoch = 3857, train loss = 0.9712, dev loss = 0.9343)\n","Saving model (epoch = 3861, train loss = 0.9707, dev loss = 0.9343)\n","Saving model (epoch = 3875, train loss = 0.9712, dev loss = 0.9341)\n","Saving model (epoch = 3885, train loss = 0.9657, dev loss = 0.9340)\n","Saving model (epoch = 3894, train loss = 0.9652, dev loss = 0.9339)\n","Saving model (epoch = 3899, train loss = 0.9653, dev loss = 0.9339)\n","Saving model (epoch = 3905, train loss = 0.9643, dev loss = 0.9338)\n","Saving model (epoch = 3917, train loss = 0.9664, dev loss = 0.9338)\n","Saving model (epoch = 3925, train loss = 0.9690, dev loss = 0.9337)\n","Saving model (epoch = 3937, train loss = 0.9658, dev loss = 0.9336)\n","Saving model (epoch = 3968, train loss = 0.9674, dev loss = 0.9335)\n","Saving model (epoch = 3972, train loss = 0.9662, dev loss = 0.9333)\n","Saving model (epoch = 3973, train loss = 0.9655, dev loss = 0.9332)\n","Saving model (epoch = 3977, train loss = 0.9675, dev loss = 0.9332)\n","Saving model (epoch = 3986, train loss = 0.9703, dev loss = 0.9332)\n","Saving model (epoch = 3988, train loss = 0.9662, dev loss = 0.9330)\n","Saving model (epoch = 4012, train loss = 0.9659, dev loss = 0.9330)\n","Saving model (epoch = 4013, train loss = 0.9668, dev loss = 0.9329)\n","Saving model (epoch = 4021, train loss = 0.9685, dev loss = 0.9328)\n","Saving model (epoch = 4028, train loss = 0.9663, dev loss = 0.9327)\n","Saving model (epoch = 4034, train loss = 0.9669, dev loss = 0.9327)\n","Saving model (epoch = 4036, train loss = 0.9697, dev loss = 0.9327)\n","Saving model (epoch = 4040, train loss = 0.9708, dev loss = 0.9327)\n","Saving model (epoch = 4050, train loss = 0.9646, dev loss = 0.9326)\n","Saving model (epoch = 4055, train loss = 0.9662, dev loss = 0.9325)\n","Saving model (epoch = 4075, train loss = 0.9652, dev loss = 0.9324)\n","Saving model (epoch = 4078, train loss = 0.9686, dev loss = 0.9323)\n","Saving model (epoch = 4111, train loss = 0.9644, dev loss = 0.9321)\n","Saving model (epoch = 4121, train loss = 0.9663, dev loss = 0.9321)\n","Saving model (epoch = 4138, train loss = 0.9646, dev loss = 0.9320)\n","Saving model (epoch = 4148, train loss = 0.9659, dev loss = 0.9318)\n","Saving model (epoch = 4157, train loss = 0.9626, dev loss = 0.9318)\n","Saving model (epoch = 4173, train loss = 0.9669, dev loss = 0.9317)\n","Saving model (epoch = 4176, train loss = 0.9633, dev loss = 0.9316)\n","Saving model (epoch = 4177, train loss = 0.9649, dev loss = 0.9316)\n","Saving model (epoch = 4200, train loss = 0.9644, dev loss = 0.9314)\n","Saving model (epoch = 4222, train loss = 0.9664, dev loss = 0.9314)\n","Saving model (epoch = 4230, train loss = 0.9673, dev loss = 0.9312)\n","Saving model (epoch = 4253, train loss = 0.9639, dev loss = 0.9310)\n","Saving model (epoch = 4260, train loss = 0.9614, dev loss = 0.9309)\n","Saving model (epoch = 4264, train loss = 0.9630, dev loss = 0.9309)\n","Saving model (epoch = 4278, train loss = 0.9657, dev loss = 0.9308)\n","Saving model (epoch = 4285, train loss = 0.9614, dev loss = 0.9308)\n","Saving model (epoch = 4288, train loss = 0.9622, dev loss = 0.9308)\n","Saving model (epoch = 4297, train loss = 0.9682, dev loss = 0.9307)\n","Saving model (epoch = 4299, train loss = 0.9643, dev loss = 0.9306)\n","Saving model (epoch = 4314, train loss = 0.9652, dev loss = 0.9305)\n","Saving model (epoch = 4317, train loss = 0.9676, dev loss = 0.9305)\n","Saving model (epoch = 4328, train loss = 0.9705, dev loss = 0.9305)\n","Saving model (epoch = 4341, train loss = 0.9679, dev loss = 0.9305)\n","Saving model (epoch = 4353, train loss = 0.9612, dev loss = 0.9304)\n","Saving model (epoch = 4357, train loss = 0.9626, dev loss = 0.9303)\n","Saving model (epoch = 4366, train loss = 0.9670, dev loss = 0.9303)\n","Saving model (epoch = 4369, train loss = 0.9637, dev loss = 0.9302)\n","Saving model (epoch = 4381, train loss = 0.9647, dev loss = 0.9301)\n","Saving model (epoch = 4405, train loss = 0.9614, dev loss = 0.9300)\n","Saving model (epoch = 4410, train loss = 0.9645, dev loss = 0.9300)\n","Saving model (epoch = 4411, train loss = 0.9643, dev loss = 0.9300)\n","Saving model (epoch = 4421, train loss = 0.9635, dev loss = 0.9299)\n","Saving model (epoch = 4428, train loss = 0.9611, dev loss = 0.9298)\n","Saving model (epoch = 4440, train loss = 0.9627, dev loss = 0.9298)\n","Saving model (epoch = 4452, train loss = 0.9645, dev loss = 0.9297)\n","Saving model (epoch = 4453, train loss = 0.9638, dev loss = 0.9297)\n","Saving model (epoch = 4463, train loss = 0.9637, dev loss = 0.9296)\n","Saving model (epoch = 4469, train loss = 0.9704, dev loss = 0.9295)\n","Saving model (epoch = 4472, train loss = 0.9648, dev loss = 0.9295)\n","Saving model (epoch = 4474, train loss = 0.9632, dev loss = 0.9295)\n","Saving model (epoch = 4491, train loss = 0.9637, dev loss = 0.9294)\n","Saving model (epoch = 4495, train loss = 0.9639, dev loss = 0.9294)\n","Saving model (epoch = 4509, train loss = 0.9638, dev loss = 0.9293)\n","Saving model (epoch = 4513, train loss = 0.9623, dev loss = 0.9293)\n","Saving model (epoch = 4525, train loss = 0.9614, dev loss = 0.9292)\n","Saving model (epoch = 4528, train loss = 0.9622, dev loss = 0.9291)\n","Saving model (epoch = 4539, train loss = 0.9615, dev loss = 0.9291)\n","Saving model (epoch = 4540, train loss = 0.9611, dev loss = 0.9291)\n","Saving model (epoch = 4544, train loss = 0.9654, dev loss = 0.9290)\n","Saving model (epoch = 4553, train loss = 0.9617, dev loss = 0.9290)\n","Saving model (epoch = 4564, train loss = 0.9610, dev loss = 0.9289)\n","Saving model (epoch = 4575, train loss = 0.9627, dev loss = 0.9289)\n","Saving model (epoch = 4585, train loss = 0.9574, dev loss = 0.9289)\n","Saving model (epoch = 4593, train loss = 0.9670, dev loss = 0.9289)\n","Saving model (epoch = 4597, train loss = 0.9609, dev loss = 0.9287)\n","Saving model (epoch = 4601, train loss = 0.9610, dev loss = 0.9287)\n","Saving model (epoch = 4620, train loss = 0.9637, dev loss = 0.9286)\n","Saving model (epoch = 4629, train loss = 0.9600, dev loss = 0.9285)\n","Saving model (epoch = 4639, train loss = 0.9612, dev loss = 0.9285)\n","Saving model (epoch = 4642, train loss = 0.9666, dev loss = 0.9284)\n","Saving model (epoch = 4648, train loss = 0.9634, dev loss = 0.9284)\n","Saving model (epoch = 4653, train loss = 0.9609, dev loss = 0.9284)\n","Saving model (epoch = 4659, train loss = 0.9596, dev loss = 0.9283)\n","Saving model (epoch = 4687, train loss = 0.9603, dev loss = 0.9283)\n","Saving model (epoch = 4688, train loss = 0.9620, dev loss = 0.9283)\n","Saving model (epoch = 4690, train loss = 0.9618, dev loss = 0.9281)\n","Saving model (epoch = 4705, train loss = 0.9662, dev loss = 0.9281)\n","Saving model (epoch = 4707, train loss = 0.9608, dev loss = 0.9281)\n","Saving model (epoch = 4716, train loss = 0.9619, dev loss = 0.9280)\n","Saving model (epoch = 4719, train loss = 0.9626, dev loss = 0.9280)\n","Saving model (epoch = 4724, train loss = 0.9643, dev loss = 0.9280)\n","Saving model (epoch = 4736, train loss = 0.9617, dev loss = 0.9279)\n","Saving model (epoch = 4738, train loss = 0.9654, dev loss = 0.9279)\n","Saving model (epoch = 4739, train loss = 0.9627, dev loss = 0.9279)\n","Saving model (epoch = 4753, train loss = 0.9613, dev loss = 0.9278)\n","Saving model (epoch = 4761, train loss = 0.9625, dev loss = 0.9278)\n","Saving model (epoch = 4780, train loss = 0.9649, dev loss = 0.9277)\n","Saving model (epoch = 4781, train loss = 0.9588, dev loss = 0.9277)\n","Saving model (epoch = 4783, train loss = 0.9584, dev loss = 0.9277)\n","Saving model (epoch = 4802, train loss = 0.9622, dev loss = 0.9276)\n","Saving model (epoch = 4811, train loss = 0.9633, dev loss = 0.9275)\n","Saving model (epoch = 4821, train loss = 0.9593, dev loss = 0.9275)\n","Saving model (epoch = 4825, train loss = 0.9599, dev loss = 0.9274)\n","Saving model (epoch = 4835, train loss = 0.9595, dev loss = 0.9274)\n","Saving model (epoch = 4856, train loss = 0.9607, dev loss = 0.9273)\n","Saving model (epoch = 4862, train loss = 0.9618, dev loss = 0.9273)\n","Saving model (epoch = 4866, train loss = 0.9598, dev loss = 0.9272)\n","Saving model (epoch = 4874, train loss = 0.9655, dev loss = 0.9272)\n","Saving model (epoch = 4880, train loss = 0.9644, dev loss = 0.9271)\n","Saving model (epoch = 4892, train loss = 0.9618, dev loss = 0.9271)\n","Saving model (epoch = 4897, train loss = 0.9614, dev loss = 0.9270)\n","Saving model (epoch = 4909, train loss = 0.9712, dev loss = 0.9270)\n","Saving model (epoch = 4932, train loss = 0.9594, dev loss = 0.9269)\n","Saving model (epoch = 4934, train loss = 0.9586, dev loss = 0.9268)\n","Saving model (epoch = 4943, train loss = 0.9600, dev loss = 0.9268)\n","Saving model (epoch = 4957, train loss = 0.9598, dev loss = 0.9268)\n","Saving model (epoch = 4965, train loss = 0.9609, dev loss = 0.9268)\n","Saving model (epoch = 4985, train loss = 0.9591, dev loss = 0.9267)\n","Saving model (epoch = 4987, train loss = 0.9606, dev loss = 0.9267)\n","Saving model (epoch = 4991, train loss = 0.9608, dev loss = 0.9266)\n","Saving model (epoch = 5006, train loss = 0.9568, dev loss = 0.9266)\n","Saving model (epoch = 5017, train loss = 0.9573, dev loss = 0.9265)\n","Saving model (epoch = 5033, train loss = 0.9605, dev loss = 0.9264)\n","Saving model (epoch = 5047, train loss = 0.9633, dev loss = 0.9264)\n","Saving model (epoch = 5062, train loss = 0.9593, dev loss = 0.9263)\n","Saving model (epoch = 5065, train loss = 0.9596, dev loss = 0.9262)\n","Saving model (epoch = 5091, train loss = 0.9595, dev loss = 0.9262)\n","Saving model (epoch = 5096, train loss = 0.9577, dev loss = 0.9262)\n","Saving model (epoch = 5113, train loss = 0.9613, dev loss = 0.9260)\n","Saving model (epoch = 5132, train loss = 0.9606, dev loss = 0.9260)\n","Saving model (epoch = 5150, train loss = 0.9590, dev loss = 0.9260)\n","Saving model (epoch = 5156, train loss = 0.9594, dev loss = 0.9259)\n","Saving model (epoch = 5164, train loss = 0.9571, dev loss = 0.9259)\n","Saving model (epoch = 5168, train loss = 0.9636, dev loss = 0.9258)\n","Saving model (epoch = 5182, train loss = 0.9663, dev loss = 0.9258)\n","Saving model (epoch = 5191, train loss = 0.9575, dev loss = 0.9257)\n","Saving model (epoch = 5236, train loss = 0.9581, dev loss = 0.9255)\n","Saving model (epoch = 5247, train loss = 0.9576, dev loss = 0.9255)\n","Saving model (epoch = 5251, train loss = 0.9569, dev loss = 0.9255)\n","Saving model (epoch = 5266, train loss = 0.9571, dev loss = 0.9254)\n","Saving model (epoch = 5268, train loss = 0.9565, dev loss = 0.9254)\n","Saving model (epoch = 5293, train loss = 0.9564, dev loss = 0.9253)\n","Saving model (epoch = 5324, train loss = 0.9536, dev loss = 0.9252)\n","Saving model (epoch = 5326, train loss = 0.9605, dev loss = 0.9252)\n","Saving model (epoch = 5331, train loss = 0.9576, dev loss = 0.9251)\n","Saving model (epoch = 5337, train loss = 0.9575, dev loss = 0.9251)\n","Saving model (epoch = 5338, train loss = 0.9604, dev loss = 0.9251)\n","Saving model (epoch = 5344, train loss = 0.9579, dev loss = 0.9250)\n","Saving model (epoch = 5353, train loss = 0.9597, dev loss = 0.9250)\n","Saving model (epoch = 5356, train loss = 0.9569, dev loss = 0.9250)\n","Saving model (epoch = 5365, train loss = 0.9585, dev loss = 0.9250)\n","Saving model (epoch = 5390, train loss = 0.9586, dev loss = 0.9250)\n","Saving model (epoch = 5391, train loss = 0.9564, dev loss = 0.9249)\n","Saving model (epoch = 5408, train loss = 0.9603, dev loss = 0.9248)\n","Saving model (epoch = 5429, train loss = 0.9561, dev loss = 0.9248)\n","Saving model (epoch = 5433, train loss = 0.9574, dev loss = 0.9247)\n","Saving model (epoch = 5435, train loss = 0.9586, dev loss = 0.9247)\n","Saving model (epoch = 5437, train loss = 0.9588, dev loss = 0.9246)\n","Saving model (epoch = 5478, train loss = 0.9565, dev loss = 0.9246)\n","Saving model (epoch = 5496, train loss = 0.9566, dev loss = 0.9245)\n","Saving model (epoch = 5501, train loss = 0.9579, dev loss = 0.9244)\n","Saving model (epoch = 5512, train loss = 0.9574, dev loss = 0.9244)\n","Saving model (epoch = 5525, train loss = 0.9562, dev loss = 0.9244)\n","Saving model (epoch = 5527, train loss = 0.9582, dev loss = 0.9243)\n","Saving model (epoch = 5563, train loss = 0.9566, dev loss = 0.9243)\n","Saving model (epoch = 5572, train loss = 0.9596, dev loss = 0.9243)\n","Saving model (epoch = 5580, train loss = 0.9553, dev loss = 0.9242)\n","Saving model (epoch = 5586, train loss = 0.9582, dev loss = 0.9242)\n","Saving model (epoch = 5605, train loss = 0.9571, dev loss = 0.9241)\n","Saving model (epoch = 5614, train loss = 0.9561, dev loss = 0.9241)\n","Saving model (epoch = 5633, train loss = 0.9555, dev loss = 0.9240)\n","Saving model (epoch = 5639, train loss = 0.9584, dev loss = 0.9240)\n","Saving model (epoch = 5647, train loss = 0.9539, dev loss = 0.9240)\n","Saving model (epoch = 5654, train loss = 0.9574, dev loss = 0.9239)\n","Saving model (epoch = 5656, train loss = 0.9551, dev loss = 0.9239)\n","Saving model (epoch = 5662, train loss = 0.9611, dev loss = 0.9239)\n","Saving model (epoch = 5698, train loss = 0.9554, dev loss = 0.9238)\n","Saving model (epoch = 5712, train loss = 0.9589, dev loss = 0.9237)\n","Saving model (epoch = 5725, train loss = 0.9562, dev loss = 0.9236)\n","Saving model (epoch = 5729, train loss = 0.9565, dev loss = 0.9236)\n","Saving model (epoch = 5736, train loss = 0.9559, dev loss = 0.9236)\n","Saving model (epoch = 5768, train loss = 0.9555, dev loss = 0.9235)\n","Saving model (epoch = 5776, train loss = 0.9563, dev loss = 0.9234)\n","Saving model (epoch = 5788, train loss = 0.9571, dev loss = 0.9234)\n","Saving model (epoch = 5794, train loss = 0.9561, dev loss = 0.9234)\n","Saving model (epoch = 5796, train loss = 0.9537, dev loss = 0.9234)\n","Saving model (epoch = 5812, train loss = 0.9542, dev loss = 0.9233)\n","Saving model (epoch = 5817, train loss = 0.9555, dev loss = 0.9233)\n","Saving model (epoch = 5848, train loss = 0.9556, dev loss = 0.9233)\n","Saving model (epoch = 5853, train loss = 0.9588, dev loss = 0.9232)\n","Saving model (epoch = 5859, train loss = 0.9584, dev loss = 0.9232)\n","Saving model (epoch = 5866, train loss = 0.9545, dev loss = 0.9232)\n","Saving model (epoch = 5885, train loss = 0.9524, dev loss = 0.9231)\n","Saving model (epoch = 5886, train loss = 0.9554, dev loss = 0.9231)\n","Saving model (epoch = 5894, train loss = 0.9578, dev loss = 0.9230)\n","Saving model (epoch = 5916, train loss = 0.9530, dev loss = 0.9230)\n","Saving model (epoch = 5929, train loss = 0.9524, dev loss = 0.9230)\n","Saving model (epoch = 5937, train loss = 0.9537, dev loss = 0.9229)\n","Saving model (epoch = 5956, train loss = 0.9576, dev loss = 0.9229)\n","Saving model (epoch = 5964, train loss = 0.9549, dev loss = 0.9229)\n","Saving model (epoch = 5997, train loss = 0.9565, dev loss = 0.9228)\n","Saving model (epoch = 6013, train loss = 0.9539, dev loss = 0.9227)\n","Saving model (epoch = 6021, train loss = 0.9627, dev loss = 0.9227)\n","Saving model (epoch = 6027, train loss = 0.9532, dev loss = 0.9227)\n","Saving model (epoch = 6034, train loss = 0.9585, dev loss = 0.9226)\n","Saving model (epoch = 6062, train loss = 0.9536, dev loss = 0.9225)\n","Saving model (epoch = 6078, train loss = 0.9545, dev loss = 0.9225)\n","Saving model (epoch = 6079, train loss = 0.9532, dev loss = 0.9225)\n","Saving model (epoch = 6086, train loss = 0.9556, dev loss = 0.9225)\n","Saving model (epoch = 6087, train loss = 0.9566, dev loss = 0.9225)\n","Saving model (epoch = 6094, train loss = 0.9581, dev loss = 0.9225)\n","Saving model (epoch = 6098, train loss = 0.9603, dev loss = 0.9224)\n","Saving model (epoch = 6112, train loss = 0.9541, dev loss = 0.9224)\n","Saving model (epoch = 6113, train loss = 0.9555, dev loss = 0.9224)\n","Saving model (epoch = 6152, train loss = 0.9534, dev loss = 0.9223)\n","Saving model (epoch = 6167, train loss = 0.9554, dev loss = 0.9223)\n","Saving model (epoch = 6181, train loss = 0.9537, dev loss = 0.9222)\n","Saving model (epoch = 6198, train loss = 0.9549, dev loss = 0.9221)\n","Saving model (epoch = 6207, train loss = 0.9595, dev loss = 0.9221)\n","Saving model (epoch = 6212, train loss = 0.9571, dev loss = 0.9221)\n","Saving model (epoch = 6217, train loss = 0.9558, dev loss = 0.9221)\n","Saving model (epoch = 6240, train loss = 0.9544, dev loss = 0.9221)\n","Saving model (epoch = 6264, train loss = 0.9550, dev loss = 0.9219)\n","Saving model (epoch = 6295, train loss = 0.9511, dev loss = 0.9219)\n","Saving model (epoch = 6314, train loss = 0.9552, dev loss = 0.9218)\n","Saving model (epoch = 6331, train loss = 0.9548, dev loss = 0.9218)\n","Saving model (epoch = 6332, train loss = 0.9554, dev loss = 0.9218)\n","Saving model (epoch = 6345, train loss = 0.9569, dev loss = 0.9217)\n","Saving model (epoch = 6357, train loss = 0.9530, dev loss = 0.9217)\n","Saving model (epoch = 6376, train loss = 0.9607, dev loss = 0.9217)\n","Saving model (epoch = 6385, train loss = 0.9542, dev loss = 0.9217)\n","Saving model (epoch = 6413, train loss = 0.9549, dev loss = 0.9216)\n","Saving model (epoch = 6414, train loss = 0.9497, dev loss = 0.9215)\n","Saving model (epoch = 6449, train loss = 0.9540, dev loss = 0.9214)\n","Saving model (epoch = 6464, train loss = 0.9550, dev loss = 0.9214)\n","Saving model (epoch = 6482, train loss = 0.9528, dev loss = 0.9214)\n","Saving model (epoch = 6502, train loss = 0.9536, dev loss = 0.9213)\n","Saving model (epoch = 6504, train loss = 0.9561, dev loss = 0.9213)\n","Saving model (epoch = 6531, train loss = 0.9557, dev loss = 0.9213)\n","Saving model (epoch = 6559, train loss = 0.9524, dev loss = 0.9212)\n","Saving model (epoch = 6569, train loss = 0.9526, dev loss = 0.9212)\n","Saving model (epoch = 6585, train loss = 0.9529, dev loss = 0.9211)\n","Saving model (epoch = 6597, train loss = 0.9541, dev loss = 0.9211)\n","Saving model (epoch = 6612, train loss = 0.9545, dev loss = 0.9211)\n","Saving model (epoch = 6621, train loss = 0.9528, dev loss = 0.9210)\n","Saving model (epoch = 6634, train loss = 0.9562, dev loss = 0.9210)\n","Saving model (epoch = 6636, train loss = 0.9571, dev loss = 0.9210)\n","Saving model (epoch = 6649, train loss = 0.9511, dev loss = 0.9209)\n","Saving model (epoch = 6681, train loss = 0.9547, dev loss = 0.9209)\n","Saving model (epoch = 6686, train loss = 0.9552, dev loss = 0.9208)\n","Saving model (epoch = 6717, train loss = 0.9551, dev loss = 0.9208)\n","Saving model (epoch = 6718, train loss = 0.9512, dev loss = 0.9207)\n","Saving model (epoch = 6728, train loss = 0.9519, dev loss = 0.9207)\n","Saving model (epoch = 6752, train loss = 0.9535, dev loss = 0.9207)\n","Saving model (epoch = 6780, train loss = 0.9524, dev loss = 0.9206)\n","Saving model (epoch = 6781, train loss = 0.9532, dev loss = 0.9206)\n","Saving model (epoch = 6787, train loss = 0.9522, dev loss = 0.9206)\n","Saving model (epoch = 6791, train loss = 0.9518, dev loss = 0.9206)\n","Saving model (epoch = 6797, train loss = 0.9536, dev loss = 0.9205)\n","Saving model (epoch = 6807, train loss = 0.9505, dev loss = 0.9205)\n","Saving model (epoch = 6836, train loss = 0.9533, dev loss = 0.9205)\n","Saving model (epoch = 6838, train loss = 0.9501, dev loss = 0.9205)\n","Saving model (epoch = 6846, train loss = 0.9575, dev loss = 0.9205)\n","Saving model (epoch = 6852, train loss = 0.9542, dev loss = 0.9204)\n","Saving model (epoch = 6858, train loss = 0.9528, dev loss = 0.9204)\n","Saving model (epoch = 6865, train loss = 0.9541, dev loss = 0.9204)\n","Saving model (epoch = 6886, train loss = 0.9518, dev loss = 0.9204)\n","Saving model (epoch = 6890, train loss = 0.9518, dev loss = 0.9203)\n","Saving model (epoch = 6941, train loss = 0.9554, dev loss = 0.9202)\n","Saving model (epoch = 6971, train loss = 0.9512, dev loss = 0.9202)\n","Saving model (epoch = 6978, train loss = 0.9553, dev loss = 0.9201)\n","Saving model (epoch = 6983, train loss = 0.9530, dev loss = 0.9201)\n","Saving model (epoch = 7045, train loss = 0.9529, dev loss = 0.9200)\n","Saving model (epoch = 7049, train loss = 0.9517, dev loss = 0.9200)\n","Saving model (epoch = 7065, train loss = 0.9529, dev loss = 0.9199)\n","Saving model (epoch = 7073, train loss = 0.9486, dev loss = 0.9199)\n","Saving model (epoch = 7090, train loss = 0.9519, dev loss = 0.9199)\n","Saving model (epoch = 7104, train loss = 0.9518, dev loss = 0.9199)\n","Saving model (epoch = 7143, train loss = 0.9536, dev loss = 0.9199)\n","Saving model (epoch = 7146, train loss = 0.9509, dev loss = 0.9198)\n","Saving model (epoch = 7148, train loss = 0.9544, dev loss = 0.9198)\n","Saving model (epoch = 7168, train loss = 0.9618, dev loss = 0.9197)\n","Saving model (epoch = 7182, train loss = 0.9501, dev loss = 0.9197)\n","Saving model (epoch = 7203, train loss = 0.9516, dev loss = 0.9197)\n","Saving model (epoch = 7238, train loss = 0.9577, dev loss = 0.9195)\n","Saving model (epoch = 7248, train loss = 0.9517, dev loss = 0.9195)\n","Saving model (epoch = 7261, train loss = 0.9530, dev loss = 0.9195)\n","Saving model (epoch = 7286, train loss = 0.9519, dev loss = 0.9194)\n","Saving model (epoch = 7302, train loss = 0.9519, dev loss = 0.9194)\n","Saving model (epoch = 7305, train loss = 0.9498, dev loss = 0.9194)\n","Saving model (epoch = 7320, train loss = 0.9515, dev loss = 0.9194)\n","Saving model (epoch = 7346, train loss = 0.9533, dev loss = 0.9194)\n","Saving model (epoch = 7351, train loss = 0.9499, dev loss = 0.9193)\n","Saving model (epoch = 7372, train loss = 0.9591, dev loss = 0.9193)\n","Saving model (epoch = 7404, train loss = 0.9540, dev loss = 0.9192)\n","Saving model (epoch = 7426, train loss = 0.9519, dev loss = 0.9192)\n","Saving model (epoch = 7445, train loss = 0.9517, dev loss = 0.9191)\n","Saving model (epoch = 7452, train loss = 0.9530, dev loss = 0.9191)\n","Saving model (epoch = 7458, train loss = 0.9495, dev loss = 0.9190)\n","Saving model (epoch = 7472, train loss = 0.9518, dev loss = 0.9190)\n","Saving model (epoch = 7520, train loss = 0.9512, dev loss = 0.9190)\n","Saving model (epoch = 7541, train loss = 0.9576, dev loss = 0.9189)\n","Saving model (epoch = 7564, train loss = 0.9498, dev loss = 0.9189)\n","Saving model (epoch = 7604, train loss = 0.9515, dev loss = 0.9189)\n","Saving model (epoch = 7616, train loss = 0.9492, dev loss = 0.9188)\n","Saving model (epoch = 7638, train loss = 0.9528, dev loss = 0.9188)\n","Saving model (epoch = 7639, train loss = 0.9515, dev loss = 0.9188)\n","Saving model (epoch = 7641, train loss = 0.9489, dev loss = 0.9188)\n","Saving model (epoch = 7643, train loss = 0.9497, dev loss = 0.9188)\n","Saving model (epoch = 7647, train loss = 0.9497, dev loss = 0.9188)\n","Saving model (epoch = 7684, train loss = 0.9641, dev loss = 0.9187)\n","Saving model (epoch = 7685, train loss = 0.9490, dev loss = 0.9187)\n","Saving model (epoch = 7695, train loss = 0.9508, dev loss = 0.9186)\n","Saving model (epoch = 7700, train loss = 0.9536, dev loss = 0.9186)\n","Saving model (epoch = 7718, train loss = 0.9527, dev loss = 0.9186)\n","Saving model (epoch = 7732, train loss = 0.9504, dev loss = 0.9185)\n","Saving model (epoch = 7761, train loss = 0.9521, dev loss = 0.9185)\n","Saving model (epoch = 7762, train loss = 0.9513, dev loss = 0.9185)\n","Saving model (epoch = 7763, train loss = 0.9490, dev loss = 0.9185)\n","Saving model (epoch = 7823, train loss = 0.9505, dev loss = 0.9185)\n","Saving model (epoch = 7830, train loss = 0.9496, dev loss = 0.9184)\n","Saving model (epoch = 7850, train loss = 0.9482, dev loss = 0.9184)\n","Saving model (epoch = 7853, train loss = 0.9507, dev loss = 0.9184)\n","Saving model (epoch = 7860, train loss = 0.9498, dev loss = 0.9183)\n","Saving model (epoch = 7867, train loss = 0.9502, dev loss = 0.9183)\n","Saving model (epoch = 7887, train loss = 0.9499, dev loss = 0.9183)\n","Saving model (epoch = 7896, train loss = 0.9484, dev loss = 0.9183)\n","Saving model (epoch = 7905, train loss = 0.9488, dev loss = 0.9183)\n","Saving model (epoch = 7926, train loss = 0.9477, dev loss = 0.9183)\n","Saving model (epoch = 7929, train loss = 0.9522, dev loss = 0.9182)\n","Saving model (epoch = 7946, train loss = 0.9516, dev loss = 0.9182)\n","Saving model (epoch = 7950, train loss = 0.9503, dev loss = 0.9182)\n","Saving model (epoch = 7971, train loss = 0.9508, dev loss = 0.9181)\n","Saving model (epoch = 7986, train loss = 0.9498, dev loss = 0.9181)\n","Saving model (epoch = 7992, train loss = 0.9508, dev loss = 0.9181)\n","Saving model (epoch = 7995, train loss = 0.9505, dev loss = 0.9181)\n","Saving model (epoch = 8017, train loss = 0.9510, dev loss = 0.9180)\n","Saving model (epoch = 8025, train loss = 0.9504, dev loss = 0.9180)\n","Saving model (epoch = 8068, train loss = 0.9531, dev loss = 0.9180)\n","Saving model (epoch = 8069, train loss = 0.9454, dev loss = 0.9180)\n","Saving model (epoch = 8081, train loss = 0.9494, dev loss = 0.9179)\n","Saving model (epoch = 8109, train loss = 0.9481, dev loss = 0.9179)\n","Saving model (epoch = 8136, train loss = 0.9519, dev loss = 0.9178)\n","Saving model (epoch = 8150, train loss = 0.9519, dev loss = 0.9178)\n","Saving model (epoch = 8169, train loss = 0.9513, dev loss = 0.9178)\n","Saving model (epoch = 8194, train loss = 0.9504, dev loss = 0.9177)\n","Saving model (epoch = 8201, train loss = 0.9518, dev loss = 0.9177)\n","Saving model (epoch = 8206, train loss = 0.9534, dev loss = 0.9177)\n","Saving model (epoch = 8212, train loss = 0.9501, dev loss = 0.9177)\n","Saving model (epoch = 8230, train loss = 0.9492, dev loss = 0.9176)\n","Saving model (epoch = 8248, train loss = 0.9542, dev loss = 0.9176)\n","Saving model (epoch = 8249, train loss = 0.9524, dev loss = 0.9176)\n","Saving model (epoch = 8279, train loss = 0.9506, dev loss = 0.9175)\n","Saving model (epoch = 8315, train loss = 0.9495, dev loss = 0.9175)\n","Saving model (epoch = 8322, train loss = 0.9528, dev loss = 0.9175)\n","Saving model (epoch = 8325, train loss = 0.9501, dev loss = 0.9175)\n","Saving model (epoch = 8357, train loss = 0.9478, dev loss = 0.9175)\n","Saving model (epoch = 8365, train loss = 0.9484, dev loss = 0.9174)\n","Saving model (epoch = 8390, train loss = 0.9512, dev loss = 0.9174)\n","Saving model (epoch = 8407, train loss = 0.9487, dev loss = 0.9173)\n","Saving model (epoch = 8418, train loss = 0.9470, dev loss = 0.9173)\n","Saving model (epoch = 8425, train loss = 0.9497, dev loss = 0.9173)\n","Saving model (epoch = 8445, train loss = 0.9492, dev loss = 0.9173)\n","Saving model (epoch = 8458, train loss = 0.9498, dev loss = 0.9173)\n","Saving model (epoch = 8472, train loss = 0.9562, dev loss = 0.9172)\n","Saving model (epoch = 8486, train loss = 0.9491, dev loss = 0.9172)\n","Saving model (epoch = 8524, train loss = 0.9604, dev loss = 0.9172)\n","Saving model (epoch = 8550, train loss = 0.9513, dev loss = 0.9171)\n","Saving model (epoch = 8573, train loss = 0.9523, dev loss = 0.9171)\n","Saving model (epoch = 8575, train loss = 0.9504, dev loss = 0.9171)\n","Saving model (epoch = 8576, train loss = 0.9468, dev loss = 0.9171)\n","Saving model (epoch = 8614, train loss = 0.9494, dev loss = 0.9171)\n","Saving model (epoch = 8623, train loss = 0.9531, dev loss = 0.9170)\n","Saving model (epoch = 8645, train loss = 0.9479, dev loss = 0.9169)\n","Saving model (epoch = 8662, train loss = 0.9490, dev loss = 0.9169)\n","Saving model (epoch = 8694, train loss = 0.9490, dev loss = 0.9169)\n","Saving model (epoch = 8752, train loss = 0.9450, dev loss = 0.9168)\n","Saving model (epoch = 8780, train loss = 0.9485, dev loss = 0.9168)\n","Saving model (epoch = 8792, train loss = 0.9604, dev loss = 0.9168)\n","Saving model (epoch = 8808, train loss = 0.9507, dev loss = 0.9168)\n","Saving model (epoch = 8830, train loss = 0.9492, dev loss = 0.9167)\n","Saving model (epoch = 8862, train loss = 0.9498, dev loss = 0.9167)\n","Saving model (epoch = 8864, train loss = 0.9473, dev loss = 0.9167)\n","Saving model (epoch = 8865, train loss = 0.9473, dev loss = 0.9167)\n","Saving model (epoch = 8892, train loss = 0.9498, dev loss = 0.9166)\n","Saving model (epoch = 8895, train loss = 0.9509, dev loss = 0.9165)\n","Saving model (epoch = 8950, train loss = 0.9488, dev loss = 0.9165)\n","Saving model (epoch = 8963, train loss = 0.9515, dev loss = 0.9165)\n","Saving model (epoch = 9015, train loss = 0.9503, dev loss = 0.9164)\n","Saving model (epoch = 9045, train loss = 0.9475, dev loss = 0.9164)\n","Saving model (epoch = 9056, train loss = 0.9463, dev loss = 0.9163)\n","Saving model (epoch = 9063, train loss = 0.9520, dev loss = 0.9163)\n","Saving model (epoch = 9066, train loss = 0.9488, dev loss = 0.9163)\n","Saving model (epoch = 9099, train loss = 0.9487, dev loss = 0.9163)\n","Saving model (epoch = 9120, train loss = 0.9488, dev loss = 0.9163)\n","Saving model (epoch = 9122, train loss = 0.9487, dev loss = 0.9162)\n","Saving model (epoch = 9148, train loss = 0.9490, dev loss = 0.9162)\n","Saving model (epoch = 9173, train loss = 0.9453, dev loss = 0.9162)\n","Saving model (epoch = 9194, train loss = 0.9528, dev loss = 0.9161)\n","Saving model (epoch = 9213, train loss = 0.9479, dev loss = 0.9161)\n","Saving model (epoch = 9235, train loss = 0.9527, dev loss = 0.9161)\n","Saving model (epoch = 9278, train loss = 0.9458, dev loss = 0.9160)\n","Saving model (epoch = 9288, train loss = 0.9479, dev loss = 0.9160)\n","Saving model (epoch = 9296, train loss = 0.9473, dev loss = 0.9160)\n","Saving model (epoch = 9319, train loss = 0.9524, dev loss = 0.9159)\n","Saving model (epoch = 9358, train loss = 0.9474, dev loss = 0.9159)\n","Saving model (epoch = 9385, train loss = 0.9470, dev loss = 0.9158)\n","Saving model (epoch = 9388, train loss = 0.9494, dev loss = 0.9158)\n","Saving model (epoch = 9421, train loss = 0.9475, dev loss = 0.9158)\n","Saving model (epoch = 9429, train loss = 0.9455, dev loss = 0.9158)\n","Saving model (epoch = 9517, train loss = 0.9487, dev loss = 0.9157)\n","Saving model (epoch = 9523, train loss = 0.9514, dev loss = 0.9157)\n","Saving model (epoch = 9526, train loss = 0.9528, dev loss = 0.9157)\n","Saving model (epoch = 9546, train loss = 0.9495, dev loss = 0.9157)\n","Saving model (epoch = 9550, train loss = 0.9463, dev loss = 0.9157)\n","Saving model (epoch = 9556, train loss = 0.9479, dev loss = 0.9156)\n","Saving model (epoch = 9582, train loss = 0.9465, dev loss = 0.9156)\n","Saving model (epoch = 9595, train loss = 0.9466, dev loss = 0.9155)\n","Saving model (epoch = 9597, train loss = 0.9502, dev loss = 0.9155)\n","Saving model (epoch = 9620, train loss = 0.9457, dev loss = 0.9155)\n","Saving model (epoch = 9642, train loss = 0.9466, dev loss = 0.9155)\n","Saving model (epoch = 9645, train loss = 0.9467, dev loss = 0.9155)\n","Saving model (epoch = 9649, train loss = 0.9474, dev loss = 0.9155)\n","Saving model (epoch = 9655, train loss = 0.9479, dev loss = 0.9154)\n","Saving model (epoch = 9686, train loss = 0.9475, dev loss = 0.9154)\n","Saving model (epoch = 9701, train loss = 0.9497, dev loss = 0.9154)\n","Saving model (epoch = 9703, train loss = 0.9473, dev loss = 0.9154)\n","Saving model (epoch = 9724, train loss = 0.9479, dev loss = 0.9154)\n","Saving model (epoch = 9727, train loss = 0.9443, dev loss = 0.9154)\n","Saving model (epoch = 9756, train loss = 0.9491, dev loss = 0.9154)\n","Saving model (epoch = 9761, train loss = 0.9467, dev loss = 0.9153)\n","Saving model (epoch = 9789, train loss = 0.9489, dev loss = 0.9153)\n","Saving model (epoch = 9796, train loss = 0.9463, dev loss = 0.9153)\n","Saving model (epoch = 9826, train loss = 0.9464, dev loss = 0.9153)\n","Saving model (epoch = 9847, train loss = 0.9465, dev loss = 0.9152)\n","Saving model (epoch = 9894, train loss = 0.9452, dev loss = 0.9152)\n","Saving model (epoch = 9895, train loss = 0.9480, dev loss = 0.9152)\n","Saving model (epoch = 9898, train loss = 0.9458, dev loss = 0.9152)\n","Saving model (epoch = 9912, train loss = 0.9469, dev loss = 0.9151)\n","Saving model (epoch = 9962, train loss = 0.9470, dev loss = 0.9151)\n","Saving model (epoch = 9976, train loss = 0.9474, dev loss = 0.9151)\n","Saving model (epoch = 9982, train loss = 0.9484, dev loss = 0.9150)\n","Saving model (epoch = 10011, train loss = 0.9490, dev loss = 0.9150)\n","Saving model (epoch = 10048, train loss = 0.9454, dev loss = 0.9150)\n","Saving model (epoch = 10075, train loss = 0.9475, dev loss = 0.9149)\n","Saving model (epoch = 10099, train loss = 0.9448, dev loss = 0.9149)\n","Saving model (epoch = 10103, train loss = 0.9465, dev loss = 0.9149)\n","Saving model (epoch = 10105, train loss = 0.9537, dev loss = 0.9149)\n","Saving model (epoch = 10110, train loss = 0.9451, dev loss = 0.9148)\n","Saving model (epoch = 10150, train loss = 0.9527, dev loss = 0.9148)\n","Saving model (epoch = 10151, train loss = 0.9491, dev loss = 0.9148)\n","Saving model (epoch = 10161, train loss = 0.9488, dev loss = 0.9148)\n","Saving model (epoch = 10225, train loss = 0.9473, dev loss = 0.9148)\n","Saving model (epoch = 10236, train loss = 0.9467, dev loss = 0.9148)\n","Saving model (epoch = 10271, train loss = 0.9481, dev loss = 0.9147)\n","Saving model (epoch = 10277, train loss = 0.9454, dev loss = 0.9147)\n","Saving model (epoch = 10280, train loss = 0.9463, dev loss = 0.9146)\n","Saving model (epoch = 10290, train loss = 0.9497, dev loss = 0.9146)\n","Saving model (epoch = 10326, train loss = 0.9477, dev loss = 0.9146)\n","Saving model (epoch = 10389, train loss = 0.9460, dev loss = 0.9146)\n","Saving model (epoch = 10392, train loss = 0.9514, dev loss = 0.9145)\n","Saving model (epoch = 10399, train loss = 0.9459, dev loss = 0.9145)\n","Saving model (epoch = 10410, train loss = 0.9466, dev loss = 0.9145)\n","Saving model (epoch = 10415, train loss = 0.9454, dev loss = 0.9145)\n","Saving model (epoch = 10431, train loss = 0.9474, dev loss = 0.9145)\n","Saving model (epoch = 10486, train loss = 0.9467, dev loss = 0.9145)\n","Saving model (epoch = 10494, train loss = 0.9478, dev loss = 0.9144)\n","Saving model (epoch = 10495, train loss = 0.9454, dev loss = 0.9144)\n","Saving model (epoch = 10506, train loss = 0.9473, dev loss = 0.9144)\n","Saving model (epoch = 10553, train loss = 0.9465, dev loss = 0.9144)\n","Saving model (epoch = 10604, train loss = 0.9460, dev loss = 0.9143)\n","Saving model (epoch = 10611, train loss = 0.9470, dev loss = 0.9143)\n","Saving model (epoch = 10632, train loss = 0.9427, dev loss = 0.9143)\n","Saving model (epoch = 10635, train loss = 0.9452, dev loss = 0.9142)\n","Saving model (epoch = 10711, train loss = 0.9513, dev loss = 0.9142)\n","Saving model (epoch = 10724, train loss = 0.9470, dev loss = 0.9141)\n","Saving model (epoch = 10777, train loss = 0.9450, dev loss = 0.9141)\n","Saving model (epoch = 10810, train loss = 0.9437, dev loss = 0.9141)\n","Saving model (epoch = 10812, train loss = 0.9468, dev loss = 0.9141)\n","Saving model (epoch = 10824, train loss = 0.9428, dev loss = 0.9141)\n","Saving model (epoch = 10843, train loss = 0.9473, dev loss = 0.9140)\n","Saving model (epoch = 10854, train loss = 0.9454, dev loss = 0.9140)\n","Saving model (epoch = 10874, train loss = 0.9443, dev loss = 0.9140)\n","Saving model (epoch = 10884, train loss = 0.9458, dev loss = 0.9139)\n","Saving model (epoch = 10894, train loss = 0.9464, dev loss = 0.9139)\n","Saving model (epoch = 10900, train loss = 0.9461, dev loss = 0.9139)\n","Saving model (epoch = 10914, train loss = 0.9459, dev loss = 0.9139)\n","Saving model (epoch = 10980, train loss = 0.9481, dev loss = 0.9139)\n","Saving model (epoch = 10997, train loss = 0.9501, dev loss = 0.9139)\n","Saving model (epoch = 11011, train loss = 0.9449, dev loss = 0.9138)\n","Saving model (epoch = 11037, train loss = 0.9475, dev loss = 0.9138)\n","Saving model (epoch = 11066, train loss = 0.9445, dev loss = 0.9138)\n","Saving model (epoch = 11075, train loss = 0.9428, dev loss = 0.9137)\n","Saving model (epoch = 11080, train loss = 0.9462, dev loss = 0.9137)\n","Saving model (epoch = 11110, train loss = 0.9444, dev loss = 0.9137)\n","Saving model (epoch = 11114, train loss = 0.9587, dev loss = 0.9137)\n","Saving model (epoch = 11174, train loss = 0.9455, dev loss = 0.9137)\n","Saving model (epoch = 11218, train loss = 0.9476, dev loss = 0.9136)\n","Saving model (epoch = 11219, train loss = 0.9455, dev loss = 0.9136)\n","Saving model (epoch = 11237, train loss = 0.9463, dev loss = 0.9136)\n","Saving model (epoch = 11323, train loss = 0.9460, dev loss = 0.9135)\n","Saving model (epoch = 11324, train loss = 0.9448, dev loss = 0.9135)\n","Saving model (epoch = 11338, train loss = 0.9500, dev loss = 0.9135)\n","Saving model (epoch = 11374, train loss = 0.9467, dev loss = 0.9134)\n","Saving model (epoch = 11399, train loss = 0.9446, dev loss = 0.9134)\n","Saving model (epoch = 11435, train loss = 0.9433, dev loss = 0.9134)\n","Saving model (epoch = 11471, train loss = 0.9478, dev loss = 0.9133)\n","Saving model (epoch = 11522, train loss = 0.9470, dev loss = 0.9133)\n","Saving model (epoch = 11523, train loss = 0.9440, dev loss = 0.9133)\n","Saving model (epoch = 11529, train loss = 0.9435, dev loss = 0.9133)\n","Saving model (epoch = 11553, train loss = 0.9432, dev loss = 0.9133)\n","Saving model (epoch = 11575, train loss = 0.9436, dev loss = 0.9133)\n","Saving model (epoch = 11596, train loss = 0.9454, dev loss = 0.9132)\n","Saving model (epoch = 11623, train loss = 0.9458, dev loss = 0.9132)\n","Saving model (epoch = 11634, train loss = 0.9452, dev loss = 0.9132)\n","Saving model (epoch = 11640, train loss = 0.9464, dev loss = 0.9132)\n","Saving model (epoch = 11694, train loss = 0.9441, dev loss = 0.9131)\n","Saving model (epoch = 11697, train loss = 0.9455, dev loss = 0.9131)\n","Saving model (epoch = 11791, train loss = 0.9430, dev loss = 0.9130)\n","Saving model (epoch = 11811, train loss = 0.9456, dev loss = 0.9130)\n","Saving model (epoch = 11874, train loss = 0.9451, dev loss = 0.9130)\n","Saving model (epoch = 11906, train loss = 0.9433, dev loss = 0.9129)\n","Saving model (epoch = 11929, train loss = 0.9457, dev loss = 0.9129)\n","Saving model (epoch = 11937, train loss = 0.9450, dev loss = 0.9129)\n","Saving model (epoch = 11943, train loss = 0.9454, dev loss = 0.9128)\n","Saving model (epoch = 11952, train loss = 0.9426, dev loss = 0.9128)\n","Saving model (epoch = 12008, train loss = 0.9461, dev loss = 0.9128)\n","Saving model (epoch = 12043, train loss = 0.9483, dev loss = 0.9128)\n","Saving model (epoch = 12046, train loss = 0.9450, dev loss = 0.9128)\n","Saving model (epoch = 12050, train loss = 0.9446, dev loss = 0.9128)\n","Saving model (epoch = 12064, train loss = 0.9462, dev loss = 0.9128)\n","Saving model (epoch = 12089, train loss = 0.9442, dev loss = 0.9127)\n","Saving model (epoch = 12128, train loss = 0.9498, dev loss = 0.9127)\n","Saving model (epoch = 12149, train loss = 0.9443, dev loss = 0.9126)\n","Saving model (epoch = 12220, train loss = 0.9463, dev loss = 0.9126)\n","Saving model (epoch = 12273, train loss = 0.9460, dev loss = 0.9125)\n","Saving model (epoch = 12308, train loss = 0.9442, dev loss = 0.9125)\n","Saving model (epoch = 12323, train loss = 0.9427, dev loss = 0.9125)\n","Saving model (epoch = 12331, train loss = 0.9435, dev loss = 0.9125)\n","Saving model (epoch = 12349, train loss = 0.9446, dev loss = 0.9125)\n","Saving model (epoch = 12368, train loss = 0.9461, dev loss = 0.9124)\n","Saving model (epoch = 12370, train loss = 0.9440, dev loss = 0.9124)\n","Saving model (epoch = 12412, train loss = 0.9463, dev loss = 0.9124)\n","Saving model (epoch = 12444, train loss = 0.9445, dev loss = 0.9124)\n","Saving model (epoch = 12465, train loss = 0.9419, dev loss = 0.9124)\n","Saving model (epoch = 12486, train loss = 0.9435, dev loss = 0.9124)\n","Saving model (epoch = 12525, train loss = 0.9427, dev loss = 0.9123)\n","Saving model (epoch = 12560, train loss = 0.9467, dev loss = 0.9123)\n","Saving model (epoch = 12602, train loss = 0.9432, dev loss = 0.9122)\n","Saving model (epoch = 12628, train loss = 0.9437, dev loss = 0.9122)\n","Saving model (epoch = 12665, train loss = 0.9428, dev loss = 0.9122)\n","Saving model (epoch = 12672, train loss = 0.9466, dev loss = 0.9122)\n","Saving model (epoch = 12689, train loss = 0.9466, dev loss = 0.9122)\n","Saving model (epoch = 12709, train loss = 0.9447, dev loss = 0.9121)\n","Saving model (epoch = 12717, train loss = 0.9432, dev loss = 0.9121)\n","Saving model (epoch = 12719, train loss = 0.9422, dev loss = 0.9121)\n","Saving model (epoch = 12741, train loss = 0.9423, dev loss = 0.9121)\n","Saving model (epoch = 12799, train loss = 0.9430, dev loss = 0.9121)\n","Saving model (epoch = 12844, train loss = 0.9474, dev loss = 0.9121)\n","Saving model (epoch = 12861, train loss = 0.9477, dev loss = 0.9120)\n","Saving model (epoch = 12867, train loss = 0.9434, dev loss = 0.9120)\n","Saving model (epoch = 12883, train loss = 0.9407, dev loss = 0.9120)\n","Saving model (epoch = 12910, train loss = 0.9446, dev loss = 0.9120)\n","Saving model (epoch = 12920, train loss = 0.9503, dev loss = 0.9120)\n","Saving model (epoch = 12958, train loss = 0.9447, dev loss = 0.9120)\n","Saving model (epoch = 12976, train loss = 0.9442, dev loss = 0.9120)\n","Saving model (epoch = 12995, train loss = 0.9498, dev loss = 0.9120)\n","Saving model (epoch = 13006, train loss = 0.9431, dev loss = 0.9120)\n","Saving model (epoch = 13032, train loss = 0.9436, dev loss = 0.9119)\n","Saving model (epoch = 13048, train loss = 0.9414, dev loss = 0.9119)\n","Saving model (epoch = 13060, train loss = 0.9428, dev loss = 0.9118)\n","Saving model (epoch = 13107, train loss = 0.9452, dev loss = 0.9118)\n","Saving model (epoch = 13177, train loss = 0.9445, dev loss = 0.9118)\n","Saving model (epoch = 13198, train loss = 0.9441, dev loss = 0.9117)\n","Saving model (epoch = 13293, train loss = 0.9412, dev loss = 0.9117)\n","Saving model (epoch = 13342, train loss = 0.9457, dev loss = 0.9117)\n","Saving model (epoch = 13350, train loss = 0.9411, dev loss = 0.9117)\n","Saving model (epoch = 13407, train loss = 0.9424, dev loss = 0.9116)\n","Saving model (epoch = 13422, train loss = 0.9435, dev loss = 0.9116)\n","Saving model (epoch = 13469, train loss = 0.9458, dev loss = 0.9115)\n","Saving model (epoch = 13566, train loss = 0.9414, dev loss = 0.9115)\n","Saving model (epoch = 13568, train loss = 0.9418, dev loss = 0.9115)\n","Saving model (epoch = 13616, train loss = 0.9428, dev loss = 0.9114)\n","Saving model (epoch = 13649, train loss = 0.9425, dev loss = 0.9114)\n","Saving model (epoch = 13666, train loss = 0.9415, dev loss = 0.9114)\n","Saving model (epoch = 13772, train loss = 0.9442, dev loss = 0.9113)\n","Saving model (epoch = 13815, train loss = 0.9459, dev loss = 0.9113)\n","Saving model (epoch = 13853, train loss = 0.9456, dev loss = 0.9112)\n","Saving model (epoch = 13881, train loss = 0.9432, dev loss = 0.9112)\n","Saving model (epoch = 13887, train loss = 0.9440, dev loss = 0.9112)\n","Saving model (epoch = 13984, train loss = 0.9412, dev loss = 0.9112)\n","Saving model (epoch = 14000, train loss = 0.9412, dev loss = 0.9111)\n","Saving model (epoch = 14021, train loss = 0.9433, dev loss = 0.9111)\n","Saving model (epoch = 14057, train loss = 0.9420, dev loss = 0.9111)\n","Saving model (epoch = 14099, train loss = 0.9473, dev loss = 0.9111)\n","Saving model (epoch = 14119, train loss = 0.9416, dev loss = 0.9111)\n","Saving model (epoch = 14130, train loss = 0.9420, dev loss = 0.9110)\n","Saving model (epoch = 14176, train loss = 0.9396, dev loss = 0.9110)\n","Saving model (epoch = 14204, train loss = 0.9403, dev loss = 0.9110)\n","Saving model (epoch = 14234, train loss = 0.9397, dev loss = 0.9109)\n","Saving model (epoch = 14319, train loss = 0.9411, dev loss = 0.9109)\n","Saving model (epoch = 14323, train loss = 0.9420, dev loss = 0.9109)\n","Saving model (epoch = 14342, train loss = 0.9447, dev loss = 0.9109)\n","Saving model (epoch = 14403, train loss = 0.9418, dev loss = 0.9108)\n","Saving model (epoch = 14435, train loss = 0.9431, dev loss = 0.9108)\n","Saving model (epoch = 14463, train loss = 0.9425, dev loss = 0.9108)\n","Saving model (epoch = 14507, train loss = 0.9408, dev loss = 0.9108)\n","Saving model (epoch = 14519, train loss = 0.9447, dev loss = 0.9107)\n","Saving model (epoch = 14520, train loss = 0.9413, dev loss = 0.9107)\n","Saving model (epoch = 14525, train loss = 0.9416, dev loss = 0.9107)\n","Saving model (epoch = 14582, train loss = 0.9425, dev loss = 0.9107)\n","Saving model (epoch = 14609, train loss = 0.9438, dev loss = 0.9107)\n","Saving model (epoch = 14641, train loss = 0.9430, dev loss = 0.9107)\n","Saving model (epoch = 14645, train loss = 0.9407, dev loss = 0.9107)\n","Saving model (epoch = 14648, train loss = 0.9437, dev loss = 0.9106)\n","Saving model (epoch = 14684, train loss = 0.9421, dev loss = 0.9106)\n","Saving model (epoch = 14756, train loss = 0.9415, dev loss = 0.9106)\n","Saving model (epoch = 14782, train loss = 0.9432, dev loss = 0.9105)\n","Saving model (epoch = 14846, train loss = 0.9421, dev loss = 0.9105)\n","Saving model (epoch = 14937, train loss = 0.9432, dev loss = 0.9105)\n","Saving model (epoch = 14958, train loss = 0.9436, dev loss = 0.9105)\n","Saving model (epoch = 14984, train loss = 0.9396, dev loss = 0.9104)\n","Finished training after 15000 epochs\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hsNO9nnXQBvP","executionInfo":{"status":"ok","timestamp":1616231875469,"user_tz":-480,"elapsed":699269,"user":{"displayName":"張君豪","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giewpu4H1RC4LywWkj1iyfrLc5i6hgwttwazWU6EQ=s64","userId":"03999016312843703738"}},"outputId":"4dfb091c-8f02-427c-e7e0-380aa00aac33"},"source":["plot_learning_curve(model_loss_record, title='deep model')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUVfrA8e+bXiEhdEKJgoIgImCvYENFUeyuva1dd+3ortjLuvaC+hNdVxE7CiIqKjZUFhQRDb13CKT35Pz+uHcm0zMzmcmE4f08T57cue28094599xzzxVjDEoppeJPQqwDUEopFR2a4JVSKk5pgldKqTilCV4ppeKUJnillIpTmuCVUipOaYJXESEih4nI4ljH0VaIyCEislREykXklCDWf01E7m+N2FqLiMwSkcuCXNeISN9ox7Sr0QQfB0RklYgcHcsYjDHfGWP2jGUMbcy9wLPGmCxjzJRYB6N2TZrgVVBEJDHWMbRUKz+H3sAfrVieUl40wccxEUkQkdtFZLmIFInIOyLSwWX5uyKySURKRORbERnosuw1EXlBRKaLSAUwwj5SuFlEFtjbvC0iafb6R4rIOpft/a5rL79VRDaKyAYRuSzQIbqIdBCRV+11d4jIFHv+RSLyvce6zv34eA4328830WX9U0VkQTCvl4+4LheRZSKyXUQ+FpHu9vzlwG7AVLuJJtXHtvuKyC8iUiYibwNpHstHi8h8ESkWkdkiMthlWXcReV9EtorIShG53mXZeBF5z369y+wy9gnwHIyIXG03J5WJyH0isrtdZqn9GqQ095ztZceIyCL7/X4WEI+yLhGRQvs9/ExEevuLS0WIMUb/dvI/YBVwtI/5NwA/AflAKvAi8JbL8kuAbHvZk8B8l2WvASXAIVgVgTS7nDlAd6ADUAhcaa9/JLDOIyZ/644CNgEDgQzgDcAAff08v0+At4FcIBk4wp5/EfC9x7rO/fh5DsuBY1zWfxe4PZjXy6OckcA2YKi97jPAt829J/ayFGA18Df7+ZwO1AH328v3BbYABwCJwIX2/lLt5zEP+Ke9n92AFcBx9rbj7X2dbu/7ZmAlkOwnFgN8BLSz348a4Et7v+2BP4ELm3vOQEegzKXcvwH1wGX28jHAMmAAkATcBcz29b7pXwRzQ6wD0L8IvIn+E3whcJTL4272lz/Jx7o59pesvf34NeB1H+Wc5/L4UWCCPX0k3gne37oTgYdclvX19wW3Y24Ecn0su4jmE7znc7gfmGhPZwMVQO8wXq9XgEddHmfZ6/YJ9J7Yyw4HNgDiMm82TQn+BeA+j20WA0dgJf01HsvuAF61p8cDP7ksSwA2Aof5icUAh7g8ngfc5vL438CTzT1n4AKPcgVYR1OC/xS41COuSpfXXhN8FP60iSa+9QY+tA/zi7ESWAPQRUQSReRhuzmiFCshgVUTc1jrY5+bXKYrsb7k/vhbt7vHvn2V49AT2G6M2RFgnUA89z0JGGs3m4wFfjHGrLaX+X29fOy3O1YtHABjTDlQBPQIIqbuwHpjZzbbapfp3sBNjjjsWHra2/UGunssG+cRo/M5G2MasRJtd/zb7DJd5eOx6/vm7zm7vaf2c3N97XsDT7nEvB3rRyCY10uFKSnWAaioWgtcYoz5wXOBiJyPddh8NFZybw/swL3dNFpDjW7EagZx6Blg3bVABxHJMcYUeyyrwGriAUBEuvrY3u05GGP+FJHVwPHAuVgJ37Usn6+XDxuwkpaj7EwgD1gfxLYbgR4iIi5JvhdW85EjjgeMMQ94bigiBwErjTH9Auy/p8v6CViv9YYg4mpOoOe80aNcwf19dTynNyMQhwqS1uDjR7KIpLn8JQETgAccJ7NEpJOIjLHXz8Zqby3CSpIPtmKs7wAXi8gAEckA/uFvRWPMRqzD++dFJFdEkkXkcHvxb8BAERlin8AdH2T5k7Da2w/HaoN3CPR6eXrLfg5D7KOBB4GfjTGrgij/R6z26evt5zMW2N9l+cvAlSJygFgyReREEcnGOq9RJiK3iUi6fSQ2SET2c9l+mIiMtT8DN2K9zz8FEVdzAj3nT7DeC0e51wOuP7gTgDvEPpEvIu1F5IwIxKQC0AQfP6ZjHU47/sYDTwEfA5+LSBnWl/wAe/3XsQ6312OdSItEAgiKMeZT4Gnga6wTb46ya/xscj5WW+8irJOPN9r7WYLV33wmsBT43s/2nt7Cas/+yhizzWV+oNfL8znMxPpheh+r9ro7cHYwhRtjarGahy7Caqo4C/jAZflc4HLgWayjqmX2uhhjGoDRwBCsk6fbgP/DOgJz+Mje5w6s126sMaYumNiaidvvc7ZfxzOAh7EqDf2AH1y2/RB4BJhsNwkuxDqKUlEk7s2ASrU+ERmA9YVPNcbUxzqenZmIjMc6WXlerGNRsac1eBUTYvU/TxWRXKya3VRN7kpFVlQTvFgXu/xuX7AxN5plqZ3OX7GaW5Zj9VS5KrbhKBV/otpEIyKrgOEe7ZxKKaVagTbRKKVUnIp2DX4l1pl8A7xojHnJxzpXAFcAZGZmDuvfv3/I5WxZuZqNHTtRsH4t7frrgIZKqV3HvHnzthljOvlaFu0E38MYs15EOgNfANcZY771t/7w4cPN3LmhN9U/ff5lPHjJtbx+99849tuvWxCxUkrtXERknjFmuK9lUW2iMcast/9vAT7E/WKOiEk0jVZ5oi1OSinlELWMaF99l+2YBo7F6usc+bIaraOQhgRN8Eop5RDNsWi6YA3c5ChnkjFmRjQKSnDW4AVjDHaZSim1S4tagjfGrAD83mggkhIarQTfmJDApnvuodv48a1RrFKqDairq2PdunVUV1fHOpSoSktLIz8/n+Tk5KC3iYvRJBPsE8WNIhRPflsTvFK7kHXr1pGdnU2fPn3i9ujdGENRURHr1q2joKAg6O3iotHatQavlNq1VFdXk5eXF7fJHUBEyMvLC/koJS4yoqMNvjGO32CllH/xnNwdwnmOcZHgHb1otAavlFJN4iIjOvrBa4JXSrW24uJinn/++ZC3O+GEEygu9rxJWWTFRUZ0tsHrhU5KqVbmL8HX1wce/Xr69Onk5OREKywgTnrRiNEmGqVUbNx+++0sX76cIUOGkJycTFpaGrm5uSxatIglS5ZwyimnsHbtWqqrq7nhhhu44oorAOjTpw9z586lvLyc448/nkMPPZTZs2fTo0cPPvroI9LT01scW1wk+MTGpgudlFK7rk0PPkhN4aKI7jN1QH+6jhvnd/nDDz/MwoULmT9/PrNmzeLEE09k4cKFzu6MEydOpEOHDlRVVbHffvtx2mmnkZeX57aPpUuX8tZbb/Hyyy9z5pln8v7773PeeS2/KVdcJHix2+AbtIlGKRVj+++/v1tf9aeffpoPP/wQgLVr17J06VKvBF9QUMCQIUMAGDZsGKtWrYpILHGR4B1t8CZBa/BK7coC1bRbS2ZmpnN61qxZzJw5kx9//JGMjAyOPPJIn33ZU1NTndOJiYlUVVVFJJa4qPI2XckaF09HKbUTyc7OpqyszOeykpIScnNzycjIYNGiRfz000+tGltc1eD1JKtSqrXl5eVxyCGHMGjQINLT0+nSpYtz2ahRo5gwYQIDBgxgzz335MADD2zV2OIjwbuMRaOUUq1t0qRJPuenpqby6aef+lzmaGfv2LEjCxc2jaR+8803RyyuuKjyag1eKaW8xUVGFG2DV0opL3GRERO1Bq+UUl7iIiM2DVWgbfBKKeUQFwledLAxpZTyEhcZUYcqUEopb3GR4B0nWRu0Bq+UagPGjx/PY489Fusw4iPBO4cq0F40SinlFBcZMUGHC1ZKxdgDDzzAHnvswaGHHsrixYsBWL58OaNGjWLYsGEcdthhLFq0iJKSEnr37k2jXTGtqKigZ8+e1NXVRTymOLmSVXvRKKXgH0vXsbA8MgN1OQzKSue+fvkB15k3bx6TJ09m/vz51NfXM3ToUIYNG8YVV1zBhAkT6NevHz///DNXX301X331FUOGDOGbb75hxIgRTJs2jeOOO47k5OSIxg3xkuD1nqxKqRj67rvvOPXUU8nIyADg5JNPprq6mtmzZ3PGGWc416upqQHgrLPO4u2332bEiBFMnjyZq6++OipxxUeC126SSilotqbdmhobG8nJyWH+/Pley04++WTGjRvH9u3bmTdvHiNHjoxKDHGREfVCJ6VULB1++OFMmTKFqqoqysrKmDp1KhkZGRQUFPDuu+8CYIzht99+AyArK4v99tuPG264gdGjR5OYmBiVuOIjwWsNXikVQ0OHDuWss85in3324fjjj2e//fYD4M033+SVV15hn332YeDAgXz00UfObc466yzeeOMNzjrrrKjFFR9NNHqhk1Iqxu68807uvPNOr/kzZszwuf7pp5+OsXsARktcVHn1QiellPIWFxkxUS90UkopL3GREUUvdFJqlxbtpo62IJznGBcZMXPYUEB70Si1K0pLS6OoqCiuk7wxhqKiItLS0kLaLi5OsmYdeigJjY1ag1dqF5Sfn8+6devYunVrrEOJqrS0NPLzQ+vnHxcJPqV3byvB2zV4U1eHROGyX6VU25OcnExBQUGsw2iT4qbKK6bReU/Wip9+inE0SikVe1FP8CKSKCK/isi0aJaT6NZEo23xSinVGjX4G4DCqJYg4tZEo5RSKsoJXkTygROB/4tmOWB1ldSTrEop1STaGfFJ4Fag0d8KInKFiMwVkbktOQue2NioFzoppZSLqGVEERkNbDHGzAu0njHmJWPMcGPM8E6dOoVbGmKMDlWglFIuopkRDwFOFpFVwGRgpIi8EZWS7DZ4HWxMKaWaRC3BG2PuMMbkG2P6AGcDXxljzotWeQkubfCNlZXRKkYppXYa8dGmIbj1otk4blyMA1JKqdhrlStZjTGzgFnRLCPBNPWDb6yoiGZRSim1U4iTGry4JXillFJxkuBFBGk0eqGTUkq5iIsEjwiJWoNXSik38ZERnUMVxMfTUUqpSIiPjGiMPVSBNtEopZRDfCR4sC90ipuno5RSLRYfGVHE7UInpZRS8ZLg0eGClVLKU5wkeLQfvFJKeYiPjOgxVIFSSql4SfDoDT+UUspT3GTERO0Hr5RSbuIjI+qFTkop5SUuMqKI6IVOSinlIS4SPCJ6T1allPIQNxlR78mqlFLu4iIjJmRm6j1ZlVLKQ1wk+NS+ffVCJ6WU8hA3GVEvdFJKKXfxk+D1QiellHITHxnR0Q9eE7xSSjnFTUYUo/dkVUopV3GT4PUkq1JKuYuPjKgXOimllJe4yYh6oZNSSrmLj4xon2TVC52UUqpJfCR4tA1eKaU8xU1G1AudlFLKXZwkeNELnZRSykPcZES94YdSSrmLi4yYkJqiN/xQSikPcZHgJTlZ78mqlFIe4iYjahONUkq5i5uMKHqSVSml3EQtI4pImojMEZHfROQPEbknWmUB9lAF2gavlFIOSVHcdw0w0hhTLiLJwPci8qkx5qdoFCamUYcqUEopF1FL8MYYA5TbD5PtPxOt8nSoAqWUchfVKq+IJIrIfGAL8IUx5mcf61whInNFZO7WrVvDLivRGExCgl7NqpRStqgmeGNMgzFmCJAP7C8ig3ys85IxZrgxZninTp3CLiu5rg6AuqRotjoppdTOI6QELyIJItIu1EKMMcXA18CoULcNVkq9I8EnR6sIpZTaqTSb4EVkkoi0E5FMYCHwp4jcEsR2nUQkx55OB44BFrU0YH9S7Bp8bbImeKWUguBq8HsZY0qBU4BPgQLg/CC26wZ8LSILgP9htcFPCzvSZiTbNfharcErpRQQXIJPtrs5ngJ8bIypI4jeMMaYBcaYfY0xg40xg4wx97Y02EA8a/BbHn8imsUppVSbF0yCfxFYBWQC34pIb6A0mkGFI6WuFmiqwRe99FIsw1FKqZhrtsuJMeZp4GmXWatFZET0QgqP4ySrtsErpZQlmJOsN9gnWUVEXhGRX4CRrRBbSJLr6wHtRaOUUg7BNNFcYp9kPRbIxTrB+nBUowqDs4lGa/BKKQUEl+Adl4aeAPzXGPOHy7w2o+kka0qMI1FKqbYhmAQ/T0Q+x0rwn4lINtAY3bBCl6JXsiqllJtgsuGlwBBghTGmUkTygIujG1boHCdZa7QGr5RSQHC9aBpFJB84V6yBvL4xxkyNemQhSq+pBqAqNS3GkSilVNsQTC+ah4EbgD/tv+tF5MFoBxaqzKpKACrSM2IciVJKtQ3BNNGcAAwxxjQCiMh/gF+BcdEMLFQp9fUk19VRmZYe61CUUqpNCHY0yRyX6fbRCCQSMqqrqEjXBK+UUhBcDf4h4FcR+Rqre+ThwO1RjSpMmdVVbjX4xspKEjK0yUYptWtqtgZvjHkLOBD4AHgfOMgY83a0AwtHRnUVFS4Jfvsbb8YwGqWUii2/CV5Ehjr+sIb+XWf/dbfntTkZVe41+K2PPx7DaJRSKrYCNdH8O8AyQxscjyazupJtObmxDkMppdoEvwneGNPmRoxsjtVE0yPWYSilVJsQ1Ztut7bMqirtB6+UUra4SvDZleWUZWQ2f7sppZTaBcRVgs+qqqQxMZHq1NRYh6KUUjEXqBfNeS7Th3gsuzaaQYUru7ICgLKMzBhHopRSsReoBv93l+lnPJZdEoVYWizLmeCzYhyJUkrFXqAEL36mfT1uE7K0Bq+UUk6BErzxM+3rcZvgaKIp1540SikV8EKn/iKyAKu2vrs9jf14t6hHFgZngtcavFJKBUzwA1otigjJsseEd22iMfX1SBRu42fq6yl+/wNyThsblf0rpVRL+W2iMcasdv0DyoGhQEf7cZuTWVWJNDa61eAXDdqbijlzIl7WjkmT2HT33eyY9FbE962UUpEQqJvkNBEZZE93AxZi9Z75r4jc2ErxhSTBGDKrK71Osm596umIl9VQXGL9Ly2N+L6VUioSAp1kLTDGLLSnLwa+MMacBBxAG+0mCZBVWenVBl81b16MolFKqdgJlODrXKaPAqYDGGPKgMZoBtUS2ZUV2otGKaUIfJJ1rYhchzUG/FBgBoCIpAPJrRBbWLKqvJtoHGpWrCAxN5ek3AgOKWzaZI9RpZQKWIO/FBgIXAScZYwptucfCLwa5bjCllVZ7vdK1hUnnMiKUcdHpiBpk9d6KaWUU6Dx4LcAV/qY/zXwdTSDaonsygoqAtyHtaGkJDIFtbGae/GUKaTk55MxfHisQ1FKtRF+E7yIfBxoQ2PMyZEPp+WyKv030URFG6nJb7z9DgAGLCoEoLGmBklIQJLbbGuaUirKArXBHwSsBd4CfqaNjj/jKbuygpqUVGqTkkipr/e5zqYHH6Rq3i8UvP9eywsMUJOv37aN5SeOpvdrr5I2oHWvG1u8zxBS+/Vjt6kBf6eVUnEsUBt8V2AcMAh4CjgG2GaM+cYY801rBBeOLOd4NP5r8Tte/y/Vf/zhfFy/YwfVi5eEVlAQNffyb7+jsaSE7f95PbR9R0jN0qUxKResE9orxpwSuSYxpVTIAl3J2mCMmWGMuRDrxOoyYFawY8GLSE8R+VpE/hSRP0TkhgjFHFBWVejj0aw67XRWjhkDQM2yZRT2H0D5d99FJb5dxbbnX6Bm8WLKv/021qEotcsKeEcnEUkVkbHAG8A1wNPAh0Huux64yRizF9YPxDUisldLgg1GODf9qNuwwTld+euvAJR+9llwGwc62drGTsQqpXYtgYYqeB34EasP/D3GmP2MMfcZY9YHs2NjzEZjzC/2dBlQCPSIQMwBNY0oGd7FTg3Fxc2v5G/b8nJWX3AhtWvXui9owYnYqvnz2fLUU2FvHzP646ZUzAWqwZ8H9ANuAGaLSKn9VyYiIQ3AIiJ9gH2xTtZ6LrtCROaKyNytW7eGslufsiq9R5T0Z+kRR7Luuuvc5m399+POaVNbS8WPPwbeiUvyLps5k8o5c9j27LMhRBzYqrPPoeiFCS3aR/n3P1D5v/9FKKJQxe7cfPXiJex4SweDU7uuQG3wCcaYbPuvnctftjGmXbAFiEgW8D5wozHG64fBGPOSMWa4MWZ4p06dwnsWLkK5q1P95s2UfTHT7/It//43ay6+hKoFC/yuU11YyIqxY2msqPBxG5S2UYtde9llrD7/gtYttA3U4FeOGcOme+6NdRhKxUzANviWEpFkrOT+pjHmg2iW5eA4yVoRoBdNsGpWrgSg/Pvv/a5T/tVX1PxZSOX8+S5zPWqtITTRNFZXU/6d//JiqXLePBrKK0LbqI1cJ6Bapm7zZspmzYp1GCpEUUvwIiLAK0ChMebx5taPlJT6elJra7xq8MZPn3i/XGqg255+hsq5c92XeyauhgYa7RuO+NqHP3Xr17Pt5Zedjzfddx9rL7889G6bUdZQUsLqv5zH+hvb5EjRKspWnX4G6668KtZhqBBFswZ/CHA+MFJE5tt/J0SxPKfsygqvBN9YXR3SPkre/8Atiddt3BRw/fU338Lme++zHojQUFJiNdsAJR98QOkM371y1vz1r2z99+POnjy1K1dZ8ZaXhRRvtJnaWgCqFy1ym184cBAbbr+D6sVLMCbQbXzVzqw+AufHVOuLWoI3xnxvjBFjzGBjzBD7b3q0ynOVVVkRVi+axqoqv8tqli6l9LPP2T5pku9tPW78seSAA9n80MPOx1see8z3dvZJYWdtvw20XQfkGV9DAyVTprByzBh2vOnjtdEWmpDULF3K+ptuDv2IU7VpJVOnUrcpcCUxGuLyZqJWgncfUbJu3bpmt1u871C3x+KSnYpeesk53eHccwPvyEe7czDlN7ePmAoinurCP53Tpq3/UHmoWbaM5G7dSMhs/Ru2165aRXLv3ogI62+9jZrCQvIuvYS0vaJ+2YhqBY3V1Wy45VaSe/Wi7+dBXl8TIVE9yRorvppoVp5yaug7CjfJ+tlu7dXXULtqFXVbtvjftq0nxhDjk7b2Q2UrmfYJdeutSzoqZs9mxeiTWHv1Na0eR9Vvv7F81PHs+O8b1owgX1/T0EBh/wFsffY5tjz5JKWffx7FKJWDaWgIvfJir18f6HsfJXGZ4NtVlFOa6XtM+JCEneB9zy7/6iuWjzqeZYcf4bVsyxNPujXpVP9Z6Lyq1pfG6mqWHnEk5d//4Da/6rffwou5OaG+FlH8naovKqLyl1+85jdWVQX+8XSx4eabWXX2OVT+8itrLrkUgMqfvS7TiLraNdZFcV7vWzOvt6mzbrhW9PLLFE14kfXXR2ckEMfQHcqyaOAgNt1zT3gbx6DyFpcJPqeslB3Z7WJ3mi+MH4bSadPY/p//OD8Em++/n9Xn+G8Kql21ivrNm9nyr3+5zd9493i3x/VFRSHHElDAoRkCb1qzYiUVP/3kTE6uSj76iDWXXxFUCKvOPIvV5/7Fa/7qCy70+ePpT/3WraxurrmtlZjaGipmz3a+vitPHcvWp4O4WXyUk0b5t77HZNry+BNttjtvc0xDA7WrVoW9ffHkt6n4KYTKQAyPYuMmwUt6unO6Q2kJdckpVKSlB9gimJ1G742p/OVXCvsPoH7DRrf5ka6BLz3kULfHdevXs93RHBCKUF8LH6uvOOEE1lx0MStPP8Nr2YbbbqciyAHeHE0rDeXlbvOrf//d7za1q1ZRNPFVCvsPoHbNmqDKaRX261r2xUzWXHIpNYsXOxdte/6FZrfzZOrr2Th+vNv4SuFw9Jryp+ill1h7+eUtKiMayr/9ltUXXIhp9H/b6K1PPsXyUcd7DykSgjUXXRT6RlqDj4ycMmuI2uLs9lHZf/WSJWx78UW/y4Npdy7/+quQy100eB9nc0I4H5bGmhqWHXU0mx94gPrt20PadunBh4RWboDVXJNYSywZvl/Q6y4fdTxbHn0UgOqFC4PerrGigsq5c9l493jKvvySpSNGBuxtFQ0Be9R4vB+V//sfxZPfZtnIoyjsP4Dlo0dTv2NHSOWVfv45iwbv0+JrMerWr6fW7lzQWFVFlY8f4Mba2qaeZH4YY1i8/wHsePudZstcd8ONVM6ZQ82yZX7XcQzbEWrXz5rly0Na30lr8JGVayf4HdlBj6jgm583ZtXZ54CPZgaXDZvfdxgJ2tTWWofxbkUF/+Gpce3DHqCGE4zqJUswDQ1+l5fNmGFNtMGTrKGcJFv3t7+x+rzzKX77bdZdcy31Gzey5qKLfTYzOdTv2OFWRuUvv9Loo0a8+aGH2HDzzQHLL50xg0WD9vZOLj5e1/qtW9kx+W23ebXLllP+9Szn47JZsyjsP8BvDb+hvJzid60b4bjeM6E5pZ9+6nWl67Kjjmb50ccAsPHOO1l1xpnUb9vmXF7YfwCLB+/D4qHDAu+8oYHG0lI23Rv8sBMrTx7jf2GYn8kVJ44Oa7tYitMEb/VJ39GuZTX48i+/9Dk/0JcbCDh2TcT4S1IhJK9tEyaw8rTTmy/K5cegobiY6sJCVp48hm0TPAZBs8sO5qKykk8+oeSTT4KOFaxaYERO+IXw21o137vJrOq331i092Cf61cvXsLSgw6m+D0rSdasXMnqc89l8wMPArDs6GNYe411S4VgbgTjGCup+s/CZtdde+21lDUzzHXJ+9aIIVW/Nx3FbH3uOUqnW5eoLD/6mKCbysBK0luefJL1f/s76668iuL33/f5A+oor3b1atbfcmvIFx4CAT/bDeXlVpNdKBUnYyj/7nvrB29zy3q4NFZWsvmRR2msqfFfXG0t1RE6eg1W/CR4l1/l3NLoNtEErr0H1wQRk37iHmVuffKpoGppOya5j8hYt9E6b7DtGe9RM3e88w6Lh+zbNMNPbWnDTTez4SbftVfT2EjVQve4dkye7KxZtlgUX/va5VbTgONIyzH8tOPoqW7dOsq//DJgE4JvvmN2nduw3XdTjKmtYcMd49yb5VyH4njmWdb//Sa3eENRNKGpuXLjnXdREeAmL5sfeZTSqVMDDvLnxfEZCvC+LRm+X/BNdvb+qhb87jyPULWgZee+iv7vFba/+io73vBxfsvlO7D5/gdaVE6o4ibBu6aR9vZl/i2twUdVhHJMzaJFNJSGNHozACVTpwW9bv0m9xPBfr9oImz6590hxbHhrru82piLXnmFVaefTuW8ec55m8bfw+YHHwxp328qINUAACAASURBVK3B1NVR/t13IbfPbpvg/xyOG5fktu2llyn+cAqF/QdQ8YPdPda1suHnfSn5eColH37I1ieebJUms+ba1JvdvrravfkvlJhdXgPPZjFTW8vi/fanyu5iu+WRR4Lera/317US4jiqN/X+my0942sNcZPgXSU1NtCuvKzlbfDRFOIbXfrFF87pFWPHsnLsac7HvvqENyeYD3f91q3hHUq7CfzlLHnvfa+jiJpF1hFQ3YaN1G3eEoWTmpH7km164AHWXn6Fs33W35GZCbdMlwS/9fHH2XjHHQBsvP2O8PYV4HPnFbsINDaTsHwI2IUwiM/94iH7su666zHGUPzee84xnVy3LZk6jcL+AwI22W152PqMF732GsuPG0X9jh00lvkZ48net6N3m2uTTX1Rkc/2923PP++6A6/l9Tt2UPX7QrdvQENJMYX9BzgvTCv//oeQmypDEZcJHqwTrTui1UQTCSEm+PXXXe+crvFoj3Ud5S9Q81CozUJLDzucNZddRu3q1Z478ldASPt3WHftdT7n16xYzrIjjmBNGN3xAnUT3HDLrX6XbX74EYpemRh0ORU/zPY5v7GkhLrNm916VLkmjdJpwR1BlU6dCkDxe++7zfd1M3Pjr/3XRw14/Y03sum++93L+vhjj+2g9LPQr5Atfvtt75m+fjwCKP/qK6p++YWNd/2DTY5B/FxsecLPALUu5Ti6HG95+BFqV6+m1h7+25ftE1+l+IMPnU0srjfI8fejUP7VVwFHmV199jmsOsO9S3DNUqtpbvtr/wGsezX4a6qMhPhN8KUlFLflGnwUVRc2f0LOl/qiIkx9PQ3lFc7eDlVz53m1l9as8P9F8drn5k2svfKqplqYr3U8u6vZXxLHnayq5s7z3MSpdPp0jDFebdrLjjk26BhdbX/tNefFYw3FxWH/aFXM/pFlRxzpHtMRwV+E5SmYO3L57fZn55zaFSso/6Hpyucdb77ptlrdps1ujzfefofXj0b9tm1sf735k8P+OK5VaK73EDQ19TT4uFjPVDTfDOR5ZLjmoov9rls1fz4bx43DsyZevWgRmx/9l++NgNXnnW8H5P05cVaMfPyYteTWoKGIn8HGPF7EnLJSlvfsHaNgghDFtrhwmjQaKypYesih5Jx9FiVTPsIEaJrZ+njww/s7hl8o/fyLZtZ0EUKb6/q/30T3hkbvBBigC2cwNj/yKNtffbVF+3BlWtgu3RKOQfO8apseSh1dW902dn8vlh56WMjll3z0UcjbAIG/I/4+Iy38XpVO/9Tev/Vv9V/OC1g58Y4ruNVqV6wILbAwxW8NvqykTbfBb//Pf1q/0ACffcePQvHktwMm97CLbubKSDchngfccMstzXZdDVUwyb123Tr3JoHff6f6d98XUTkOzWMiyB/MmjCP/Jqz4bbbwzqXU/SSdSMc12s/GsrLrSa0VjpZGW5vt8JBezunt7q11bvsO8KfWV/itgafW1ZKeUYWtUlJpOxiY2v7u8TdX++GmuXLSWwfgfMVAb4MJVOmBL2b0o+nhlx0aYATVeuiNBDX8qOPITk/3/l41Rlneq2zKsB4QjuDSF113OBygZMv9du2kdSxo9s8X0ccSw862Csxrhg7tulBhHoJNZaVWdcGBHNS+IADyTn9NPeZLjnH0dToabnLiVtjTFRGXo3bGnx7+2Knrbl5MY6k9VX4uYfs2ssu8zl/xYmj2TBuXDRDCnjVq6tw2yZdjxA8u12WxXIo3bY+/HOUhDrOS7BNP75qvW6dDiL0em8afw/r/34TJojmzkaPE95lM4Pr41/nMiZSKM2eoYifBO/x6/d73/4APHm2/xMrqkmFn1EDQxGoll4d5NW9Sw48qMVxhH4RUfhCvpFLLMRguIiGYu9ePq2hNZo9muOvV1gg2994s/mVwhA/Cd5D33WrANp2V0kVHbtordmfus2tf6s417t7Bat0xmcsH73zjffiKuxmlih9ZuM2wR89x+oOdthvgXsOqDikCd5N3Wr/wyN7XeMQIaFe0QxW3/zaZWGO2OhHa9+spGym7/GrmhONjg0QTwne45ezQ6nVlvvGqFNiEY2KoXCGbthVrb/V/0VfKgR2naJq/vzYxuEhfhK8B0e6z64MoQ+riguBLmhR7kIdE13tXOInwfto+zrs1zlkVZb7WFkpBXjdUUyFZ/vE4Ie3aE3xk+B96LVpPRs6daE+ITHWoSilVKuL6wSfWldHQ2ISM/c/JNahKKVUq4ubBO+rc9JeK6x7Sv6499DWDUYppdqAuEnwvtrg911i9cX9dugBrR2NUkrFXPwkeB8SXG98nJoWw0iUUqr1xXWCB+i1cT0Abxyv/eGVUruWuE/wDz9n3bbrrePGxDgSpZRqXXGT4NudcILP+d2Kmi7kGPHCW60VjlJKxVzcJPgud/of7vbB5x51To944S2+Gn4QdYnaN14pFd/i5oYfEiBhH7TwV/qtWcHSXrsBcN+l17stn3H9BaS2gWFGlVIqkuImwTfnpYfuZHNuHmc/+KzXslFPN91E+PxP3uecz6eyo117KlPT6Lve/0h8SinVlkm49xxsdsciE4HRwBZjzKBgthk+fLiZ28yNgQMJdmjQqtRUTnjytaD3e+YX07jqA/cB+RsSEljTpTsFG3eCGz4opdq8AYvCuyeuiMwzxgz3uSyKCf5woBx4va0leFeNIpx9/zNs7RDerf32XraIp/59DxDyvaKVUsopGgk+ak00xphvRaRPtPYfKQnG8M6d1zofG+DeS69n1vDgbh33e9/+jPTonXPZlMkkNjbwzlEnMnjZIs788hP2WmndRq4kM4s5A4dwzBzf901VSqlIiVoNHsBO8NMC1eBF5ArgCoBevXoNW92CO8xE6+4tBphyxDE8ffYlUdk/WM1A2ZXl/DxwCAv79mfSXdeTXVFOVnUVy3v0omPxDtpXlDnX39CxM4V9+nLU3NkUtcshpb6OlLraoE8Wf7Pv/hywcD5pdbXNr6yUirqdqonGLrgPzSR4V7FoomkJA2xrn8tj51/BnIFDWrXsYI2b+CwfH340C/v2Z+9li7jt9Qm8POZsvhl2IJlVlRz+y8+c8/nHFLXPZUmvAk76/kt+GrgvXw8/kMunTKY2OYWyjEwmH3MSp3/1KQUb1pJn3y2rLjGRd44ezRlfTueHwcMYvuh36hMTaV9eRoIxbMzrRGVaOl2KtlKdmkZabQ0ARoSsygpeOflMFvQbwO3/eYHu27awpks3GhITaVdeTl5pMWu6dKf71s28fuJYDvz9F/Za1XQ7t4q0dFZ078neK5ZQlZpKeXomnYq3O5ev6N6Tgg1rnc1m9QmJ7GjXno72Oi1pTvt2yH4MXfwHaTU1VKemklVV6XO9soxMsiorwi6rUYRGSWBTXifytwZ/X1UDbMrr5HYNSLDlVaekklETndvHObxz1Il027Y5qNtpbs3pQHl6hte5rj8K+tFt2xY6lIV/c+9GEcSYFn0WDJFrmtUE34zWTvDBMMCSXgW8OvoMft5731iHo3ZiF017z/oRTEjkm2EHAnDNO//hoyOOJbuynPOnf8j3Q4Yz/ZCRzm1O/XoGRe1z+XXPgQCUZWZx0rczmXr40QCk1tZw58Rn+eeVN7mV1a68jC7bt3Hvi49zzgPPMHD5EmpSUqwfnYQEVnXvyZ6rlnP0/34go6qKf13wV8A6J3XMz9/RbdtWlvQq4OVTzwHgjJmf8O7RJ7qVceNbr/DkOZdy1udT+WnQvqzuns9hv84hub6O7MoKfhg8jG251rmxrMpyjvn5ey79+B1qklM47dEJALzw8J1szelAXVIS9112A2/+4wZWdO/JtEOP4rifvsUIFLXPRYDsinIO+GM+L556Lt8N2Y+KjEwAxn71KZ8feBj/uedm7rvkOrbk5jHqp29Z17kr5RmZdCnaxoZOnRn71QySG+r59KAjSGxsZMS8n7jtutsBuHPiM4iBrbkdSK6vo+emjbx68hmk1dRwydR3WdO1OwcvmMfj517GeZ9OYdbQA5h83MkAfHHNeRiBwX8sDOtzscsl+OxRoyibMSPs/bQF9QmJrO7Wg+U9ejF78DDmDRhEeUZWrMNSSkXJphHhtQLE5CSriLwFHAl0FJF1wN3GmFeiVV68SWpsYPf1a9h9/RqObcUTso6fe2MPv2zV2hKoT0xic4c8SjOzqU9MpCI9g7KMTDZ36MjGjp0pzcqmPD2d0sxsSjKzSauroTi7favFrZTyFs1eNOdEa9/NkZTkWBW903O0J4p9ZJdeU+Nc5nqSVykVYWG2wQcSN2PRuOo6bhzpw4bFOgyllIqpuEzwiTk5dP7732IdhlJKxVRcJnillFLxnOB93KNVKaV2JXGX4NuPHRvrEJRSqk2Iq+GC95z/K5KsPWiUUgriLMEnpKXFOgSllGoz4q6JxiFt4MBYh6CUUjEVtwk+ITWVvt98Q7uTTop1KEopFRNxm+ABkrt0psMFF8Q6DKWUiom4TvAA6Xs3jXOWkKWDdSmldh1xn+Bd7fHzT7EOQSmlWs0ukeBzzjid/OeeRRITnfPyLr8shhEppVT07RIJvtt995F91FFu8zrfdJOftZVSKj7sEgnen36zfyDvqitjHYbTbtM/iXUIEZO+r969qjV0vfufAZcn9+jRSpGotmiXTvBJHTrQ6frryTjwQK9lXcePb/V4JCG0tyPL46gkFjpc6vtG5D2efLLZbbOPOSbS4bR5SV27hrR+s/fpbGbMpb5fzgypPBWadiccH+sQAtqlEzyAiNBu1Cjn49wLzgcg+7hjnfPajxlDwZQP6XzbbV7b5z/3rNvjTjfeGFzBSd4XEbvePjGY2nyX226l8y23uM3r/d/X3R53uPBCt8d7/PQj/b7/zuf+ejz9FACSnt5s2f72D5A1YgTJXTr7XD+xU8em6Zwc3zuN4XATu8+cSdfxdwOQsd9+XssLPv7I53aJHTo0u+8BiwrpN+vrkGNyrWx0f/QRt2WZBx8c1s2au4y7I6j1er78MgUffhDcTl3OccVSQmZm8yv5+P6FwzSGeMtTlx/khHbtIhJDILt8ggfIGjnCOd113DgGLCokKTfXOa/7Iw+T1r8/eRdf5JzX4eKLyT3/fLKPOorMQw91zu945V+d0ym9ewOQNmgQXe66K6SYUnfbrfmVfNT4U1y2yznzTHLP+4vzcfYxx5CYk0NSx44UfPC+17ZZjufh5z69vd/4b9ODxETyn32G5M6d6fLPfzQfq63XSy85jzySOvv+Edg9zKaqjIOajsRyzjwTgKyRI9l95kzyn38+qB/flPwetB8zhvannkqPp5/ySp5pe+zhs1acvvfetD/9NAD6vPuO13JJSQnpubjz/X4MWFRISq9eYe0x99xzg1ov67BDSe7WLah1Pc9zueof5g2lw9H5tlubXafTtdc0u46/I0zH+wyQsb93JcD1ZkN9v5nlVgnsdMP1zunEXD8VnAjSBA8k+0k0gXS57Va63jkOgPYnjfa5Ts9X/g+A9MGDSeu/p9uyYD5gTnbNKLFDB3q+OIGCDz+g8623kpKf77Za32++ISkvz/m48803kZyfT4dLLmG3T6eT/8zTzmVpe+3lXY5L7WK36dPZbdpU99E5XZa3P/UUso8+GoDUggL3/dg/EL1ee40ud93Fbp9Mcy5KSE8n/9ln6P6vf7n9GLpK7tLF53xP3R64n8yDD/a5rOs/7mL3z2bQ4/F/k5Lfg+yRI+h45V/pPenNZvebkJ5O94cedPuRd4uvRw/6F/7pNs9g6H7//QxYVEj63nt7bxTg5vaJeXnkP/uM/4Bct/Wznx5PPkHeZZf6XOazWSjM2naXO273uyxj2FDnYH/JPXs657c//TS3HmwOHS50vwgx+1j7qNmj4tL51qaEnTVyJPkTXqD/n3/4jUMS/dfOc8+17iSaeehhftdxrvuXv5A+xPtG2GJ/D7JHjSL3HPc7kw5YVEifN9+g20MP0eHSS0ju0oW0QU2fh45XXkmy/aMskhD1Jh5N8LY+k9+ih0sCBOuLF4z2Y8a4Pe6/8Hf2/GUeKfn5FEz5kC6330bG8OFkH9/UFNTxyiu9D609vruOppK+M7+g96RJ7DH7B7KOOIK0AQPIu+Ri91hzc53NIql77GFtn5KCiNDl1lu8kzDW4X6Pp55yKdBO4MaQulsBqX37ut/f1iXBd7v7br+vR9ZRIwHIPPAAOpz3F1J3391Ze07s0AERof1Jo91G/nRtWpKUFNqfcorf/TvknHYavSa63Mfd5fWT5GRSevf2GoAuY+hQ5/Qec35utgx/RISu995D/nPPknPG6XS7556A66ftM9g53fGaph/37o89xh4/fO/zPJCD8Ujq7cec7LVOu1Gj3BKJK8eRZNaIpiNVESFtkHURYLPnS1ze99wLLqDbA/c7H+c//1xTDKNHs/vnn1lHegnWNn3ef8/va5Nz9tkk2T/mfb/+iu4PP2QVl5hIar9+1nM95RRy/3KuM9aMYcPIPvJIn+erHJ/7QDIPOYT+hX+SPmig1/uf3L1701NOSSFtrwH0/u/r9Hz5JY+9WM8t86CDnMne67mdegpd7OZTz+bK/GfsH3MRejz+OJKcTO5f/uK5i4iIq9Ekg9Hvx9lQX+81P33IEDxbnneb8iG1a9cFtd/E3FzajbZq8pKUhNhtfGn9+zvXyX/iCQo/nRF0rAUfvE/VvHkkd+vm/zDZ/oC5/sj0mvgKVQsWNDu6ZvuTrUSxHkgbPLjpi9zY6LV/gMR27SAxkeQuXQIOy+z6RXHI++sV5F16idd2qf36UrN0GRn77Ue/777FNDRYCwLUeD11ve9eNv0jcG8SXxJb2AaaazcDBWqaSBs4kIwDD3A7V9HpumupXbmC0umfOu9yHsoJ9u6PPEL3Rx7xuzz7mGMo++IL5+P8p5+i8pdfyB4xgqVHHEn95s1u6yf3aHq/EjIyaKystGJyvFcBTuRmjxzZ9CAhweuzmpiZ6VV7T+7Vi7o1axARdv/ic+o3biS5Wzcaq6ud5fWZ/BYNZWUk20cfBe+9S+2aNW5HBr0nTaJm8SI23XOvFcvRR1GzZAmJuTmk77MPVb/95h2wiDMpe77/nW64ng233Q7JyfRf0LRt1mGHMWBRIYX9B/h8DdL33ZeqX3/1+xqBdX7LcW5AEhOcsQD0/31BwG1bYpdL8P4Ou32u26kTSZ06BbXuHj/ODjckv1ILCnzWvJuT1LGj+xevGf3/WAgiGPuHzzW1Or4MWUcfRWrfvvT/bb7XFz7V5UcM8PljJCI+T572mTyZhrIyK+4gX2uAjOHDndNZhx8OWD9YDUXbyLviioDb5px1FmUzrB/a3pPeJKlzZ+rWrad+8yaf63e89lq2Pfusz2XNkZQUZ03OVWr/ATD9U5K7Wck1ISODzrfdxhaXxO04T5J50EFBlma/cx7vT2L79mTbtfeCKR/SsH2715ZJXbvS6bprSd93KCtOOIF2J55Ij38/ZsWWnd30fHwke0lNxdTUuCXyzn+/iQ233uq7echlFwkpKc4jDMcPSocLLyQhM9PrZKnn+YaMofuSMXRfZ4LveNVVpO6xJ1kjRlD00svO9Tpedy1F//cKpqrKeSTgfN6dO1O/ZYsdjBV/u2OPJZCcM06n+N13yTzkEPv5NH/3OLd9Gt/vUzTscgm+pbr/618k5rSP2P7yn3+OmmXLqVm2lJTevdh95hfUb/KdaHxJH2wdlvs62RMs5xfTrkUmd3dN0Pbh6IFWkhEfvQ+ScnMpmPIhK085FUlPD+4Esc3XF9lr/1270uGCCyiaOJHdP52OqasjyaXXSnKXLs7mrpyxpzZbZrd7xtPtnvFAU5ON5/kMV52uvSbsBO/vS5x32aVkHnww6YOahrXOu/giZ4LvX/inM5mmFhTQ9d572PTPu91OonuXZdcME/0fDSTl5jorOR0uvogNN91MSq9ebr17Cj76iJQ+vV2eglDw0RTKZ30D4JUk+30zi6oFC9xqxO2OO5Z2x7knyj3+NweAHW9OYuuTT3r1PJLExLB6BLUfM4Yk+6iy3ajjrJkuR4CdrrmGTtf4Pue1+xefs3gfRzt780eN2ccdR/rgwW5xdrzqStZeHrhS4SrJ/lHveMXlQW8TLk3wIfJ3QjVYvSa+4laryR450q22nZKfHzDZeMoYPpw9fv6JxPYt/9GRpCR6PP0U6ft4n1hqtsnEXh5urw5PnW+9hdrVq6maP5+E9HTyLrnY67zDTsFPgpeEBLfk7pD//HMkderkVVPOOeMMsg4/3Nlk4Uv2yBHknnsuHa++iqxDD6P6zz/9rgvQ/sQTaX/iiV7z0/b0bstO23NP0vbc0zntKjEnx3kUFUiifSTgr7kuXN0feTjsbRNSU71n+nnP/P34ZB12GKkDBmDspq3mJGZlhvVDFg5N8K3MX6+PlohEcnfwOjx1OfEaDF+9JcKRlJdH/vPPsfTgQ5oOhXciu039mBUnnewziQfir2lNRAImd7CaOLraXVZzThsLp0Xv/sTZo0YFdbTki7/mutgLv8lkt2CvFWhlmuBVQJmHHMyON98kbe9BAddL3XNPOlx0kVu/+5ZK6tCBvl996be/fFuW2q8ffd57z2dtOB7kP/lErEMIqNPf/86aCy8M6orq5B49qFu/nuzjjiVnzhw63XhDK0TYOsSzC1YsDR8+3MydOzfWYSgPDeUVJGYFcXVgHCv/7ntqli7dOZuJVED1O3ZQt2ED6TvpbT5FZJ4xZrivZVqDV83a1ZM7WFd0Zh12aPMrqp2O64nneKMXOimlVJzSBK+UUnFKE7xSSsUpTfBKKRWnNMErpVSc0gSvlFJxShO8UkrFqagmeBEZJSKLRWSZiPi/U4BSSqmIi1qCF5FE4DngeGAv4BwR8XEbIaWUUtEQzRr8/sAyY8wKY0wtMBkY08w2SimlIiSaQxX0ANa6PF4HHOC5kohcATgGUy4XkcVhltcR2BbmtrGkcbcujbt1adzR19vfgpiPRWOMeQnwvOlhyERkrr8Bd9oyjbt1adytS+OOrWg20awHero8zrfnKaWUagXRTPD/A/qJSIGIpABnAx9HsTyllFIuotZEY4ypF5Frgc+ARGCiMeaPaJVHBJp5YkTjbl0ad+vSuGOoTd3wQymlVOTolaxKKRWnNMErpVSc2ukTfFsZDkFEVonI7yIyX0Tm2vM6iMgXIrLU/p9rzxcRedqOeYGIDHXZz4X2+ktF5EKX+cPs/S+ztw3rFvAiMlFEtojIQpd5UY/TXxktjHu8iKy3X/P5InKCy7I77BgWi8hxLvN9fl7szgA/2/PftjsGICKp9uNl9vI+IcbdU0S+FpE/ReQPEbkh0OvRVl7zAHG36ddcRNJEZI6I/GbHfU+4ZUXq+cSUMWan/cM6ebsc2A1IAX4D9opRLKuAjh7zHgVut6dvBx6xp08APgUEOBD42Z7fAVhh/8+1p3PtZXPsdcXe9vgw4zwcGAosbM04/ZXRwrjHAzf7WHcv+7OQChTYn5HEQJ8X4B3gbHt6AnCVPX01MMGePht4O8S4uwFD7elsYIkdX5t+zQPE3aZfc/s1yLKnk4Gf7dcmpLIi+Xxi+RfTwlscPBwEfOby+A7gjhjFsgrvBL8Y6GZPdwMW29MvAud4rgecA7zoMv9Fe143YJHLfLf1woi1D+6JMupx+iujhXGPx3eycfscYPXkOsjf58VOCtuAJM/PlWNbezrJXk9a8Np/BByzs7zmPuLeaV5zIAP4BesK+pDKiuTzieXfzt5E42s4hB4xisUAn4vIPLGGXwDoYozZaE9vArrY0/7iDjR/nY/5kdIacforo6WutZsyJro0QYQadx5QbIyp9xG3cxt7eYm9fsjsw/99sWqVO81r7hE3tPHXXEQSRWQ+sAX4AqvGHWpZkXw+MbOzJ/i25FBjzFCs0TOvEZHDXRca62e9zfdJbY04I1jGC8DuwBBgI/DvCOwzKkQkC3gfuNEYU+q6rC2/5j7ibvOvuTGmwRgzBOvq+f2B/jEOKWZ29gTfZoZDMMast/9vAT7E+mBtFpFuAPb/Lfbq/uIOND/fx/xIaY04/ZURNmPMZvvL3Ai8jPWahxN3EZAjIkke8932ZS9vb68fNBFJxkqSbxpjPrBnt/nX3FfcO8trbsdaDHyN1VwSalmRfD4xs7Mn+DYxHIKIZIpItmMaOBZYaMfi6O1wIVY7Jvb8C+weEwcCJfah9GfAsSKSax/6HovVjrcRKBWRA+0eEhe47CsSWiNOf2WEzZG8bKdiveaOss62e0gUAP2wTkT6/LzYtduvgdP9vAaOuE8HvrLXDzZGAV4BCo0xj7ssatOvub+42/prLiKdRCTHnk7HOm9QGEZZkXw+sRPrkwAt/cPqdbAEq53tzhjFsBvW2fTfgD8ccWC1y30JLAVmAh3s+YJ1M5TlwO/AcJd9XQIss/8udpk/HOvLtBx4ljBP9AFvYR1a12G1E17aGnH6K6OFcf/XjmsB1heym8v6d9oxLMalx5G/z4v9Hs6xn8+7QKo9P81+vMxevluIcR+K1TSyAJhv/53Q1l/zAHG36dccGAz8ase3EPhnuGVF6vnE8k+HKlBKqTi1szfRKKWU8kMTvFJKxSlN8EopFac0wSulVJzSBK+UUnFKE7xqs0QkT5pGLdwk7qMYBhypT0SGi8jTQZQxO3IRe+07R0Sujtb+lWqOdpNUOwURGQ+UG2Mec5mXZJrG/mhz7DFcphljBsU4FLWL0hq82qmIyGsiMkFEfgYeFZH9ReRHEflVRGaLyJ72ekeKyDR7erw9MNYsEVkhIte77K/cZf1ZIvKeiCwSkTftqzkRkRPsefPEGm99mo+4Boo1Dvl8sQbi6gc8DOxuz/uXvd4tIvI/ex3HWOV9XMostGPIsJc9LNaY7AtE5DHPcpUKJGo33VYqivKBg40xDSLSDjjMWDd5Pxp4EDjNxzb9U9PWKQAAAiFJREFUgRFYY5svFpEXjDF1HuvsCwwENgA/AIeIdfOWF4HDjTErReQtPzFdCTxljHnTbj5KxBqDfZCxBr5CRI7FuuR9f6wrVj8Wa1C6NcCewKXGmB9EZCJwtYi8ijUcQH9jjHFcgq9UsLQGr3ZG7xpjGuzp9sC7Yt3p6QmsBO3LJ8aYGmPMNqxBt3wNnTvHGLPOWANpzccaf74/sMIYs9Jex1+C/xEYJyK3Ab2NMVU+1jnW/vsVa5zy/lgJH2CtMeYHe/oNrKECSoBq4BURGQtU+ilbKZ80waudUYXL9H3A13Y790lYY4v4UuMy3YDvo9dg1vHJGDMJOBmoAqaLyEgfqwnwkDFmiP3X1xjzimMX3rs09Vi1/feA0cCMYONRCjTBq51fe5qGZb0oCvtfDOwmTffqPMvXSiKyG1ZN/2msUQQHA2VYTUIOnwGXiDXGOiLSQ0Q628t6ichB9vS5wPf2eu2NMdOBvwH7ROxZqV2CJni1s3sUeEhEfiUK55TsppargRkiMg8raZf4WPVMYKFYdxIaBLxujCkCfhCRhSLyL2PM58Ak4EcR+R2rZu74AViMdaOYQqx7rr5gL5smIguA74G/R/r5qfim3SSVaoaIZBljyu1eNc8BS40xT0Rw/33Q7pQqCrQGr1TzLrdr5n9gNQm9GON4lAqK1uCVUipOaQ1eKaXilCZ4pZSKU5rglVIqTmmCV0qpOKUJXiml4tT/A4gZymq4ly0pAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3iZTVn5WQFpX","executionInfo":{"status":"ok","timestamp":1616231875859,"user_tz":-480,"elapsed":699648,"user":{"displayName":"張君豪","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giewpu4H1RC4LywWkj1iyfrLc5i6hgwttwazWU6EQ=s64","userId":"03999016312843703738"}},"outputId":"47c7c329-9116-485b-a99e-b1f8ea16896b"},"source":["del model\n","model = NeuralNet(tr_set.dataset.dim).to(device)\n","ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n","model.load_state_dict(ckpt)\n","plot_pred(dv_set, model, device)  # Show prediction on the validation set"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 360x360 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"aQikz3IPiyPf"},"source":["# **Testing**\n","The predictions of your model on testing set will be stored at `pred.csv`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O8cTuQjQQOon","executionInfo":{"status":"ok","timestamp":1616231875861,"user_tz":-480,"elapsed":699641,"user":{"displayName":"張君豪","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giewpu4H1RC4LywWkj1iyfrLc5i6hgwttwazWU6EQ=s64","userId":"03999016312843703738"}},"outputId":"7c6eaf17-0c84-41b9-cab8-7dd240b3849f"},"source":["def save_pred(preds, file):\n","    ''' Save predictions to specified file '''\n","    print('Saving results to {}'.format(file))\n","    with open(file, 'w') as fp:\n","        writer = csv.writer(fp)\n","        writer.writerow(['id', 'tested_positive'])\n","        for i, p in enumerate(preds):\n","            writer.writerow([i, p])\n","\n","preds = test(tt_set, model, device)  # predict COVID-19 cases with your model\n","save_pred(preds, 'pred.csv')         # save prediction file to pred.csv"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Saving results to pred.csv\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nfrVxqJanGpE"},"source":["# **Hints**\n","\n","## **Simple Baseline**\n","* Run sample code\n","\n","## **Medium Baseline**\n","* Feature selection: 40 states + 2 `tested_positive` (`TODO` in dataset)\n","\n","## **Strong Baseline**\n","* Feature selection (what other features are useful?)\n","* DNN architecture (layers? dimension? activation function?)\n","* Training (mini-batch? optimizer? learning rate?)\n","* L2 regularization\n","* There are some mistakes in the sample code, can you find them?"]},{"cell_type":"markdown","metadata":{"id":"9tmCwXgpot3t"},"source":["# **Reference**\n","This code is completely written by Heng-Jui Chang @ NTUEE.  \n","Copying or reusing this code is required to specify the original author. \n","\n","1. Heng-Jui Chang @ NTUEE (https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb)\n"]}]}